{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexisVillpy/tesis-deteccion-de-hurtos/blob/main/TesisV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d374O-qcXHaQ"
      },
      "source": [
        "### **CONECTAR A GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PbYVRcQXHEb",
        "outputId": "2ccca14a-75fe-4b84-902e-12fc14eb78b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hMBbaVnXOkTf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Ruta real en tu Google Drive\n",
        "origen = \"/content/drive/MyDrive/ColabNotebooks/TesisV2\"\n",
        "\n",
        "# Ruta simbólica dentro de Colab\n",
        "destino = \"/content/tesisV2\"\n",
        "\n",
        "# Crear enlace simbólico (acceso directo)\n",
        "if not os.path.exists(destino):\n",
        "    os.symlink(origen, destino)\n",
        "\n",
        "# Cambiar el directorio actual a esa carpeta\n",
        "os.chdir(destino)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6aXTdlEyW8"
      },
      "source": [
        "### **ARBOL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSFaGo_Ri5Nf",
        "outputId": "eb89142c-8262-41c2-c735-3f0b8dfd8a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tree is already the newest version (2.0.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "\u001b[01;34m/content/tesis/runsPersona/\u001b[0m\n",
            "└── \u001b[01;34mdetect\u001b[0m\n",
            "    ├── \u001b[01;34mpredict\u001b[0m\n",
            "    │   └── \u001b[01;35mhurto1.avi\u001b[0m\n",
            "    └── \u001b[01;34myolov8_personas_hurtos\u001b[0m\n",
            "        ├── \u001b[00margs.yaml\u001b[0m\n",
            "        ├── \u001b[00mBoxF1_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxP_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxPR_curve.png\u001b[0m\n",
            "        ├── \u001b[00mBoxR_curve.png\u001b[0m\n",
            "        ├── \u001b[00mconfusion_matrix_normalized.png\u001b[0m\n",
            "        ├── \u001b[00mconfusion_matrix.png\u001b[0m\n",
            "        ├── \u001b[01;35mlabels_correlogram.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mlabels.jpg\u001b[0m\n",
            "        ├── \u001b[00mresults.csv\u001b[0m\n",
            "        ├── \u001b[00mresults.png\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch0.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch1.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch2.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33920.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33921.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mtrain_batch33922.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch0_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch0_pred.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch1_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch1_pred.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch2_labels.jpg\u001b[0m\n",
            "        ├── \u001b[01;35mval_batch2_pred.jpg\u001b[0m\n",
            "        └── \u001b[01;34mweights\u001b[0m\n",
            "            ├── \u001b[00mbest.pt\u001b[0m\n",
            "            └── \u001b[00mlast.pt\u001b[0m\n",
            "\n",
            "4 directories, 26 files\n"
          ]
        }
      ],
      "source": [
        "!apt install tree\n",
        "!tree /content/tesis/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi_mpvRHkss0"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sample_data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1wlXlvnXfRI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "# Ruta base donde se va a crear el dataset_final\n",
        "base_path = 'datasetv1'  # O podés usar 'content/tesis/dataset_final' si estás en Colab\n",
        "\n",
        "# Crear carpetas necesarias\n",
        "splits = ['train', 'valid', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_path, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_path, split, 'labels'), exist_ok=True)\n",
        "\n",
        "# Crear archivo data.yaml\n",
        "data_yaml = {\n",
        "    'train': 'train/images',\n",
        "    'val': 'valid/images',\n",
        "    'test': 'test/images',\n",
        "    'nc': 2,\n",
        "    'names': ['normal', 'shoplifting']\n",
        "}\n",
        "\n",
        "with open(os.path.join(base_path, 'data.yaml'), 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(f\"✅ Estructura creada en: {base_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnTj4j9gNQww"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0nZ5qLQoxVN"
      },
      "source": [
        "[texto del vínculo](https://)## ***DESCOMPRIMIR ZIP***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXzPndh-c5JM",
        "outputId": "ba6fb696-0efd-47c9-b29e-e6f5180e67c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "warning [/content/dataset3.zip]:  514762 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  514762\n",
            "  (attempting to re-compensate)\n",
            "error: not enough memory for bomb detection\n",
            "file #2:  bad zipfile offset (local header sig):  895700260\n",
            "  (attempting to re-compensate)\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/dataset3.zip -d  /content/tesis/dataset3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJnijYtdd7DK"
      },
      "source": [
        "## ***YOLO, OPENCV, SYMPY***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgmlH-XOBYuU",
        "outputId": "5da565af-ff61-46a8-fb12-ba198d10551d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.188-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.4.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.23.0 (from ultralytics)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.9.86)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.188-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: numpy, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6 ultralytics-8.3.188 ultralytics-thop-2.0.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "54464f76b16d40ac950ea02e62b3a6aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "Successfully installed sympy-1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "f51e1772eda344a291d4566bb50dabc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics opencv-python\n",
        "!pip install -U sympy==1.12\n",
        "import sympy\n",
        "print(sympy.__version__)  # debería mostrar 1.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc4I-AOkpTtG"
      },
      "source": [
        "## ***PRUEBA: TRACKING DE PERSONAS CON YOLO, COCO Y BYTETRACK PARA ID***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N8Bt61PXBpPZ",
        "outputId": "4936b5a1-9c3a-4ce9-d183-92c7ce1cd604"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100%|██████████| 21.5M/21.5M [00:00<00:00, 69.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "video 1/1 (frame 1/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 528.1ms\n",
            "video 1/1 (frame 2/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 364.4ms\n",
            "video 1/1 (frame 3/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.1ms\n",
            "video 1/1 (frame 4/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.2ms\n",
            "video 1/1 (frame 5/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 324.9ms\n",
            "video 1/1 (frame 6/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 342.5ms\n",
            "video 1/1 (frame 7/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 322.3ms\n",
            "video 1/1 (frame 8/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 345.7ms\n",
            "video 1/1 (frame 9/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.0ms\n",
            "video 1/1 (frame 10/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 346.9ms\n",
            "video 1/1 (frame 11/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 329.0ms\n",
            "video 1/1 (frame 12/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 340.2ms\n",
            "video 1/1 (frame 13/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 458.9ms\n",
            "video 1/1 (frame 14/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 529.9ms\n",
            "video 1/1 (frame 15/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 525.1ms\n",
            "video 1/1 (frame 16/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 530.7ms\n",
            "video 1/1 (frame 17/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 549.1ms\n",
            "video 1/1 (frame 18/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 394.5ms\n",
            "video 1/1 (frame 19/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.9ms\n",
            "video 1/1 (frame 20/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.5ms\n",
            "video 1/1 (frame 21/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 338.9ms\n",
            "video 1/1 (frame 22/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 400.9ms\n",
            "video 1/1 (frame 23/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.4ms\n",
            "video 1/1 (frame 24/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.9ms\n",
            "video 1/1 (frame 25/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.6ms\n",
            "video 1/1 (frame 26/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 325.5ms\n",
            "video 1/1 (frame 27/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 334.2ms\n",
            "video 1/1 (frame 28/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.4ms\n",
            "video 1/1 (frame 29/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.3ms\n",
            "video 1/1 (frame 30/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 324.7ms\n",
            "video 1/1 (frame 31/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 343.2ms\n",
            "video 1/1 (frame 32/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 33/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 340.6ms\n",
            "video 1/1 (frame 34/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.2ms\n",
            "video 1/1 (frame 35/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.1ms\n",
            "video 1/1 (frame 36/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 355.7ms\n",
            "video 1/1 (frame 37/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.3ms\n",
            "video 1/1 (frame 38/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 355.6ms\n",
            "video 1/1 (frame 39/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 354.2ms\n",
            "video 1/1 (frame 40/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 544.3ms\n",
            "video 1/1 (frame 41/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 540.3ms\n",
            "video 1/1 (frame 42/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 527.5ms\n",
            "video 1/1 (frame 43/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.7ms\n",
            "video 1/1 (frame 44/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 535.3ms\n",
            "video 1/1 (frame 45/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.7ms\n",
            "video 1/1 (frame 46/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 355.5ms\n",
            "video 1/1 (frame 47/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.5ms\n",
            "video 1/1 (frame 48/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.9ms\n",
            "video 1/1 (frame 49/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.9ms\n",
            "video 1/1 (frame 50/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.4ms\n",
            "video 1/1 (frame 51/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 334.1ms\n",
            "video 1/1 (frame 52/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.2ms\n",
            "video 1/1 (frame 53/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 352.0ms\n",
            "video 1/1 (frame 54/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 55/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 368.7ms\n",
            "video 1/1 (frame 56/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.5ms\n",
            "video 1/1 (frame 57/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 340.3ms\n",
            "video 1/1 (frame 58/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.2ms\n",
            "video 1/1 (frame 59/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.2ms\n",
            "video 1/1 (frame 60/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.1ms\n",
            "video 1/1 (frame 61/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 332.6ms\n",
            "video 1/1 (frame 62/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.5ms\n",
            "video 1/1 (frame 63/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.0ms\n",
            "video 1/1 (frame 64/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.3ms\n",
            "video 1/1 (frame 65/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.8ms\n",
            "video 1/1 (frame 66/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 435.6ms\n",
            "video 1/1 (frame 67/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 562.7ms\n",
            "video 1/1 (frame 68/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 629.1ms\n",
            "video 1/1 (frame 69/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 523.1ms\n",
            "video 1/1 (frame 70/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 526.4ms\n",
            "video 1/1 (frame 71/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 470.7ms\n",
            "video 1/1 (frame 72/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.5ms\n",
            "video 1/1 (frame 73/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 74/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 337.8ms\n",
            "video 1/1 (frame 75/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 366.2ms\n",
            "video 1/1 (frame 76/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 339.0ms\n",
            "video 1/1 (frame 77/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.5ms\n",
            "video 1/1 (frame 78/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.9ms\n",
            "video 1/1 (frame 79/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 360.8ms\n",
            "video 1/1 (frame 80/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.1ms\n",
            "video 1/1 (frame 81/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 363.0ms\n",
            "video 1/1 (frame 82/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.9ms\n",
            "video 1/1 (frame 83/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 329.2ms\n",
            "video 1/1 (frame 84/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.9ms\n",
            "video 1/1 (frame 85/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 369.7ms\n",
            "video 1/1 (frame 86/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 343.9ms\n",
            "video 1/1 (frame 87/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 327.6ms\n",
            "video 1/1 (frame 88/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.8ms\n",
            "video 1/1 (frame 89/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 90/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 339.0ms\n",
            "video 1/1 (frame 91/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 365.7ms\n",
            "video 1/1 (frame 92/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 480.6ms\n",
            "video 1/1 (frame 93/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 518.4ms\n",
            "video 1/1 (frame 94/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 518.7ms\n",
            "video 1/1 (frame 95/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 517.2ms\n",
            "video 1/1 (frame 96/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 523.1ms\n",
            "video 1/1 (frame 97/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 429.0ms\n",
            "video 1/1 (frame 98/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 323.2ms\n",
            "video 1/1 (frame 99/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.6ms\n",
            "video 1/1 (frame 100/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.8ms\n",
            "video 1/1 (frame 101/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 340.1ms\n",
            "video 1/1 (frame 102/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 347.5ms\n",
            "video 1/1 (frame 103/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.0ms\n",
            "video 1/1 (frame 104/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.8ms\n",
            "video 1/1 (frame 105/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.6ms\n",
            "video 1/1 (frame 106/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 352.4ms\n",
            "video 1/1 (frame 107/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.3ms\n",
            "video 1/1 (frame 108/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.8ms\n",
            "video 1/1 (frame 109/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 343.5ms\n",
            "video 1/1 (frame 110/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.7ms\n",
            "video 1/1 (frame 111/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 350.3ms\n",
            "video 1/1 (frame 112/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.3ms\n",
            "video 1/1 (frame 113/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 327.7ms\n",
            "video 1/1 (frame 114/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 391.5ms\n",
            "video 1/1 (frame 115/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 340.6ms\n",
            "video 1/1 (frame 116/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 358.3ms\n",
            "video 1/1 (frame 117/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.2ms\n",
            "video 1/1 (frame 118/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 352.0ms\n",
            "video 1/1 (frame 119/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.1ms\n",
            "video 1/1 (frame 120/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 412.3ms\n",
            "video 1/1 (frame 121/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 519.0ms\n",
            "video 1/1 (frame 122/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1001.3ms\n",
            "video 1/1 (frame 123/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1055.0ms\n",
            "video 1/1 (frame 124/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 524.7ms\n",
            "video 1/1 (frame 125/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 361.2ms\n",
            "video 1/1 (frame 126/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.2ms\n",
            "video 1/1 (frame 127/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.1ms\n",
            "video 1/1 (frame 128/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 337.4ms\n",
            "video 1/1 (frame 129/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.4ms\n",
            "video 1/1 (frame 130/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.5ms\n",
            "video 1/1 (frame 131/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 340.9ms\n",
            "video 1/1 (frame 132/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.6ms\n",
            "video 1/1 (frame 133/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.0ms\n",
            "video 1/1 (frame 134/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 356.8ms\n",
            "video 1/1 (frame 135/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.4ms\n",
            "video 1/1 (frame 136/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.1ms\n",
            "video 1/1 (frame 137/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 370.4ms\n",
            "video 1/1 (frame 138/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.5ms\n",
            "video 1/1 (frame 139/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 140/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 141/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.7ms\n",
            "video 1/1 (frame 142/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.9ms\n",
            "video 1/1 (frame 143/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 345.4ms\n",
            "video 1/1 (frame 144/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 332.4ms\n",
            "video 1/1 (frame 145/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.0ms\n",
            "video 1/1 (frame 146/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 521.8ms\n",
            "video 1/1 (frame 147/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 518.9ms\n",
            "video 1/1 (frame 148/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 524.2ms\n",
            "video 1/1 (frame 149/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 547.9ms\n",
            "video 1/1 (frame 150/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 538.4ms\n",
            "video 1/1 (frame 151/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 460.5ms\n",
            "video 1/1 (frame 152/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 394.2ms\n",
            "video 1/1 (frame 153/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.3ms\n",
            "video 1/1 (frame 154/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.3ms\n",
            "video 1/1 (frame 155/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 353.2ms\n",
            "video 1/1 (frame 156/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 324.7ms\n",
            "video 1/1 (frame 157/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 342.9ms\n",
            "video 1/1 (frame 158/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.1ms\n",
            "video 1/1 (frame 159/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 338.5ms\n",
            "video 1/1 (frame 160/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 390.6ms\n",
            "video 1/1 (frame 161/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 512.8ms\n",
            "video 1/1 (frame 162/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 505.1ms\n",
            "video 1/1 (frame 163/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 525.8ms\n",
            "video 1/1 (frame 164/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 517.9ms\n",
            "video 1/1 (frame 165/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 521.9ms\n",
            "video 1/1 (frame 166/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 526.0ms\n",
            "video 1/1 (frame 167/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 351.5ms\n",
            "video 1/1 (frame 168/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.7ms\n",
            "video 1/1 (frame 169/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.4ms\n",
            "video 1/1 (frame 170/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.1ms\n",
            "video 1/1 (frame 171/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 350.7ms\n",
            "video 1/1 (frame 172/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 475.2ms\n",
            "video 1/1 (frame 173/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 514.8ms\n",
            "video 1/1 (frame 174/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 531.4ms\n",
            "video 1/1 (frame 175/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 537.1ms\n",
            "video 1/1 (frame 176/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 508.1ms\n",
            "video 1/1 (frame 177/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 529.3ms\n",
            "video 1/1 (frame 178/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 395.6ms\n",
            "video 1/1 (frame 179/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 325.7ms\n",
            "video 1/1 (frame 180/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 339.6ms\n",
            "video 1/1 (frame 181/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 326.9ms\n",
            "video 1/1 (frame 182/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 348.5ms\n",
            "video 1/1 (frame 183/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 368.4ms\n",
            "video 1/1 (frame 184/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.0ms\n",
            "video 1/1 (frame 185/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 353.1ms\n",
            "video 1/1 (frame 186/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.9ms\n",
            "video 1/1 (frame 187/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.8ms\n",
            "video 1/1 (frame 188/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 189/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.9ms\n",
            "video 1/1 (frame 190/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.0ms\n",
            "video 1/1 (frame 191/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.1ms\n",
            "video 1/1 (frame 192/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 350.8ms\n",
            "video 1/1 (frame 193/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.3ms\n",
            "video 1/1 (frame 194/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 345.5ms\n",
            "video 1/1 (frame 195/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.5ms\n",
            "video 1/1 (frame 196/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 331.3ms\n",
            "video 1/1 (frame 197/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 331.7ms\n",
            "video 1/1 (frame 198/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 331.3ms\n",
            "video 1/1 (frame 199/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 342.4ms\n",
            "video 1/1 (frame 200/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.1ms\n",
            "video 1/1 (frame 201/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 332.3ms\n",
            "video 1/1 (frame 202/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 511.8ms\n",
            "video 1/1 (frame 203/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 515.5ms\n",
            "video 1/1 (frame 204/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 515.3ms\n",
            "video 1/1 (frame 205/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 517.9ms\n",
            "video 1/1 (frame 206/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 635.0ms\n",
            "video 1/1 (frame 207/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 434.6ms\n",
            "video 1/1 (frame 208/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 358.8ms\n",
            "video 1/1 (frame 209/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 329.3ms\n",
            "video 1/1 (frame 210/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 211/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.2ms\n",
            "video 1/1 (frame 212/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.8ms\n",
            "video 1/1 (frame 213/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.5ms\n",
            "video 1/1 (frame 214/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 350.4ms\n",
            "video 1/1 (frame 215/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.1ms\n",
            "video 1/1 (frame 216/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.1ms\n",
            "video 1/1 (frame 217/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.5ms\n",
            "video 1/1 (frame 218/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 219/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 347.4ms\n",
            "video 1/1 (frame 220/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 331.6ms\n",
            "video 1/1 (frame 221/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 347.3ms\n",
            "video 1/1 (frame 222/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 326.0ms\n",
            "video 1/1 (frame 223/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 224/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 343.9ms\n",
            "video 1/1 (frame 225/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.7ms\n",
            "video 1/1 (frame 226/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.9ms\n",
            "video 1/1 (frame 227/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 337.7ms\n",
            "video 1/1 (frame 228/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 345.8ms\n",
            "video 1/1 (frame 229/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 545.1ms\n",
            "video 1/1 (frame 230/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 517.8ms\n",
            "video 1/1 (frame 231/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 525.2ms\n",
            "video 1/1 (frame 232/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 579.9ms\n",
            "video 1/1 (frame 233/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 538.6ms\n",
            "video 1/1 (frame 234/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 530.3ms\n",
            "video 1/1 (frame 235/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 351.6ms\n",
            "video 1/1 (frame 236/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 369.3ms\n",
            "video 1/1 (frame 237/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 390.2ms\n",
            "video 1/1 (frame 238/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.6ms\n",
            "video 1/1 (frame 239/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.6ms\n",
            "video 1/1 (frame 240/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 349.8ms\n",
            "video 1/1 (frame 241/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 329.7ms\n",
            "video 1/1 (frame 242/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 330.9ms\n",
            "video 1/1 (frame 243/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 343.3ms\n",
            "video 1/1 (frame 244/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 335.3ms\n",
            "video 1/1 (frame 245/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 353.9ms\n",
            "video 1/1 (frame 246/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 338.4ms\n",
            "video 1/1 (frame 247/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 339.3ms\n",
            "video 1/1 (frame 248/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 344.0ms\n",
            "video 1/1 (frame 249/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.4ms\n",
            "video 1/1 (frame 250/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.3ms\n",
            "video 1/1 (frame 251/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.5ms\n",
            "video 1/1 (frame 252/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 396.6ms\n",
            "video 1/1 (frame 253/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.8ms\n",
            "video 1/1 (frame 254/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 361.3ms\n",
            "video 1/1 (frame 255/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 344.4ms\n",
            "video 1/1 (frame 256/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 410.3ms\n",
            "video 1/1 (frame 257/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 596.1ms\n",
            "video 1/1 (frame 258/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 539.1ms\n",
            "video 1/1 (frame 259/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 538.5ms\n",
            "video 1/1 (frame 260/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 567.3ms\n",
            "video 1/1 (frame 261/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.1ms\n",
            "video 1/1 (frame 262/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 348.2ms\n",
            "video 1/1 (frame 263/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 349.9ms\n",
            "video 1/1 (frame 264/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 371.1ms\n",
            "video 1/1 (frame 265/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 358.6ms\n",
            "video 1/1 (frame 266/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 347.3ms\n",
            "video 1/1 (frame 267/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 354.9ms\n",
            "video 1/1 (frame 268/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.0ms\n",
            "video 1/1 (frame 269/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 351.5ms\n",
            "video 1/1 (frame 270/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.2ms\n",
            "video 1/1 (frame 271/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.0ms\n",
            "video 1/1 (frame 272/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 340.1ms\n",
            "video 1/1 (frame 273/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 349.7ms\n",
            "video 1/1 (frame 274/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 372.9ms\n",
            "video 1/1 (frame 275/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 421.2ms\n",
            "video 1/1 (frame 276/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 342.5ms\n",
            "video 1/1 (frame 277/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 350.2ms\n",
            "video 1/1 (frame 278/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.3ms\n",
            "video 1/1 (frame 279/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 360.4ms\n",
            "video 1/1 (frame 280/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 605.2ms\n",
            "video 1/1 (frame 281/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 523.8ms\n",
            "video 1/1 (frame 282/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 534.8ms\n",
            "video 1/1 (frame 283/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 535.6ms\n",
            "video 1/1 (frame 284/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 438.0ms\n",
            "video 1/1 (frame 285/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 369.3ms\n",
            "video 1/1 (frame 286/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 343.2ms\n",
            "video 1/1 (frame 287/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 342.8ms\n",
            "video 1/1 (frame 288/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 361.0ms\n",
            "video 1/1 (frame 289/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 338.0ms\n",
            "video 1/1 (frame 290/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 366.7ms\n",
            "video 1/1 (frame 291/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.2ms\n",
            "video 1/1 (frame 292/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 362.5ms\n",
            "video 1/1 (frame 293/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.2ms\n",
            "video 1/1 (frame 294/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 348.9ms\n",
            "video 1/1 (frame 295/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 364.8ms\n",
            "video 1/1 (frame 296/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 353.1ms\n",
            "video 1/1 (frame 297/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 339.8ms\n",
            "video 1/1 (frame 298/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 392.3ms\n",
            "video 1/1 (frame 299/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 366.2ms\n",
            "video 1/1 (frame 300/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.9ms\n",
            "video 1/1 (frame 301/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 337.1ms\n",
            "video 1/1 (frame 302/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.8ms\n",
            "video 1/1 (frame 303/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 448.7ms\n",
            "video 1/1 (frame 304/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 533.9ms\n",
            "video 1/1 (frame 305/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 519.2ms\n",
            "video 1/1 (frame 306/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 516.5ms\n",
            "video 1/1 (frame 307/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 557.4ms\n",
            "video 1/1 (frame 308/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 502.2ms\n",
            "video 1/1 (frame 309/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.2ms\n",
            "video 1/1 (frame 310/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 334.0ms\n",
            "video 1/1 (frame 311/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.8ms\n",
            "video 1/1 (frame 312/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 331.8ms\n",
            "video 1/1 (frame 313/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.3ms\n",
            "video 1/1 (frame 314/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.5ms\n",
            "video 1/1 (frame 315/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.9ms\n",
            "video 1/1 (frame 316/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 327.2ms\n",
            "video 1/1 (frame 317/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.6ms\n",
            "video 1/1 (frame 318/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.6ms\n",
            "Speed: 4.7ms preprocess, 387.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Listo. Clips guardados en: /content/tesisV2/clips_personas\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "SOURCE_VIDEO = \"/content/tesisV2/videos/supermas1-1.mp4\"   # <-- cambia esto\n",
        "OUTPUT_DIR   = \"/content/tesisV2/clips_personas\"      # <-- cambia si querés\n",
        "MODEL_PATH   = \"yolov8s.pt\"                         # o 'yolov8n.pt' para más FPS\n",
        "CONF         = 0.25\n",
        "CLIP_SECS    = 3           # duración de cada clip\n",
        "IDLE_SECS    = 0.7         # si un ID no aparece por este tiempo, se cierra su writer\n",
        "TRACKER_CFG  = \"bytetrack.yaml\"  # liviano y sin re-id\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Abrimos para conocer FPS y frame size\n",
        "cap0 = cv2.VideoCapture(SOURCE_VIDEO)\n",
        "if not cap0.isOpened():\n",
        "    raise RuntimeError(f\"No pude abrir el video: {SOURCE_VIDEO}\")\n",
        "fps = cap0.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "width  = int(cap0.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap0.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "cap0.release()\n",
        "\n",
        "clip_len_frames = int(CLIP_SECS * fps)\n",
        "idle_frames     = int(IDLE_SECS * fps)\n",
        "\n",
        "# Estado por ID: writer actual, frames escritos en el segmento, índice de segmento, última vez visto\n",
        "state = {}\n",
        "# Para contar frames globales (necesario para medir idle)\n",
        "frame_idx = 0\n",
        "\n",
        "# Codec y helper para crear writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "\n",
        "def ensure_writer_for_id(person_id):\n",
        "    info = state.get(person_id)\n",
        "    if info is None:\n",
        "        person_dir = os.path.join(OUTPUT_DIR, f\"person_{person_id:04d}\")\n",
        "        os.makedirs(person_dir, exist_ok=True)\n",
        "        seg_idx = 0\n",
        "        out_path = os.path.join(\n",
        "            person_dir,\n",
        "            f\"id{person_id:04d}_seg{seg_idx:03d}.mp4\"\n",
        "        )\n",
        "        writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
        "        state[person_id] = {\n",
        "            \"writer\": writer,\n",
        "            \"frames_in_seg\": 0,\n",
        "            \"seg_idx\": seg_idx,\n",
        "            \"last_seen\": frame_idx\n",
        "        }\n",
        "    return state[person_id]\n",
        "\n",
        "def rotate_segment(person_id):\n",
        "    info = state[person_id]\n",
        "    # Cerrar actual\n",
        "    if info[\"writer\"] is not None:\n",
        "        info[\"writer\"].release()\n",
        "    # Abrir nuevo\n",
        "    info[\"seg_idx\"] += 1\n",
        "    person_dir = os.path.join(OUTPUT_DIR, f\"person_{person_id:04d}\")\n",
        "    out_path = os.path.join(\n",
        "        person_dir,\n",
        "        f\"id{person_id:04d}_seg{info['seg_idx']:03d}.mp4\"\n",
        "    )\n",
        "    info[\"writer\"] = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
        "    info[\"frames_in_seg\"] = 0\n",
        "\n",
        "def close_and_remove(person_id):\n",
        "    info = state.get(person_id)\n",
        "    if not info:\n",
        "        return\n",
        "    if info[\"writer\"] is not None:\n",
        "        info[\"writer\"].release()\n",
        "    del state[person_id]\n",
        "\n",
        "# Cargar el modelo\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# Stream de tracking frame a frame\n",
        "for result in model.track(\n",
        "    source=SOURCE_VIDEO,\n",
        "    stream=True,\n",
        "    classes=[0],           # solo 'person'\n",
        "    conf=CONF,\n",
        "    tracker=TRACKER_CFG,\n",
        "    persist=True\n",
        "):\n",
        "    frame = result.orig_img  # BGR\n",
        "    ids = []\n",
        "    if result.boxes is not None and result.boxes.id is not None:\n",
        "        ids = result.boxes.id.int().tolist()\n",
        "\n",
        "    # Escribir frame por cada ID detectado\n",
        "    for pid in ids:\n",
        "        info = ensure_writer_for_id(pid)\n",
        "        info[\"writer\"].write(frame)\n",
        "        info[\"frames_in_seg\"] += 1\n",
        "        info[\"last_seen\"] = frame_idx\n",
        "\n",
        "        # rotar segmento cuando llegue a CLIP_SECS\n",
        "        if info[\"frames_in_seg\"] >= clip_len_frames:\n",
        "            rotate_segment(pid)\n",
        "\n",
        "    # Cerrar escritores de IDs que no aparecieron recientemente\n",
        "    to_close = []\n",
        "    for pid, info in list(state.items()):\n",
        "        if frame_idx - info[\"last_seen\"] > idle_frames:\n",
        "            to_close.append(pid)\n",
        "    for pid in to_close:\n",
        "        close_and_remove(pid)\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "# Flush final por si quedó algo abierto\n",
        "for pid in list(state.keys()):\n",
        "    close_and_remove(pid)\n",
        "\n",
        "print(f\"Listo. Clips guardados en: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWhg5N3qyBl"
      },
      "source": [
        "## ***PRUEBA: CORRE YOLOv8 + TRACKER***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slvvHBELD1Jt",
        "outputId": "f592dfa4-7bf5-4630-98a1-dc2f19047437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.176 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8s summary (fused): 72 layers, 11,156,544 parameters, 0 gradients, 28.6 GFLOPs\n",
            "\n",
            "video 1/1 (frame 1/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 368.3ms\n",
            "video 1/1 (frame 2/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.5ms\n",
            "video 1/1 (frame 3/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.8ms\n",
            "video 1/1 (frame 4/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 443.9ms\n",
            "video 1/1 (frame 5/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 594.2ms\n",
            "video 1/1 (frame 6/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 556.7ms\n",
            "video 1/1 (frame 7/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 530.6ms\n",
            "video 1/1 (frame 8/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 543.5ms\n",
            "video 1/1 (frame 9/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 549.6ms\n",
            "video 1/1 (frame 10/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 335.9ms\n",
            "video 1/1 (frame 11/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 337.6ms\n",
            "video 1/1 (frame 12/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 342.9ms\n",
            "video 1/1 (frame 13/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 364.6ms\n",
            "video 1/1 (frame 14/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 343.7ms\n",
            "video 1/1 (frame 15/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 352.2ms\n",
            "video 1/1 (frame 16/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.4ms\n",
            "video 1/1 (frame 17/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 353.6ms\n",
            "video 1/1 (frame 18/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.0ms\n",
            "video 1/1 (frame 19/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 333.2ms\n",
            "video 1/1 (frame 20/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 370.6ms\n",
            "video 1/1 (frame 21/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 361.9ms\n",
            "video 1/1 (frame 22/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 376.6ms\n",
            "video 1/1 (frame 23/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 356.4ms\n",
            "video 1/1 (frame 24/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 338.1ms\n",
            "video 1/1 (frame 25/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 337.5ms\n",
            "video 1/1 (frame 26/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 362.5ms\n",
            "video 1/1 (frame 27/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.0ms\n",
            "video 1/1 (frame 28/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.1ms\n",
            "video 1/1 (frame 29/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.0ms\n",
            "video 1/1 (frame 30/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 413.7ms\n",
            "video 1/1 (frame 31/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 546.6ms\n",
            "video 1/1 (frame 32/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 518.6ms\n",
            "video 1/1 (frame 33/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 562.7ms\n",
            "video 1/1 (frame 34/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 520.3ms\n",
            "video 1/1 (frame 35/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 546.6ms\n",
            "video 1/1 (frame 36/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 357.1ms\n",
            "video 1/1 (frame 37/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 333.6ms\n",
            "video 1/1 (frame 38/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.7ms\n",
            "video 1/1 (frame 39/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.6ms\n",
            "video 1/1 (frame 40/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 41/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 359.5ms\n",
            "video 1/1 (frame 42/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 348.9ms\n",
            "video 1/1 (frame 43/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 352.9ms\n",
            "video 1/1 (frame 44/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.5ms\n",
            "video 1/1 (frame 45/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 397.0ms\n",
            "video 1/1 (frame 46/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.3ms\n",
            "video 1/1 (frame 47/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 349.2ms\n",
            "video 1/1 (frame 48/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 336.6ms\n",
            "video 1/1 (frame 49/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 337.0ms\n",
            "video 1/1 (frame 50/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.6ms\n",
            "video 1/1 (frame 51/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 336.9ms\n",
            "video 1/1 (frame 52/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 356.8ms\n",
            "video 1/1 (frame 53/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 338.9ms\n",
            "video 1/1 (frame 54/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 361.9ms\n",
            "video 1/1 (frame 55/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 335.9ms\n",
            "video 1/1 (frame 56/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 360.0ms\n",
            "video 1/1 (frame 57/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 513.8ms\n",
            "video 1/1 (frame 58/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 523.4ms\n",
            "video 1/1 (frame 59/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 539.5ms\n",
            "video 1/1 (frame 60/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 511.8ms\n",
            "video 1/1 (frame 61/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 575.9ms\n",
            "video 1/1 (frame 62/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 587.8ms\n",
            "video 1/1 (frame 63/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 341.7ms\n",
            "video 1/1 (frame 64/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.8ms\n",
            "video 1/1 (frame 65/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 453.0ms\n",
            "video 1/1 (frame 66/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 694.4ms\n",
            "video 1/1 (frame 67/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 1582.0ms\n",
            "video 1/1 (frame 68/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 1329.9ms\n",
            "video 1/1 (frame 69/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 519.6ms\n",
            "video 1/1 (frame 70/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 359.4ms\n",
            "video 1/1 (frame 71/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 361.5ms\n",
            "video 1/1 (frame 72/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.9ms\n",
            "video 1/1 (frame 73/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 390.0ms\n",
            "video 1/1 (frame 74/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 369.8ms\n",
            "video 1/1 (frame 75/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 417.4ms\n",
            "video 1/1 (frame 76/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 549.4ms\n",
            "video 1/1 (frame 77/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 544.6ms\n",
            "video 1/1 (frame 78/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 528.0ms\n",
            "video 1/1 (frame 79/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 529.8ms\n",
            "video 1/1 (frame 80/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 547.3ms\n",
            "video 1/1 (frame 81/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 423.5ms\n",
            "video 1/1 (frame 82/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 352.4ms\n",
            "video 1/1 (frame 83/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 331.9ms\n",
            "video 1/1 (frame 84/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 331.0ms\n",
            "video 1/1 (frame 85/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.9ms\n",
            "video 1/1 (frame 86/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 329.4ms\n",
            "video 1/1 (frame 87/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 341.5ms\n",
            "video 1/1 (frame 88/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 331.2ms\n",
            "video 1/1 (frame 89/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 344.3ms\n",
            "video 1/1 (frame 90/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.6ms\n",
            "video 1/1 (frame 91/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 417.8ms\n",
            "video 1/1 (frame 92/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 332.7ms\n",
            "video 1/1 (frame 93/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 350.6ms\n",
            "video 1/1 (frame 94/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.2ms\n",
            "video 1/1 (frame 95/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 331.3ms\n",
            "video 1/1 (frame 96/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 339.2ms\n",
            "video 1/1 (frame 97/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.5ms\n",
            "video 1/1 (frame 98/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 345.6ms\n",
            "video 1/1 (frame 99/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 334.0ms\n",
            "video 1/1 (frame 100/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 357.3ms\n",
            "video 1/1 (frame 101/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.2ms\n",
            "video 1/1 (frame 102/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 462.7ms\n",
            "video 1/1 (frame 103/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 519.5ms\n",
            "video 1/1 (frame 104/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 533.6ms\n",
            "video 1/1 (frame 105/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 551.7ms\n",
            "video 1/1 (frame 106/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 515.4ms\n",
            "video 1/1 (frame 107/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 538.8ms\n",
            "video 1/1 (frame 108/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 376.3ms\n",
            "video 1/1 (frame 109/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 344.5ms\n",
            "video 1/1 (frame 110/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 369.6ms\n",
            "video 1/1 (frame 111/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 337.4ms\n",
            "video 1/1 (frame 112/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 340.0ms\n",
            "video 1/1 (frame 113/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 339.6ms\n",
            "video 1/1 (frame 114/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 381.1ms\n",
            "video 1/1 (frame 115/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 350.7ms\n",
            "video 1/1 (frame 116/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 336.0ms\n",
            "video 1/1 (frame 117/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.9ms\n",
            "video 1/1 (frame 118/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.4ms\n",
            "video 1/1 (frame 119/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.7ms\n",
            "video 1/1 (frame 120/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 335.4ms\n",
            "video 1/1 (frame 121/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 353.7ms\n",
            "video 1/1 (frame 122/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 336.5ms\n",
            "video 1/1 (frame 123/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 335.7ms\n",
            "video 1/1 (frame 124/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 331.8ms\n",
            "video 1/1 (frame 125/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 337.5ms\n",
            "video 1/1 (frame 126/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 362.7ms\n",
            "video 1/1 (frame 127/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 339.8ms\n",
            "video 1/1 (frame 128/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 350.4ms\n",
            "video 1/1 (frame 129/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 521.5ms\n",
            "video 1/1 (frame 130/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 550.2ms\n",
            "video 1/1 (frame 131/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 536.3ms\n",
            "video 1/1 (frame 132/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 515.2ms\n",
            "video 1/1 (frame 133/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 563.0ms\n",
            "video 1/1 (frame 134/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 491.8ms\n",
            "video 1/1 (frame 135/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 327.1ms\n",
            "video 1/1 (frame 136/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 342.7ms\n",
            "video 1/1 (frame 137/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 8 persons, 366.4ms\n",
            "video 1/1 (frame 138/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 328.7ms\n",
            "video 1/1 (frame 139/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 323.6ms\n",
            "video 1/1 (frame 140/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 326.0ms\n",
            "video 1/1 (frame 141/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 347.3ms\n",
            "video 1/1 (frame 142/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 326.9ms\n",
            "video 1/1 (frame 143/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 347.4ms\n",
            "video 1/1 (frame 144/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 326.3ms\n",
            "video 1/1 (frame 145/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.5ms\n",
            "video 1/1 (frame 146/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 324.1ms\n",
            "video 1/1 (frame 147/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 334.3ms\n",
            "video 1/1 (frame 148/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 320.7ms\n",
            "video 1/1 (frame 149/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.6ms\n",
            "video 1/1 (frame 150/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 364.8ms\n",
            "video 1/1 (frame 151/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 333.3ms\n",
            "video 1/1 (frame 152/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.8ms\n",
            "video 1/1 (frame 153/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 327.6ms\n",
            "video 1/1 (frame 154/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 349.6ms\n",
            "video 1/1 (frame 155/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 330.0ms\n",
            "video 1/1 (frame 156/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 524.1ms\n",
            "video 1/1 (frame 157/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 514.1ms\n",
            "video 1/1 (frame 158/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 7 persons, 553.1ms\n",
            "video 1/1 (frame 159/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 519.4ms\n",
            "video 1/1 (frame 160/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 652.3ms\n",
            "video 1/1 (frame 161/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 506.8ms\n",
            "video 1/1 (frame 162/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 328.3ms\n",
            "video 1/1 (frame 163/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 333.0ms\n",
            "video 1/1 (frame 164/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 323.5ms\n",
            "video 1/1 (frame 165/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 346.5ms\n",
            "video 1/1 (frame 166/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 325.9ms\n",
            "video 1/1 (frame 167/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 6 persons, 344.3ms\n",
            "video 1/1 (frame 168/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.7ms\n",
            "video 1/1 (frame 169/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 345.3ms\n",
            "video 1/1 (frame 170/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 331.5ms\n",
            "video 1/1 (frame 171/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 5 persons, 326.1ms\n",
            "video 1/1 (frame 172/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 324.6ms\n",
            "video 1/1 (frame 173/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.3ms\n",
            "video 1/1 (frame 174/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.6ms\n",
            "video 1/1 (frame 175/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.4ms\n",
            "video 1/1 (frame 176/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.9ms\n",
            "video 1/1 (frame 177/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 323.8ms\n",
            "video 1/1 (frame 178/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 351.3ms\n",
            "video 1/1 (frame 179/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.2ms\n",
            "video 1/1 (frame 180/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.2ms\n",
            "video 1/1 (frame 181/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.9ms\n",
            "video 1/1 (frame 182/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.6ms\n",
            "video 1/1 (frame 183/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 601.9ms\n",
            "video 1/1 (frame 184/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 531.5ms\n",
            "video 1/1 (frame 185/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 528.3ms\n",
            "video 1/1 (frame 186/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 557.1ms\n",
            "video 1/1 (frame 187/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 580.6ms\n",
            "video 1/1 (frame 188/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 590.1ms\n",
            "video 1/1 (frame 189/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 606.5ms\n",
            "video 1/1 (frame 190/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 550.4ms\n",
            "video 1/1 (frame 191/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 519.4ms\n",
            "video 1/1 (frame 192/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 531.7ms\n",
            "video 1/1 (frame 193/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 460.5ms\n",
            "video 1/1 (frame 194/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 331.5ms\n",
            "video 1/1 (frame 195/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.5ms\n",
            "video 1/1 (frame 196/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.5ms\n",
            "video 1/1 (frame 197/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.1ms\n",
            "video 1/1 (frame 198/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 330.8ms\n",
            "video 1/1 (frame 199/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.4ms\n",
            "video 1/1 (frame 200/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 344.0ms\n",
            "video 1/1 (frame 201/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.5ms\n",
            "video 1/1 (frame 202/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.4ms\n",
            "video 1/1 (frame 203/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.4ms\n",
            "video 1/1 (frame 204/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 331.8ms\n",
            "video 1/1 (frame 205/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 336.6ms\n",
            "video 1/1 (frame 206/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 375.9ms\n",
            "video 1/1 (frame 207/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 343.8ms\n",
            "video 1/1 (frame 208/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.5ms\n",
            "video 1/1 (frame 209/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 346.7ms\n",
            "video 1/1 (frame 210/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 513.0ms\n",
            "video 1/1 (frame 211/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 513.9ms\n",
            "video 1/1 (frame 212/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 539.3ms\n",
            "video 1/1 (frame 213/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 516.6ms\n",
            "video 1/1 (frame 214/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 558.6ms\n",
            "video 1/1 (frame 215/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 410.2ms\n",
            "video 1/1 (frame 216/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 328.7ms\n",
            "video 1/1 (frame 217/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.4ms\n",
            "video 1/1 (frame 218/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.0ms\n",
            "video 1/1 (frame 219/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.9ms\n",
            "video 1/1 (frame 220/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 351.7ms\n",
            "video 1/1 (frame 221/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 323.4ms\n",
            "video 1/1 (frame 222/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 344.7ms\n",
            "video 1/1 (frame 223/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 325.0ms\n",
            "video 1/1 (frame 224/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 2 persons, 326.8ms\n",
            "video 1/1 (frame 225/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.0ms\n",
            "video 1/1 (frame 226/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 334.3ms\n",
            "video 1/1 (frame 227/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.0ms\n",
            "video 1/1 (frame 228/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.6ms\n",
            "video 1/1 (frame 229/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 389.5ms\n",
            "video 1/1 (frame 230/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 231/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.7ms\n",
            "video 1/1 (frame 232/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.7ms\n",
            "video 1/1 (frame 233/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.1ms\n",
            "video 1/1 (frame 234/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 324.3ms\n",
            "video 1/1 (frame 235/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 333.1ms\n",
            "video 1/1 (frame 236/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 339.8ms\n",
            "video 1/1 (frame 237/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 521.5ms\n",
            "video 1/1 (frame 238/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 536.8ms\n",
            "video 1/1 (frame 239/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 524.3ms\n",
            "video 1/1 (frame 240/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 510.1ms\n",
            "video 1/1 (frame 241/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 521.0ms\n",
            "video 1/1 (frame 242/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 453.9ms\n",
            "video 1/1 (frame 243/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.5ms\n",
            "video 1/1 (frame 244/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 353.4ms\n",
            "video 1/1 (frame 245/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.3ms\n",
            "video 1/1 (frame 246/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.2ms\n",
            "video 1/1 (frame 247/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.0ms\n",
            "video 1/1 (frame 248/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.2ms\n",
            "video 1/1 (frame 249/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.5ms\n",
            "video 1/1 (frame 250/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.8ms\n",
            "video 1/1 (frame 251/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 342.2ms\n",
            "video 1/1 (frame 252/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 370.6ms\n",
            "video 1/1 (frame 253/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 350.8ms\n",
            "video 1/1 (frame 254/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 339.1ms\n",
            "video 1/1 (frame 255/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 329.6ms\n",
            "video 1/1 (frame 256/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.3ms\n",
            "video 1/1 (frame 257/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.4ms\n",
            "video 1/1 (frame 258/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 347.5ms\n",
            "video 1/1 (frame 259/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 325.0ms\n",
            "video 1/1 (frame 260/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 350.6ms\n",
            "video 1/1 (frame 261/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 322.4ms\n",
            "video 1/1 (frame 262/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 346.1ms\n",
            "video 1/1 (frame 263/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 324.6ms\n",
            "video 1/1 (frame 264/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 578.9ms\n",
            "video 1/1 (frame 265/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 522.6ms\n",
            "video 1/1 (frame 266/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 544.7ms\n",
            "video 1/1 (frame 267/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 538.5ms\n",
            "video 1/1 (frame 268/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 534.1ms\n",
            "video 1/1 (frame 269/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 430.8ms\n",
            "video 1/1 (frame 270/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 4 persons, 340.1ms\n",
            "video 1/1 (frame 271/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 343.2ms\n",
            "video 1/1 (frame 272/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 273/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 349.4ms\n",
            "video 1/1 (frame 274/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 332.5ms\n",
            "video 1/1 (frame 275/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 366.7ms\n",
            "video 1/1 (frame 276/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 327.8ms\n",
            "video 1/1 (frame 277/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.1ms\n",
            "video 1/1 (frame 278/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 345.2ms\n",
            "video 1/1 (frame 279/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 326.3ms\n",
            "video 1/1 (frame 280/280) /content/tesisV2/videos/supermas1_2.mp4: 384x640 3 persons, 340.1ms\n",
            "Speed: 4.9ms preprocess, 399.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/tesisV2/outputs/boxed2\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/track\n"
          ]
        }
      ],
      "source": [
        "!yolo track model=yolov8s.pt source=\"/content/tesisV2/videos/supermas1_2.mp4\" classes=0 conf=0.25 save=True project=\"/content/tesisV2/outputs\" name=\"boxed\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyahByY7v3mB"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.43.3 timm==1.0.7 pytorchvideo==0.1.5 accelerate evaluate decord\n",
        "import os, pandas as pd, torch, decord\n",
        "from datasets import Dataset\n",
        "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
        "decord.bridge.set_bridge('torch')\n",
        "\n",
        "BASE = \"/content/cortos/video_clips_224\"  # el mismo OUT_DIR\n",
        "label2id={\"normal\":0,\"shoplifting\":1}; id2label={v:k for k,v in label2id.items()}\n",
        "\n",
        "def load_csv(name):\n",
        "    df=pd.read_csv(os.path.join(BASE,f\"{name}.csv\"),header=None,names=[\"path\",\"label\"])\n",
        "    return Dataset.from_pandas(df.assign(label=df[\"label\"].map(label2id)))\n",
        "\n",
        "def load_video_torch(path,num_frames=16):\n",
        "    vr=decord.VideoReader(path)\n",
        "    import torch\n",
        "    idx=torch.linspace(0,len(vr)-1,steps=num_frames).long()\n",
        "    return vr.get_batch(idx).permute(0,3,1,2)\n",
        "\n",
        "train_ds, val_ds = load_csv(\"train\"), load_csv(\"val\")\n",
        "processor=AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "model=VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\", num_labels=2, label2id=label2id, id2label=id2label\n",
        ")\n",
        "\n",
        "def preprocess(ex):\n",
        "    frames=load_video_torch(ex[\"path\"],num_frames=16)\n",
        "    ex[\"pixel_values\"]=processor(list(frames),return_tensors=\"pt\")[\"pixel_values\"][0]\n",
        "    return ex\n",
        "\n",
        "train_ds=train_ds.map(preprocess); val_ds=val_ds.map(preprocess)\n",
        "def collate(b):\n",
        "    return {\"pixel_values\":torch.stack([x[\"pixel_values\"] for x in b]),\n",
        "            \"labels\":torch.tensor([x[\"label\"] for x in b])}\n",
        "\n",
        "args=TrainingArguments(output_dir=\"/content/videomae_shoplift\",\n",
        "    per_device_train_batch_size=4, per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2, fp16=True, learning_rate=5e-5,\n",
        "    num_train_epochs=5, evaluation_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    logging_steps=20, report_to=\"none\")\n",
        "\n",
        "trainer=Trainer(model=model,args=args,train_dataset=train_ds,eval_dataset=val_ds,data_collator=collate)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jfOQgmS6szQ"
      },
      "source": [
        "## ***CREACION DE CLIPS NORMALES (SOLO CON PERSONAS)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbTHnl6jFRkd",
        "outputId": "b5e9bdb8-fc68-446c-bf82-c38abed0678f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: CPU\n",
            "POS existentes: 9834\n",
            "Normales a procesar (videos): 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Segmentando NEG: 100%|██████████| 150/150 [4:06:24<00:00, 98.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEG brutos nuevos: 2989  | TOTAL NEG brutos: 3681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filtrando NEG (person): 100%|██████████| 3681/3681 [1:01:24<00:00,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEG con persona: 2391\n",
            "Balance final -> POS: 9834  NEG: 2391\n",
            "train -> 10668\n",
            "val -> 787\n",
            "test -> 770\n",
            "CSV y clips listos en: /content/tesisV2/cortos/video_clips_192\n",
            "Espejo en Drive: /content/drive/MyDrive/tesisV2/cortos/video_clips_192\n"
          ]
        }
      ],
      "source": [
        "# ================= SOLO NEGATIVOS (masivos) + CACHE + ESPEJO A DRIVE =================\n",
        "import os, glob, random, subprocess, cv2, collections, numpy as np, json, time, torch\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ---------- PARÁMETROS RÁPIDOS ----------\n",
        "SRC_DIR = \"/content/tesisV2/Datasets\"                      # normal en: Training_/Testing_*\n",
        "OUT_DIR = \"/content/tesisV2/cortos/video_clips_192\"        # donde ya están tus POS\n",
        "DRIVE_MIRROR = \"/content/drive/MyDrive/tesisV2/cortos/video_clips_192\"  # espejo\n",
        "N_NEG_VIDEOS = 150     # ← subí esto para más negativos (antes 50/100/120). Poné 150–300 si querés muchos.\n",
        "HOP_NEG = 8.0          # ← bajá a 6.0 si querés aún más clips por video\n",
        "SIZE, FPS = 192, 12\n",
        "FFMPEG_PRESET = \"veryfast\"\n",
        "\n",
        "PERSON_CONF = 0.25\n",
        "NEG_PERSON_THR = 0.40\n",
        "MAX_FRAMES = 60\n",
        "VID_STRIDE = 3\n",
        "IMGSZ = 384\n",
        "DEVICE = 0 if (torch.cuda.is_available() and torch.cuda.device_count()>0) else \"cpu\"\n",
        "SYNC_EVERY = 10        # copia a Drive cada X videos segmentados\n",
        "\n",
        "print(\"Device:\", \"GPU\" if DEVICE==0 else \"CPU\")\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "POS_DIR = os.path.join(OUT_DIR, \"shoplifting\")\n",
        "NEG_DIR = os.path.join(OUT_DIR, \"normal\")\n",
        "CACHE_DIR = os.path.join(OUT_DIR, \"_cache_pr\")\n",
        "os.makedirs(NEG_DIR, exist_ok=True)\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_MIRROR, exist_ok=True)\n",
        "\n",
        "# ---------- FUNCIONES ----------\n",
        "def list_videos(folder):\n",
        "    vids=[]; exts=(\"*.mp4\",\"*.avi\",\"*.mov\",\"*.mkv\",\"*.MP4\",\"*.AVI\",\"*.MOV\",\"*.MKV\")\n",
        "    for e in exts: vids += glob.glob(os.path.join(folder, \"**\", e), recursive=True)\n",
        "    return vids\n",
        "\n",
        "def segment_to_clips_resume(src_path, out_dir, clip_s=3, hop_s=8.0, size=192, fps=12, preset=\"veryfast\"):\n",
        "    cap=cv2.VideoCapture(src_path)\n",
        "    if not cap.isOpened(): return []\n",
        "    vfps=cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    frames=int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    if frames<=0: return []\n",
        "    dur=frames/vfps\n",
        "    starts=[]; t=0.0\n",
        "    while t+clip_s<=dur: starts.append(round(t,3)); t+=hop_s\n",
        "    base=os.path.splitext(os.path.basename(src_path))[0]\n",
        "    outs=[]\n",
        "    for ss in starts:\n",
        "        outp=os.path.join(out_dir, f\"{base}_ss{int(ss*1000):07d}.mp4\")\n",
        "        if os.path.exists(outp):\n",
        "            outs.append(outp); continue\n",
        "        cmd=[\"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n",
        "             \"-ss\", f\"{ss:.3f}\", \"-t\", str(clip_s), \"-i\", src_path,\n",
        "             \"-vf\", f\"scale={size}:{size}:force_original_aspect_ratio=decrease,pad={size}:{size}:(ow-iw)/2:(oh-ih)/2,format=yuv420p,fps={fps}\",\n",
        "             \"-an\",\"-r\", str(fps), \"-preset\", preset, outp]\n",
        "        subprocess.run(cmd)\n",
        "        if os.path.exists(outp): outs.append(outp)\n",
        "    return outs\n",
        "\n",
        "def _pr_path(p):\n",
        "    return os.path.join(CACHE_DIR, os.path.basename(p)+\".pr.json\")\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "def person_ratio_cached(path, conf=PERSON_CONF, max_frames=MAX_FRAMES, vid_stride=VID_STRIDE, imgsz=IMGSZ):\n",
        "    c=_pr_path(path)\n",
        "    if os.path.exists(c):\n",
        "        try: return json.load(open(c))[\"pr\"]\n",
        "        except: pass\n",
        "    det=tot=0\n",
        "    gen = model.predict(source=path, classes=[0], conf=conf, stream=True, verbose=False,\n",
        "                        vid_stride=vid_stride, imgsz=imgsz, device=DEVICE)\n",
        "    for r in gen:\n",
        "        tot += 1\n",
        "        if r.boxes is not None and len(r.boxes)>0:\n",
        "            det += 1\n",
        "        if tot >= max_frames:\n",
        "            break\n",
        "    pr = (det/tot) if tot else 0.0\n",
        "    try: json.dump({\"pr\":pr}, open(c,\"w\"))\n",
        "    except: pass\n",
        "    return pr\n",
        "\n",
        "def rsync_to_drive():\n",
        "    # copia incremental, no pisa lo existente\n",
        "    os.system(f'rsync -a --ignore-existing \"{OUT_DIR}/\" \"{DRIVE_MIRROR}/\"')\n",
        "\n",
        "# ---------- POS EXISTENTES ----------\n",
        "pos_clips = sorted(glob.glob(os.path.join(POS_DIR, \"*.mp4\")))\n",
        "print(\"POS existentes:\", len(pos_clips))\n",
        "\n",
        "# ---------- ELEGIR NORMALES ----------\n",
        "train_norm = list_videos(os.path.join(SRC_DIR, \"Training_Normal_Videos_Anomaly\"))\n",
        "test_norm  = list_videos(os.path.join(SRC_DIR, \"Testing_Normal_Videos_Anomaly\"))\n",
        "normal_pool = train_norm + test_norm\n",
        "random.shuffle(normal_pool)\n",
        "normal_pool = normal_pool[:N_NEG_VIDEOS]\n",
        "print(\"Normales a procesar (videos):\", len(normal_pool))\n",
        "\n",
        "# ---------- SEGMENTAR + SYNC POR LOTES ----------\n",
        "neg_raw_before = len(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "processed = 0\n",
        "for v in tqdm(normal_pool, desc=\"Segmentando NEG\"):\n",
        "    _ = segment_to_clips_resume(v, NEG_DIR, 3, HOP_NEG, SIZE, FPS, FFMPEG_PRESET)\n",
        "    processed += 1\n",
        "    if processed % SYNC_EVERY == 0:\n",
        "        rsync_to_drive()\n",
        "neg_raw_after = len(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "print(\"NEG brutos nuevos:\", neg_raw_after - neg_raw_before, \" | TOTAL NEG brutos:\", neg_raw_after)\n",
        "\n",
        "# ---------- FILTRAR PERSONA (usa cache / reanuda) ----------\n",
        "neg_all = sorted(glob.glob(os.path.join(NEG_DIR, \"*.mp4\")))\n",
        "neg_kept = []\n",
        "for p in tqdm(neg_all, desc=\"Filtrando NEG (person)\"):\n",
        "    try:\n",
        "        if person_ratio_cached(p) >= NEG_PERSON_THR:\n",
        "            neg_kept.append(p)\n",
        "        else:\n",
        "            os.remove(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"NEG con persona:\", len(neg_kept))\n",
        "\n",
        "# ---------- BALANCE FINAL vs POS (1.5x) ----------\n",
        "target_neg = int(len(pos_clips) * 1.5)\n",
        "random.shuffle(neg_kept)\n",
        "if len(neg_kept) > target_neg:\n",
        "    for p in neg_kept[target_neg:]:\n",
        "        try: os.remove(p)\n",
        "        except: pass\n",
        "    neg_kept = neg_kept[:target_neg]\n",
        "print(f\"Balance final -> POS: {len(pos_clips)}  NEG: {len(neg_kept)}\")\n",
        "\n",
        "# ---------- CSVs (split por video, sin fuga) ----------\n",
        "def base_from_clip(p): return os.path.basename(p).split(\"_ss\")[0]\n",
        "def group_by_video(clips):\n",
        "    d=collections.defaultdict(list)\n",
        "    for p in clips: d[base_from_clip(p)].append(p)\n",
        "    return list(d.values())\n",
        "def split_groups(groups, r=(0.8,0.1,0.1)):\n",
        "    n=len(groups); a=int(r[0]*n); b=int((r[0]+r[1])*n)\n",
        "    return groups[:a], groups[a:b], groups[b:]\n",
        "def flatten(gs):\n",
        "    out=[]; [out.extend(g) for g in gs]; return out\n",
        "\n",
        "pos_groups = group_by_video(pos_clips)\n",
        "neg_groups = group_by_video(neg_kept)\n",
        "random.shuffle(pos_groups); random.shuffle(neg_groups)\n",
        "p_tr,p_va,p_te = split_groups(pos_groups)\n",
        "n_tr,n_va,n_te = split_groups(neg_groups)\n",
        "\n",
        "splits = {\n",
        "  \"train\": [(p,\"shoplifting\") for p in flatten(p_tr)] + [(p,\"normal\") for p in flatten(n_tr)],\n",
        "  \"val\":   [(p,\"shoplifting\") for p in flatten(p_va)] + [(p,\"normal\") for p in flatten(n_va)],\n",
        "  \"test\":  [(p,\"shoplifting\") for p in flatten(p_te)] + [(p,\"normal\") for p in flatten(n_te)],\n",
        "}\n",
        "for name,items in splits.items():\n",
        "    random.shuffle(items)\n",
        "    with open(os.path.join(OUT_DIR,f\"{name}.csv\"),\"w\") as f:\n",
        "        for p,c in items: f.write(f\"{p},{c}\\n\")\n",
        "    print(name, \"->\", len(items))\n",
        "\n",
        "# sync final\n",
        "rsync_to_drive()\n",
        "print(\"CSV y clips listos en:\", OUT_DIR)\n",
        "print(\"Espejo en Drive:\", DRIVE_MIRROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9LYZ4072Hz"
      },
      "source": [
        "## ***PRUEBA: JSON CON LA PROBABILIDAD DE HURTO. MP4 CON BARRAS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQoaHYfPhL13",
        "outputId": "5468e48e-79f6-4d1e-866f-945c5e24659d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === INFERENCIA en video largo (usa checkpoint linear probe) ===\n",
        "import os, json, cv2, math, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "\n",
        "# RUTAS\n",
        "MODEL_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\"  # <- el que guardó el training\n",
        "VIDEO_IN   = \"/content/tesisV2/videos/supermas1-1.mp4\"                                 # <-- cambia a tu video\n",
        "OUT_JSON   = \"/content/tesisV2/preds_supermas1-1.json\"\n",
        "OUT_VIS    = \"/content/tesisV2/preds_supermas1-1_vis.mp4\"\n",
        "\n",
        "# Debe matchear lo que usaste en training rápido\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "\n",
        "# Ventana y hop (en segundos)\n",
        "WIN_SEC = 3.0\n",
        "HOP_SEC = 1.0\n",
        "THRESH  = 0.55  # umbral para marcar \"alarma\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Modelo igual que en training (congelado, misma capa final)\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "def clip_to_tensor(frames_rgb):\n",
        "    x = np.stack(frames_rgb, axis=0).astype(np.float32)/255.0  # T,H,W,C\n",
        "    x = torch.from_numpy(x).permute(3,0,1,2).to(device)        # C,T,H,W\n",
        "    x = (x - mean) / std\n",
        "    return x.unsqueeze(0)  # B=1\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(f\"Video: {n} frames, {fps:.1f} fps\")\n",
        "\n",
        "win = int(WIN_SEC*fps); hop = int(HOP_SEC*fps)\n",
        "scores = []  # (t0,t1,p_shop)\n",
        "\n",
        "for start in range(0, max(1, n - win + 1), hop):\n",
        "    idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    x = clip_to_tensor(frames)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]\n",
        "        p_shop = torch.softmax(logits, dim=0)[1].item()\n",
        "    t0 = start/fps; t1 = (start+win)/fps\n",
        "    scores.append((t0, t1, p_shop))\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Suavizado (media móvil simple para reducir ruido)\n",
        "win_smooth = 3\n",
        "probs = np.array([p for _,_,p in scores], dtype=np.float32)\n",
        "if len(probs) >= win_smooth:\n",
        "    kernel = np.ones(win_smooth)/win_smooth\n",
        "    probs_s = np.convolve(probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = probs\n",
        "\n",
        "# Guardar JSON\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([{\"t0\":float(a),\"t1\":float(b),\"p\":float(p),\"p_smooth\":float(ps)}\n",
        "               for (a,b,_), ps in zip(scores, probs_s)], f, indent=2)\n",
        "print(\"Guardado:\", OUT_JSON)\n",
        "\n",
        "# Render con barra de probabilidad\n",
        "cap = cv2.VideoCapture(VIDEO_IN)\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(OUT_VIS, fourcc, fps, (w,h))\n",
        "bar_h = 18\n",
        "\n",
        "# Expande probs_smooth por frame para pintar\n",
        "score_by_frame = np.zeros(n, dtype=np.float32)\n",
        "for i, (t0,t1,_) in enumerate(scores):\n",
        "    a = int(t0*fps); b = min(n-1, int(t1*fps))\n",
        "    score_by_frame[a:b+1] = max(0.0, min(1.0, probs_s[i]))\n",
        "\n",
        "i=0\n",
        "while True:\n",
        "    ok, fr = cap.read()\n",
        "    if not ok: break\n",
        "    p = float(score_by_frame[i]) if i < len(score_by_frame) else 0.0\n",
        "    bw = int(p * w)\n",
        "    color = (0,0,255) if p >= THRESH else (0,255,0)\n",
        "    cv2.rectangle(fr, (0,0), (bw, bar_h), color, -1)\n",
        "    cv2.putText(fr, f\"shoplifting prob: {p:.2f}\", (10, bar_h+22),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "    out.write(fr); i+=1\n",
        "\n",
        "cap.release(); out.release()\n",
        "print(\"Video con barra:\", OUT_VIS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7CcPF0JmQLtr",
        "outputId": "d07ddb6f-2553-4237-a0da-fbc7eb6dcb39"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'R3D_18_Weights'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2401321534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr3d_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR3D_18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKINETICS400_V1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# dummy para que no binde? (ignorado)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr3d_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR3D_18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKINETICS400_V1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'R3D_18_Weights'"
          ]
        }
      ],
      "source": [
        "# ===== Elegir umbral óptimo en VAL =====\n",
        "import os, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV  = os.path.join(OUT_DIR, \"val.csv\")\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 8, 96\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[];\n",
        "    for line in open(p):\n",
        "        path,lab = line.strip().split(\",\")\n",
        "        items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "val_items = read_csv(VAL_CSV)\n",
        "\n",
        "# Modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "weights = r3d_18(weights=r3d_18.R3D_18_Weights.KINETICS400_V1)\n",
        "weights.fc = None  # dummy para que no binde? (ignorado)\n",
        "model = r3d_18(weights=r3d_18.R3D_18_Weights.KINETICS400_V1)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "import cv2\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path); n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None: fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2).to(device)\n",
        "    return ((x-mean)/std).unsqueeze(0)\n",
        "\n",
        "# Barrido de thresholds\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "probs=[]; ys=[]\n",
        "with torch.no_grad():\n",
        "    for p,y in val_items:\n",
        "        x = clip_tensor(p)\n",
        "        s = torch.softmax(model(x)[0], dim=0)[1].item()  # prob shoplifting\n",
        "        probs.append(s); ys.append(y)\n",
        "probs = np.array(probs); ys = np.array(ys)\n",
        "\n",
        "best = (0.0, -1.0, 0,0,0)  # thr, metric, p,r,f1\n",
        "for thr in np.linspace(0.3, 0.9, 25):\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    p,r,f1,_ = precision_recall_fscore_support(ys, preds, average='binary')  # binario = clase 1 (shoplifting)\n",
        "    # Ej: max F1 de shoplifting (cambia a 'p' si quieres max precision)\n",
        "    score = f1\n",
        "    if score > best[1]:\n",
        "        best = (thr, score, p, r, f1)\n",
        "print(f\"Umbral óptimo(F1 shop): {best[0]:.2f}  | P:{best[2]:.3f} R:{best[3]:.3f} F1:{best[4]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaEj4NPF9cNH"
      },
      "source": [
        "## ***ENTRENAMIENTO RAPIDO CON CPU***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNC9CDpfxeJ",
        "outputId": "3cb1ad18-3020-4e19-ea4a-02979ddf6068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "Epoch 01/5 | train 0.2054/0.924 | val 0.5633/0.804 | 105.4 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 02/5 | train 0.1344/0.952 | val 0.6471/0.809 | 85.1 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 03/5 | train 0.1187/0.957 | val 0.7793/0.816 | 84.8 min\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n",
            "Epoch 04/5 | train 0.1041/0.964 | val 1.1228/0.748 | 85.2 min\n",
            "Epoch 05/5 | train 0.1175/0.959 | val 1.0136/0.773 | 85.7 min\n",
            "[[ 85 106]\n",
            " [181 398]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.32      0.45      0.37       191\n",
            " shoplifting       0.79      0.69      0.73       579\n",
            "\n",
            "    accuracy                           0.63       770\n",
            "   macro avg       0.55      0.57      0.55       770\n",
            "weighted avg       0.67      0.63      0.64       770\n",
            "\n",
            "Best ckpt: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_linearprobe.pt\n"
          ]
        }
      ],
      "source": [
        "# === LINEAR PROBE en CPU (rápido) ===\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# Rutas\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"        # contiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR= \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Hiperparámetros (ligeros para CPU)\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "BATCH      = 8             # si se queda corto de RAM, baja a 4\n",
        "EPOCHS     = 5             # rápido; luego puedes subir\n",
        "LR_HEAD    = 1e-3          # LR más alto para la capa final\n",
        "NUM_WORKERS= 0             # CPU en Colab → 0\n",
        "\n",
        "# CSVs\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "# (Opcional) Submuestreo para ir aún más rápido en la primera corrida\n",
        "SUBSAMPLE_TRAIN = None  # p.ej. 6000; déjalo en None para usar todo\n",
        "if SUBSAMPLE_TRAIN:\n",
        "    random.shuffle(train_items); train_items = train_items[:SUBSAMPLE_TRAIN]\n",
        "\n",
        "# -------- utilidades dataset con cache de frames --------\n",
        "def sample_frame_indices(n_total, n_sample):\n",
        "    if n_total <= n_sample:\n",
        "        return np.linspace(0, max(0, n_total-1), num=n_sample, dtype=int)\n",
        "    return np.linspace(0, n_total-1, num=n_sample, dtype=int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)  # (T,H,W,C)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE)       # (T,H,W,C) uint8\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "        if self.train and random.random() < 0.1:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(ClipDataset(train_items, True),  batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(ClipDataset(val_items,   False), batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader  = DataLoader(ClipDataset(test_items,  False), batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# -------- modelo: R3D-18 congelado (solo entrenamos la capa final) --------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "\n",
        "# congelar todo\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# reemplazar y entrenar solo la capa final\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.fc.parameters(), lr=LR_HEAD, weight_decay=0.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.train(False)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1); correct += int((pred==y).sum())\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def train_epoch(loader):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1); correct += int((pred==y).sum())\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "best_val = 0.0\n",
        "best_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_linearprobe.pt\")\n",
        "for e in range(1, EPOCHS+1):\n",
        "    t0=time.time()\n",
        "    tr_loss, tr_acc = train_epoch(train_loader)\n",
        "    val_loss, val_acc = eval_epoch(val_loader)\n",
        "    dt = time.time()-t0\n",
        "    print(f\"Epoch {e:02d}/{EPOCHS} | train {tr_loss:.4f}/{tr_acc:.3f} | val {val_loss:.4f}/{val_acc:.3f} | {dt/60:.1f} min\")\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, best_path)\n",
        "        print(\"✔️ guardado mejor:\", best_path)\n",
        "    gc.collect()\n",
        "\n",
        "# TEST final\n",
        "ckpt = torch.load(best_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", best_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "LjvWwlb5-UsE",
        "outputId": "3ef82201-eb98-46c8-98a8-ac89a95483a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:03<00:00, 41.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Fase 1: Linear probe (solo fc) | epochs=3 batch=8 lr=0.001 =====\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.830 P_norm=0.325\n",
            "Ep 01/3 | train 0.2160/0.931 | val 0.5774/0.740 | P_norm 0.325 R_norm 0.686 F1_norm 0.441 | P_shop 0.931 R_shop 0.749 F1_shop 0.830 | 380.3 min\n",
            "Ep 02/3 | train 0.1388/0.956 | val 1.0044/0.642 | P_norm 0.273 R_norm 0.839 F1_norm 0.412 | P_shop 0.955 R_shop 0.607 F1_shop 0.742 | 293.3 min\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# ----------------- FASES -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# Fase 1: solo cabeza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mbest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 1: Linear probe (solo fc)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_FC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# Fase 2: descongelar layer4 (+ fc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36mphase\u001b[0;34m(name, unfreeze_parts, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2017141767.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, optimizer)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases (con pesos de clase) =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# Tamaño de clip\n",
        "NUM_FRAMES = 16      # (mejor que 8)\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Batch/épocas por fase (ajustá si CPU/GPU)\n",
        "BATCH_LINEAR = 8\n",
        "BATCH_FT     = 6\n",
        "EPOCHS_LINEAR = 3    # fase 1: solo fc\n",
        "EPOCHS_L4     = 5    # fase 2: descongelar layer4\n",
        "EPOCHS_L34    = 0    # fase 3 opcional: layer3+layer4 (poné 3-5 si querés)\n",
        "\n",
        "# Pesos de clase para bajar FP (más peso a \"normal\")\n",
        "W_NORMAL = 1.6\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "LR_FC   = 1e-3       # capa final\n",
        "LR_L4   = 3e-4       # fine-tune layer4\n",
        "LR_L34  = 2e-4       # fine-tune layer3+4\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    # muestreo uniforme con pequeño jitter temporal\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        # Augment simple\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train):\n",
        "    return DataLoader(ClipDataset(items, train),\n",
        "                      batch_size=batch, shuffle=train,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "\n",
        "# Reemplazamos la cabeza\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Pérdida con pesos de clase (más peso a 'normal' para bajar FP)\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers de entrenamiento\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])  # [normal, shop]\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\");\n",
        "        return None\n",
        "    # congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # descongelar lo que toque\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "    # optim solo sobre params entrenables\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "    best_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(train_loader, optimizer)\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "        # criterio de guardado: F1_shop, y desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, best_path)\n",
        "            print(f\"✔️ guardado mejor: {best_path} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return best_path, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "# Fase 1: solo cabeza\n",
        "best1 = phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "\n",
        "# Fase 2: descongelar layer4 (+ fc)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "\n",
        "# Fase 3 (opcional): layer3 + layer4 (+ fc)\n",
        "if EPOCHS_L34 > 0:\n",
        "    parts = [model.layer3, model.layer4, model.fc]\n",
        "    best3 = phase(\"Fase 3: Fine-tune layer3+4 (opcional)\", parts, EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt_path = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_loader(test_items, batch_size=8, train=False)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", ckpt_path)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ASikNR9qN68c",
        "outputId": "c444a4cc-b88e-4e3e-fe97-78dd9f9fc50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "[Fase 1: Linear probe (solo fc)] saltado (epochs=0)\n",
            "↪️  [Fase 2: Fine-tune layer4] Arranco desde el mejor checkpoint previo.\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 | epochs=5 batch=6 lr=0.0003 =====\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0mbest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 1: Linear probe (solo fc)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_LINEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_FC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m \u001b[0mbest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fase 2: Fine-tune layer4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_L4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_FT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR_L4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mEPOCHS_L34\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36mphase\u001b[0;34m(name, unfreeze_parts, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         tr_loss, tr_acc = train_epoch(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_EVERY_BATCHES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mSHORT_SESSION\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1647014803.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, optimizer, save_every, max_batches)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases (con pesos de clase) [RESUME + SESIÓN CORTA + BEST/LATEST] =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Checkpoints\n",
        "CKPT_BEST   = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.best.pt\")\n",
        "CKPT_LATEST = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.latest.pt\")\n",
        "\n",
        "# Tamaño de clip (ajusta si vas en CPU)\n",
        "NUM_FRAMES = 16      # en CPU: puedes bajar a 8\n",
        "IMG_SIZE   = 112     # en CPU: puedes bajar a 96\n",
        "\n",
        "# Batch/épocas por fase (ajusta si CPU/GPU)\n",
        "BATCH_LINEAR  = 8\n",
        "BATCH_FT      = 6\n",
        "EPOCHS_LINEAR = 0    # fase 1: solo fc  (0 = saltar)\n",
        "EPOCHS_L4     = 5    # fase 2: descongelar layer4\n",
        "EPOCHS_L34    = 0    # fase 3 opcional: layer3+layer4\n",
        "\n",
        "# Pesos de clase para bajar FP (más peso a \"normal\")\n",
        "W_NORMAL = 1.6\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "LR_FC   = 1e-3       # capa final\n",
        "LR_L4   = 3e-4       # fine-tune layer4\n",
        "LR_L34  = 2e-4       # fine-tune layer3+4\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# --- SESIÓN CORTA (para que no se caiga en Colab) ---\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300    # procesa solo N batches por época (ajusta)\n",
        "SAVE_EVERY_BATCHES  = 150    # guarda intermedio cada N batches\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train):\n",
        "    return DataLoader(ClipDataset(items, train),\n",
        "                      batch_size=batch, shuffle=train,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# >>> RESUME GLOBAL: preferir LATEST, si no existe usar BEST\n",
        "resume_from = CKPT_LATEST if os.path.exists(CKPT_LATEST) else (CKPT_BEST if os.path.exists(CKPT_BEST) else None)\n",
        "if resume_from:\n",
        "    try:\n",
        "        ckpt = torch.load(resume_from, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {resume_from}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {resume_from}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# Pérdida con pesos de clase (más peso a 'normal')\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers de entrenamiento\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])  # [normal, shop]\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def _save_latest(meta):\n",
        "    try:\n",
        "        torch.save({\"model\": model.state_dict(), \"classes\": CLASSES, \"meta\": meta}, CKPT_LATEST)\n",
        "        # print(f\"💾 Guardado latest -> {CKPT_LATEST}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ No pude guardar LATEST:\", e)\n",
        "\n",
        "def _save_best(meta):\n",
        "    try:\n",
        "        torch.save({\"model\": model.state_dict(), \"classes\": CLASSES, \"meta\": meta}, CKPT_BEST)\n",
        "        print(f\"✔️ guardado mejor: {CKPT_BEST} | F1_shop={meta['F1_shop']:.3f} P_norm={meta['P_norm']:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ No pude guardar BEST:\", e)\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None, phase_name=\"\"):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i, (x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            meta = {\"phase\": phase_name, \"epoch\": None, \"batch\": i}\n",
        "            _save_latest(meta)\n",
        "\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "\n",
        "    # Congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # Descongelar lo que toque\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "\n",
        "    # Re-cargar desde LAST/BEST al entrar en la fase (por si hubo corte)\n",
        "    resume_from = CKPT_LATEST if os.path.exists(CKPT_LATEST) else (CKPT_BEST if os.path.exists(CKPT_BEST) else None)\n",
        "    if resume_from:\n",
        "        try:\n",
        "            ckpt = torch.load(resume_from, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde checkpoint: {resume_from}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {resume_from}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None),\n",
        "            phase_name=name\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "\n",
        "        # Guardado LATEST por-época con meta completa\n",
        "        meta = {\n",
        "            \"phase\": name, \"epoch\": e,\n",
        "            \"val_loss\": val_loss, \"val_acc\": val_acc,\n",
        "            \"P_norm\": m[\"P_norm\"], \"R_norm\": m[\"R_norm\"], \"F1_norm\": m[\"F1_norm\"],\n",
        "            \"P_shop\": m[\"P_shop\"], \"R_shop\": m[\"R_shop\"], \"F1_shop\": m[\"F1_shop\"],\n",
        "            \"lr\": lr, \"batch_size\": batch_size, \"dt_min\": dt/60.0\n",
        "        }\n",
        "        _save_latest(meta)\n",
        "\n",
        "        # Criterio de BEST: F1_shop, desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            _save_best(meta)\n",
        "\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_BEST, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "best1 = phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "if EPOCHS_L34 > 0:\n",
        "    parts = [model.layer3, model.layer4, model.fc]\n",
        "    best3 = phase(\"Fase 3: Fine-tune layer3+4 (opcional)\", parts, EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt_path = CKPT_BEST if os.path.exists(CKPT_BEST) else CKPT_LATEST\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_loader(test_items, batch_size=8, train=False)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt (si existe):\", CKPT_BEST)\n",
        "print(\"Latest ckpt:\", CKPT_LATEST)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYorMdIKz7fm",
        "outputId": "bb601d09-8f99-4bd5-a51b-2bde7dde0065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test clips: 770\n",
            "\n",
            "Matriz de confusión (TEST):\n",
            "[[ 88 103]\n",
            " [205 374]]\n",
            "\n",
            "Reporte (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.30      0.46      0.36       191\n",
            " shoplifting       0.78      0.65      0.71       579\n",
            "\n",
            "    accuracy                           0.60       770\n",
            "   macro avg       0.54      0.55      0.54       770\n",
            "weighted avg       0.66      0.60      0.62       770\n",
            "\n",
            "Checkpoint evaluado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Tiempo eval: 26.9 min en cpu\n"
          ]
        }
      ],
      "source": [
        "# === EVAL ÚNICA DE UN CHECKPOINT (.pt) SIN ENTRENAR ===\n",
        "import os, cv2, time, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Rutas\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TEST_CSV = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Cambiá acá el ckpt que quieras evaluar:\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # o ...linearprobe.pt\n",
        "\n",
        "# Hiperparámetros de PREPROCESADO (ajustá a cómo entrenaste ese ckpt)\n",
        "NUM_FRAMES = 16   # si el ckpt es el finetune\n",
        "IMG_SIZE   = 112  # si el ckpt es el finetune\n",
        "# Si evaluás el linearprobe original, usá:\n",
        "# NUM_FRAMES, IMG_SIZE = 8, 96\n",
        "\n",
        "BATCH = 8\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "test_items = read_csv(TEST_CSV)\n",
        "print(\"test clips:\", len(test_items))\n",
        "\n",
        "# Normalización usada en entrenamiento (Kinetics)\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample):\n",
        "    if n_total <= 0: return np.zeros(n_sample, dtype=int)\n",
        "    return np.linspace(0, max(0, n_total-1), num=n_sample, dtype=int)\n",
        "\n",
        "def clip_tensor(path, nframes=NUM_FRAMES, size=IMG_SIZE):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "    x = (x - mean) / std\n",
        "    return x  # CPU\n",
        "\n",
        "class TestDS(Dataset):\n",
        "    def __init__(self, items): self.items = items\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        p,y = self.items[i]\n",
        "        return clip_tensor(p), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "loader = DataLoader(TestDS(test_items), batch_size=BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "# Modelo + checkpoint\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Eval\n",
        "all_y, all_p = [], []\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        logits = model(X)\n",
        "        pred = logits.argmax(1).cpu().numpy().tolist()\n",
        "        all_p += pred\n",
        "        all_y += y.numpy().tolist()\n",
        "dt = time.time()-t0\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(f\"Checkpoint evaluado: {CKPT_PATH}\")\n",
        "print(f\"Tiempo eval: {dt/60:.1f} min en {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HhJvfBM-Kvj",
        "outputId": "be0daad0-305c-4d5c-fac2-c5337e5cbc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "train:10668  val:787  test:770\n",
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Device: cpu\n",
            "Fases: {'EPOCHS_LINEAR': 0, 'EPOCHS_L4': 5, 'EPOCHS_L34': 0}\n",
            "[Fase 1: Linear probe (solo fc)] saltado (epochs=0)\n",
            "↪️  [Fase 2: Fine-tune layer4] Arranco desde el mejor checkpoint previo.\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 | epochs=5 batch=6 lr=0.0003 =====\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.391 P_norm=0.186\n",
            "Ep 01/5 | train 0.3496/0.917 | val 2.9601/0.355 | P_norm 0.186 R_norm 0.983 F1_norm 0.314 | P_shop 0.988 R_shop 0.244 F1_shop 0.391 | 85.4 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.455 P_norm=0.192\n",
            "Ep 02/5 | train 0.0856/0.972 | val 3.1282/0.395 | P_norm 0.192 R_norm 0.949 F1_norm 0.320 | P_shop 0.971 R_shop 0.297 F1_shop 0.455 | 81.5 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.628 P_norm=0.243\n",
            "Ep 03/5 | train 0.0705/0.978 | val 1.6795/0.537 | P_norm 0.243 R_norm 0.983 F1_norm 0.389 | P_shop 0.994 R_shop 0.459 F1_shop 0.628 | 81.2 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.651 P_norm=0.236\n",
            "Ep 04/5 | train 0.0253/0.988 | val 1.7826/0.551 | P_norm 0.236 R_norm 0.890 F1_norm 0.373 | P_shop 0.962 R_shop 0.492 F1_shop 0.651 | 80.0 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 05/5 | train 0.0299/0.991 | val 1.7391/0.544 | P_norm 0.235 R_norm 0.907 F1_norm 0.373 | P_shop 0.967 R_shop 0.480 F1_shop 0.641 | 80.2 min\n",
            "\n",
            "=== Selección de umbral por FPR objetivo en VALIDACIÓN ===\n",
            "Umbral elegido (FPR≤0.030): 0.88 | FPR:0.025 TPR:0.236 P:0.981 R:0.236 F1:0.381 | cm(normal tn,fp; shop fn,tp)=(115, 3, 511, 158)\n",
            "Umbral guardado en: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.threshold.json\n",
            "\n",
            "=== TEST con umbral elegido ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.28      0.97      0.44       191\n",
            " shoplifting       0.96      0.19      0.32       579\n",
            "\n",
            "    accuracy                           0.39       770\n",
            "   macro avg       0.62      0.58      0.38       770\n",
            "weighted avg       0.79      0.39      0.35       770\n",
            "\n",
            "[[186   5]\n",
            " [467 112]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3991"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 por fases + Sampler balanceado + Umbral por FPR =====\n",
        "import os, random, time, gc, hashlib, json\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import cv2\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # tiene train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# Checkpoint (mejor modelo)\n",
        "CKPT_PATH = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "THR_PATH  = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.threshold.json\")\n",
        "\n",
        "# Tamaño de clip (en CPU podés bajar a 8/96 para acelerar)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Fases (podés poner 0 para saltear alguna)\n",
        "EPOCHS_LINEAR = 0     # solo fc\n",
        "EPOCHS_L4     = 5     # fine-tune layer4\n",
        "EPOCHS_L34    = 0     # opcional: layer3 + layer4\n",
        "\n",
        "# Batch y lrs\n",
        "BATCH_LINEAR  = 8\n",
        "BATCH_FT      = 6\n",
        "LR_FC   = 1e-3\n",
        "LR_L4   = 3e-4\n",
        "LR_L34  = 2e-4\n",
        "\n",
        "# Pesos de clase (más peso a \"normal\" para bajar FP)\n",
        "W_NORMAL = 2.0\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "# Sampler balanceado (recomendado ON)\n",
        "USE_BALANCED_SAMPLER = True\n",
        "\n",
        "# Sesión corta (para Colab) -> guarda intermedio y limita batches por época\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300\n",
        "SAVE_EVERY_BATCHES  = 150\n",
        "\n",
        "# Umbral orientado a baja FPR (falsas alarmas). Apuntá a 0.02–0.05\n",
        "TARGET_FPR = 0.03\n",
        "THR_GRID   = np.linspace(0.05, 0.95, 37)  # rejilla de thresholds\n",
        "\n",
        "# Device + threads\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- UTILIDADES DE DATOS (cache de frames) -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)  # (T,H,W,C)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)     # C,T,H,W\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])  # flip horizontal\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_loader(items, batch, train, balanced=False):\n",
        "    ds = ClipDataset(items, train)\n",
        "    if balanced and train:\n",
        "        # pesos por clase inversamente proporcionales a su frecuencia\n",
        "        counts = np.bincount([y for _,y in items], minlength=2)\n",
        "        w_per_class = 1.0 / np.maximum(counts, 1)\n",
        "        sample_weights = [w_per_class[y] for _, y in items]\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        return DataLoader(ds, batch_size=batch, sampler=sampler, num_workers=0, pin_memory=False)\n",
        "    else:\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=train, num_workers=0, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Reanudar si hay ckpt\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    try:\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {CKPT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {CKPT_PATH}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# Pérdida con pesos de clase\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Helpers\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1])\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i,(x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            try:\n",
        "                torch.save({\"model\": model.state_dict(), \"classes\": CLASSES}, CKPT_PATH)\n",
        "                print(f\"💾 Guardado intermedio (batch {i}) -> {CKPT_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(\"⚠️ No pude guardar intermedio:\", e)\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "    # Congelar todo\n",
        "    set_trainable(model, False)\n",
        "    # Descongelar las partes\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "    # Asegurar que arrancamos desde el mejor ckpt de antes\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        try:\n",
        "            ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde el mejor checkpoint previo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {CKPT_PATH}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_loader(train_items, batch_size, train=True,  balanced=USE_BALANCED_SAMPLER)\n",
        "    val_loader   = make_loader(val_items,   batch_size, train=False, balanced=False)\n",
        "\n",
        "    best_key = None\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None)\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])  # prioriza F1 de robo y precisión de normal (menos FP)\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, CKPT_PATH)\n",
        "            print(f\"✔️ guardado mejor: {CKPT_PATH} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_PATH\n",
        "\n",
        "# ----------------- ENTRENAMIENTO POR FASES -----------------\n",
        "print(\"Device:\", device)\n",
        "from torchvision.models.video import r3d_18\n",
        "print(\"Fases:\", dict(EPOCHS_LINEAR=EPOCHS_LINEAR, EPOCHS_L4=EPOCHS_L4, EPOCHS_L34=EPOCHS_L34))\n",
        "\n",
        "# Fase 1 (opcional)\n",
        "phase(\"Fase 1: Linear probe (solo fc)\", [model.fc], EPOCHS_LINEAR, BATCH_LINEAR, LR_FC)\n",
        "# Fase 2\n",
        "phase(\"Fase 2: Fine-tune layer4\", [model.layer4, model.fc], EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "# Fase 3 (opcional)\n",
        "if EPOCHS_L34 > 0:\n",
        "    phase(\"Fase 3: Fine-tune layer3+4\", [model.layer3, model.layer4, model.fc], EPOCHS_L34, BATCH_FT, LR_L34)\n",
        "\n",
        "# ----------------- ELEGIR UMBRAL POR FPR EN VALIDACIÓN -----------------\n",
        "@torch.no_grad()\n",
        "def infer_probs(items, batch=8):\n",
        "    ds = ClipDataset(items, train=False)\n",
        "    loader = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=0, pin_memory=False)\n",
        "    model.eval()\n",
        "    pr=[]; ys=[]\n",
        "    for x,y in loader:\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        pr.extend(p.tolist()); ys.extend(y.numpy().tolist())\n",
        "    return np.array(pr), np.array(ys)\n",
        "\n",
        "# cargar mejor ckpt final\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "print(\"\\n=== Selección de umbral por FPR objetivo en VALIDACIÓN ===\")\n",
        "probs_va, yva = infer_probs(val_items, batch=8)\n",
        "\n",
        "def fpr_at_threshold(y_true, y_prob, thr):\n",
        "    pred = (y_prob >= thr).astype(int)\n",
        "    tn = int(((y_true==0) & (pred==0)).sum())\n",
        "    fp = int(((y_true==0) & (pred==1)).sum())\n",
        "    fn = int(((y_true==1) & (pred==0)).sum())\n",
        "    tp = int(((y_true==1) & (pred==1)).sum())\n",
        "    fpr = fp / (fp + tn) if (fp+tn)>0 else 0.0\n",
        "    tpr = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    P,R,F1,_ = precision_recall_fscore_support(y_true, pred, average='binary', zero_division=0)\n",
        "    return fpr, tpr, P, R, F1, (tn,fp,fn,tp)\n",
        "\n",
        "best_thr, best_rec, best_stats = 0.5, -1, None\n",
        "for thr in THR_GRID:\n",
        "    fpr, tpr, P, R, F1, cm = fpr_at_threshold(yva, probs_va, thr)\n",
        "    if fpr <= TARGET_FPR and R > best_rec:\n",
        "        best_thr, best_rec, best_stats = float(thr), R, dict(fpr=fpr, tpr=tpr, P=P, R=R, F1=F1, cm=cm)\n",
        "\n",
        "if best_stats is None:\n",
        "    # si ningún thr cumple FPR, elegimos el de menor FPR\n",
        "    best_thr, best_stats = None, None\n",
        "    min_fpr, best = 999, None\n",
        "    for thr in THR_GRID:\n",
        "        fpr, tpr, P, R, F1, cm = fpr_at_threshold(yva, probs_va, thr)\n",
        "        if fpr < min_fpr:\n",
        "            min_fpr, best = fpr, (thr, dict(fpr=fpr, tpr=tpr, P=P, R=R, F1=F1, cm=cm))\n",
        "    best_thr, best_stats = float(best[0]), best[1]\n",
        "    print(f\"⚠️ Ningún thr alcanzó FPR≤{TARGET_FPR:.3f}. Tomo el menor FPR: {best_stats['fpr']:.3f} @ thr={best_thr:.2f}\")\n",
        "else:\n",
        "    print(f\"Umbral elegido (FPR≤{TARGET_FPR:.3f}): {best_thr:.2f} | \"\n",
        "          f\"FPR:{best_stats['fpr']:.3f} TPR:{best_stats['tpr']:.3f} \"\n",
        "          f\"P:{best_stats['P']:.3f} R:{best_stats['R']:.3f} F1:{best_stats['F1']:.3f} | \"\n",
        "          f\"cm(normal tn,fp; shop fn,tp)={best_stats['cm']}\")\n",
        "\n",
        "# guardar umbral\n",
        "with open(THR_PATH, \"w\") as f:\n",
        "    json.dump({\"threshold\": best_thr,\n",
        "               \"target_fpr\": TARGET_FPR,\n",
        "               \"val_metrics\": best_stats}, f, indent=2)\n",
        "print(\"Umbral guardado en:\", THR_PATH)\n",
        "\n",
        "# ----------------- EVALUAR EN TEST con ese umbral -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "probs_te, yte = infer_probs(test_items, batch=8)\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\n=== TEST con umbral elegido ===\")\n",
        "print(classification_report(yte, preds_te, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(confusion_matrix(yte, preds_te))\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NEDcMUZMEIu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CUARTO ENTRENAMIENTO**"
      ],
      "metadata": {
        "id": "8QjeIiFkELv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y update >/dev/null\n",
        "!apt-get -y install ffmpeg >/dev/null\n",
        "!pip -q install \"av>=10,<14\""
      ],
      "metadata": {
        "id": "k2zyVMNtELv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ENTRENAMIENTO GRANDE R3D-18 (A/B/C) - Shoplifting\n",
        "# =========================\n",
        "# - Fase A: l2+l3+l4+fc (10 ep)\n",
        "# - Fase B: full unfreeze + LRs discriminativos (10 ep)\n",
        "# - Fase C: fine-tune fino (10-12 ep)\n",
        "# Incluye: acumulación de gradiente, sampler balanceado, stride temporal,\n",
        "# augment robusto, reanudación completa y barrido de umbral con F2\n",
        "# =========================\n",
        "\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, f1_score\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# =========================\n",
        "# CONFIG RÁPIDA (EDITAR AQUÍ)\n",
        "# =========================\n",
        "PHASE       = \"A\"    # \"A\" | \"B\" | \"C\"\n",
        "SEED        = 1337\n",
        "BASE_DIR    = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV   = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV     = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "# EXPERIMENTO\n",
        "EXP_NAME    = \"exp_l234_fullrun\"\n",
        "ROOT_OUT    = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "OUT_DIR     = f\"{ROOT_OUT}/{EXP_NAME}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# CHECKPOINT INICIAL (pesos del mejor previo)\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# EP, batch y acumulación por fase\n",
        "EPOCHS_A, EPOCHS_B, EPOCHS_C = 10, 10, 12\n",
        "BATCH_SIZE = 6\n",
        "ACCUM_STEPS= 4  # 6*4 => batch efectivo 24\n",
        "\n",
        "# CLIP/CROP/STRIDE (ajustados por fase más abajo)\n",
        "CLIP_LEN_A = 32; TEMP_STRIDE_A = 2; CROP_A = 128\n",
        "CLIP_LEN_B = 32; TEMP_STRIDE_B = 2; CROP_B = 128\n",
        "CLIP_LEN_C = 40; TEMP_STRIDE_C = 2; CROP_C = 160  # subir contexto y/o crop en fine\n",
        "\n",
        "# Pérdida y pesos\n",
        "GAMMA_A = 1.2; WEIGHT_DEC_A = 7e-4\n",
        "GAMMA_B = 1.2; WEIGHT_DEC_B = 1e-3\n",
        "GAMMA_C = 1.2; WEIGHT_DEC_C = 5e-4\n",
        "CLASS_WEIGHTS = torch.tensor([1.0, 1.6], dtype=torch.float32)  # normal, hurto\n",
        "\n",
        "# LRs base y discriminativos\n",
        "LR_A = 1.0e-4\n",
        "LR_B_FC, LR_B_L4, LR_B_L3, LR_B_L2, LR_B_L1 = 1.5e-4, 1.0e-4, 7.5e-5, 5.0e-5, 3.0e-5\n",
        "LR_C_FC, LR_C_L4, LR_C_L3, LR_C_L2, LR_C_L1 = 8.0e-5, 6.0e-5, 4.5e-5, 3.0e-5, 2.0e-5\n",
        "\n",
        "# Sweep thresholds y F-beta\n",
        "TH_MIN, TH_MAX, TH_STEPS = 0.20, 0.60, 41\n",
        "FBETAS = [1.0, 1.5, 2.0]  # reporta F1/F1.5/F2 (clase hurto) y fija operativo por F2\n",
        "\n",
        "# LOGS/CHECKPOINTS\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"bestLoss.pt\")\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"last.pt\")\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"runstate.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"threshold_sweep.csv\")\n",
        "OPER_JSON   = os.path.join(OUT_DIR, \"operating_threshold.json\")\n",
        "\n",
        "# DataLoader\n",
        "NUM_WORKERS = 0\n",
        "PINMEM      = False\n",
        "\n",
        "# Sampler balanceado\n",
        "USE_SAMPLER = True\n",
        "\n",
        "# =========================\n",
        "# UTILIDADES\n",
        "# =========================\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"medium\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "try:\n",
        "    from torch.amp import GradScaler, autocast\n",
        "except Exception:\n",
        "    # Fallback para versiones viejas\n",
        "    class GradScaler:\n",
        "        def __init__(self,*a,**k): pass\n",
        "        def scale(self,x): return x\n",
        "        def step(self,opt): opt.step()\n",
        "        def update(self): pass\n",
        "    from contextlib import contextmanager\n",
        "    @contextmanager\n",
        "    def autocast(*a, **k): yield\n",
        "\n",
        "scaler = GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "# =========================\n",
        "# DATASET\n",
        "# =========================\n",
        "RESIZE = 144  # se ajusta con CROP por fase\n",
        "\n",
        "def _parse_label(s):\n",
        "    s = str(s).strip().lower()\n",
        "    if s.isdigit(): return int(s)\n",
        "    if s in (\"normal\", \"0\"): return 0\n",
        "    if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "    raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "def load_list(csv_path):\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row: continue\n",
        "            head = str(row[0]).strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # header\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = _parse_label(row[1])\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand): path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True, clip_len=32, temp_stride=2, crop=128):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.clip_len = clip_len\n",
        "        self.temp_stride = temp_stride\n",
        "        self.crop = crop\n",
        "        # Augments\n",
        "        if train:\n",
        "            self.spatial = T.Compose([\n",
        "                T.Resize((RESIZE, RESIZE)),\n",
        "                T.RandomResizedCrop(self.crop, scale=(0.65, 1.0), ratio=(0.8, 1.25)),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.RandomApply([T.ColorJitter(0.2,0.2,0.2,0.06)], p=0.5),\n",
        "                T.RandomGrayscale(p=0.10),\n",
        "                T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.1)),\n",
        "            ])\n",
        "        else:\n",
        "            self.spatial = T.Compose([\n",
        "                T.Resize((RESIZE, RESIZE)),\n",
        "                T.CenterCrop(self.crop),\n",
        "            ])\n",
        "        # Normalización Kinetics\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
        "        T_total = video.shape[0]\n",
        "\n",
        "        # Elegir ventana base\n",
        "        need_total = self.clip_len * self.temp_stride\n",
        "        if T_total >= need_total:\n",
        "            if self.train:\n",
        "                start = random.randint(0, T_total - need_total)\n",
        "            else:\n",
        "                start = max(0, (T_total - need_total)//2)\n",
        "            idxs = torch.arange(start, start+need_total, self.temp_stride)\n",
        "        else:\n",
        "            # cubrir con padding y stride\n",
        "            idxs = torch.arange(0, min(T_total, need_total), self.temp_stride)\n",
        "            if len(idxs) < self.clip_len:\n",
        "                pad = self.clip_len - len(idxs)\n",
        "                idxs = torch.cat([idxs, idxs[-1:].repeat(pad)])\n",
        "\n",
        "        idxs = idxs[idxs < T_total]\n",
        "        if len(idxs) < self.clip_len:\n",
        "            idxs = torch.cat([idxs, idxs[-1:].repeat(self.clip_len - len(idxs))])\n",
        "\n",
        "        clip = video[idxs]  # [T,C,H,W]\n",
        "\n",
        "        # Spatial aug por frame\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "\n",
        "        # r3d espera [C,T,H,W]\n",
        "        clip = clip.permute(1,0,2,3).contiguous()\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# =========================\n",
        "# MODELO / CONGELADO\n",
        "# =========================\n",
        "def build_model(num_classes=2):\n",
        "    try:\n",
        "        model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "    except TypeError:\n",
        "        model = r3d_18(pretrained=True)\n",
        "    in_feats = model.fc.in_features\n",
        "    model.fc = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(in_feats, num_classes))\n",
        "    return model\n",
        "\n",
        "def set_trainable_params(model, phase):\n",
        "    if phase == \"A\":\n",
        "        # Entrenar l2+l3+l4+fc\n",
        "        for n,p in model.named_parameters():\n",
        "            p.requires_grad = any(k in n for k in [\"layer2\",\"layer3\",\"layer4\",\"fc\"])\n",
        "    else:\n",
        "        # B y C: full unfreeze\n",
        "        for _,p in model.named_parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "def build_optimizer(model, phase):\n",
        "    if phase == \"A\":\n",
        "        return torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=LR_A, weight_decay=WEIGHT_DEC_A\n",
        "        )\n",
        "    elif phase == \"B\":\n",
        "        params = [\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"fc\" in n) and p.requires_grad], \"lr\": LR_B_FC},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer4\" in n) and p.requires_grad], \"lr\": LR_B_L4},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer3\" in n) and p.requires_grad], \"lr\": LR_B_L3},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer2\" in n) and p.requires_grad], \"lr\": LR_B_L2},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer1\" in n) and p.requires_grad], \"lr\": LR_B_L1},\n",
        "        ]\n",
        "        return torch.optim.AdamW(params, weight_decay=WEIGHT_DEC_B)\n",
        "    else:  # phase C\n",
        "        params = [\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"fc\" in n) and p.requires_grad], \"lr\": LR_C_FC},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer4\" in n) and p.requires_grad], \"lr\": LR_C_L4},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer3\" in n) and p.requires_grad], \"lr\": LR_C_L3},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer2\" in n) and p.requires_grad], \"lr\": LR_C_L2},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer1\" in n) and p.requires_grad], \"lr\": LR_C_L1},\n",
        "        ]\n",
        "        return torch.optim.AdamW(params, weight_decay=WEIGHT_DEC_C)\n",
        "\n",
        "# =========================\n",
        "# LOSS (Focal)\n",
        "# =========================\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.2, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup: return base_lr * (t+1)/max(1,warmup)\n",
        "    tw = max(1, T - warmup); tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "# =========================\n",
        "# REANUDACIÓN\n",
        "# =========================\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, model, optimizer, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if torch.cuda.is_available() else None,\n",
        "        \"rng\": _rng_state(),\n",
        "        \"class_weights\": CLASS_WEIGHTS,\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume(model, optimizer):\n",
        "    start_epoch = 0; best_f1 = -1.0; best_loss = float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd[\"model\"], strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        if torch.cuda.is_available() and sd.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(sd[\"scaler\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        # cargar solo pesos del modelo si existe CKPT_IN\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            sd = torch.load(CKPT_IN, map_location=\"cpu\")\n",
        "            state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "            missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Cargado CKPT_IN:\", CKPT_IN)\n",
        "            print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# =========================\n",
        "# LOOP\n",
        "# =========================\n",
        "def run_epoch(loader, model, optimizer, criterion, train=True, current_epoch=0,\n",
        "              max_batches=None, global_step_start=0, accum_steps=1):\n",
        "    model.train() if train else model.eval()\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "\n",
        "    if train:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches): break\n",
        "        x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            if train: loss = loss / accum_steps\n",
        "\n",
        "        if train:\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            # Step cada ACCUM_STEPS\n",
        "            if ((i + 1) % accum_steps == 0) or (i + 1 == len(loader)):\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                if torch.cuda.is_available():\n",
        "                    scaler.step(optimizer); scaler.update()\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                global_step += 1\n",
        "\n",
        "        probs_hurto = logits.softmax(1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += (loss.item() * accum_steps) * y.size(0)  # deshacer divisón para calc global\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    # F1 ref (umbral 0.5) clase hurto\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# =========================\n",
        "# MAIN\n",
        "# =========================\n",
        "def main():\n",
        "    # Fase-specific setup\n",
        "    if PHASE == \"A\":\n",
        "        clip_len, tstride, crop = CLIP_LEN_A, TEMP_STRIDE_A, CROP_A\n",
        "        epochs = EPOCHS_A\n",
        "        gamma  = GAMMA_A\n",
        "    elif PHASE == \"B\":\n",
        "        clip_len, tstride, crop = CLIP_LEN_B, TEMP_STRIDE_B, CROP_B\n",
        "        epochs = EPOCHS_B\n",
        "        gamma  = GAMMA_B\n",
        "    else:\n",
        "        clip_len, tstride, crop = CLIP_LEN_C, TEMP_STRIDE_C, CROP_C\n",
        "        epochs = EPOCHS_C\n",
        "        gamma  = GAMMA_C\n",
        "\n",
        "    # Data\n",
        "    train_items = load_list(TRAIN_CSV)\n",
        "    val_items   = load_list(VAL_CSV)\n",
        "    print(f\"Train clips: {len(train_items)} | Val clips: {len(val_items)}\")\n",
        "    N_normal = sum(1 for _,y in train_items if y==0)\n",
        "    N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "    print(f\"Conteo train → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "    train_ds = VideoDataset(train_items, train=True,  clip_len=clip_len, temp_stride=tstride, crop=crop)\n",
        "    val_ds   = VideoDataset(val_items,   train=False, clip_len=clip_len, temp_stride=tstride, crop=crop)\n",
        "\n",
        "    if USE_SAMPLER:\n",
        "        from torch.utils.data import WeightedRandomSampler\n",
        "        weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "        sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                                  num_workers=NUM_WORKERS, pin_memory=PINMEM,\n",
        "                                  drop_last=True, persistent_workers=(NUM_WORKERS>0))\n",
        "    else:\n",
        "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                  num_workers=NUM_WORKERS, pin_memory=PINMEM,\n",
        "                                  drop_last=True, persistent_workers=(NUM_WORKERS>0))\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                            num_workers=NUM_WORKERS, pin_memory=PINMEM,\n",
        "                            persistent_workers=(NUM_WORKERS>0))\n",
        "\n",
        "    # Modelo / Opt / Loss\n",
        "    model = build_model(num_classes=2).to(DEVICE)\n",
        "    set_trainable_params(model, PHASE)\n",
        "    optimizer = build_optimizer(model, PHASE)\n",
        "    criterion = FocalLoss(gamma=gamma, weight=CLASS_WEIGHTS.to(DEVICE))\n",
        "\n",
        "    # Reanudación o CKPT_IN\n",
        "    start_epoch, best_f1, best_loss = try_resume(model, optimizer)\n",
        "\n",
        "    # Historial\n",
        "    history = []\n",
        "    global_step = 0\n",
        "\n",
        "    # Entrenamiento\n",
        "    for epoch in range(start_epoch + 1, start_epoch + epochs + 1):\n",
        "        # LR schedule (cosine + warmup relativo a total de FASE)\n",
        "        if PHASE == \"A\":\n",
        "            base_lr = LR_A\n",
        "            for pg in optimizer.param_groups: pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_A, pg.get(\"lr\", base_lr), warmup=2)\n",
        "        elif PHASE == \"B\":\n",
        "            # mantener los group LRs relativos (no reasignar fijo cada epoch; solo decaimos suave)\n",
        "            for pg in optimizer.param_groups:\n",
        "                base = pg[\"lr\"]\n",
        "                pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_B, base, warmup=2)\n",
        "        else:\n",
        "            for pg in optimizer.param_groups:\n",
        "                base = pg[\"lr\"]\n",
        "                pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_C, base, warmup=2)\n",
        "\n",
        "        tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "            train_loader, model, optimizer, criterion, train=True,\n",
        "            current_epoch=epoch, max_batches=None, global_step_start=global_step, accum_steps=ACCUM_STEPS\n",
        "        )\n",
        "        va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "            val_loader, model, optimizer, criterion, train=False,\n",
        "            current_epoch=epoch, max_batches=None, global_step_start=global_step, accum_steps=1\n",
        "        )\n",
        "\n",
        "        row = {\"epoch\": epoch, \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "               \"val_loss\": va_loss, \"val_f1\": va_f1,\n",
        "               \"phase\": PHASE}\n",
        "        history.append(row)\n",
        "        print(f\"[{epoch:02d}/{start_epoch+epochs:02d}] phase={PHASE} | train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "        # Guardado \"last\"\n",
        "        save_ckpt(CKPT_LAST, epoch, best_f1, best_loss, row, model, optimizer)\n",
        "        try: shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "        except Exception: pass\n",
        "\n",
        "        # Bests\n",
        "        if va_f1 > best_f1:\n",
        "            best_f1 = va_f1\n",
        "            torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "            print(\"✔️ Mejor F1 ->\", CKPT_BESTF1)\n",
        "        if va_loss < best_loss:\n",
        "            best_loss = va_loss\n",
        "            torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "            print(\"✔️ Mejor Loss ->\", CKPT_BESTL)\n",
        "\n",
        "    # Log JSON\n",
        "    with open(LOG_JSON, \"w\") as f: json.dump(history, f, indent=2)\n",
        "    print(\"Log guardado en:\", LOG_JSON)\n",
        "\n",
        "    # Reporte y sweep de thresholds (incluye Fβ y guarda operativo por F2)\n",
        "    if 'probs' in locals() and probs.size > 0:\n",
        "        # Reporte estándar t=0.5\n",
        "        yp05 = (probs >= 0.5).astype(int)\n",
        "        print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "        print(classification_report(ys, yp05, target_names=[\"normal\",\"hurto\"]))\n",
        "        print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, yp05))\n",
        "\n",
        "        ths = np.round(np.linspace(TH_MIN, TH_MAX, TH_STEPS), 3)\n",
        "        with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f); w.writerow([\"threshold\",\"beta\",\"precision_hurto\",\"recall_hurto\",\"f1_hurto\",\"fbeta_hurto\"])\n",
        "            for t in ths:\n",
        "                ypt = (probs >= t).astype(int)\n",
        "                p, r, f1, _ = precision_recall_fscore_support(ys, ypt, average=None, zero_division=0)\n",
        "                p1, r1, f1_1 = p[1], r[1], f1[1]\n",
        "                for beta in FBETAS:\n",
        "                    b2 = beta*beta\n",
        "                    fbeta = (1+b2) * (p1*r1) / (b2*p1 + r1 + 1e-12)\n",
        "                    w.writerow([t, beta, p1, r1, f1_1, fbeta])\n",
        "        print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "\n",
        "        # Elegir operativo por F2\n",
        "        rows = []\n",
        "        with open(THR_CSV, newline=\"\") as f:\n",
        "            r = csv.DictReader(f)\n",
        "            for row in r:\n",
        "                if float(row[\"beta\"]) == 2.0:\n",
        "                    rows.append((float(row[\"fbeta_hurto\"]), float(row[\"threshold\"]),\n",
        "                                 float(row[\"precision_hurto\"]), float(row[\"recall_hurto\"]),\n",
        "                                 float(row[\"f1_hurto\"])))\n",
        "        if rows:\n",
        "            rows.sort(reverse=True)\n",
        "            best = rows[0]\n",
        "            f2b, t_best, p1b, r1b, f1b = best\n",
        "            print(f\"\\n⭐ Mejor threshold por F2 (hurto): t={t_best:.3f} | P={p1b:.3f} | R={r1b:.3f} | F1={f1b:.3f} | F2={f2b:.3f}\")\n",
        "            with open(OPER_JSON, \"w\") as f:\n",
        "                json.dump({\"threshold\": float(t_best), \"metric\":\"F2\"}, f, indent=2)\n",
        "            print(\"Operating threshold ->\", OPER_JSON)\n",
        "    else:\n",
        "        print(\"\\n(No hubo validación con muestras; sin sweep de thresholds)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "WXYvbhSRELv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HmyRxJK96O5"
      },
      "source": [
        "## ***CONSULTAR CANTIDAD DE CLIPS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jQQTtdqsmXM",
        "outputId": "7e13960c-7f38-4df1-daea-0a606b57756b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "===== DISTRIBUCIÓN POR CLASE =====\n",
            "\n",
            "TRAIN\n",
            "class\n",
            "shoplifting    8619\n",
            "normal          601\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "VALIDACIÓN\n",
            "class\n",
            "shoplifting    857\n",
            "normal          39\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "TEST\n",
            "class\n",
            "shoplifting    358\n",
            "normal          31\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "===== TOTALES =====\n",
            "Train total: 9220\n",
            "Val total:   896\n",
            "Test total:  389\n",
            "Total clips: 10505\n",
            "\n",
            "===== CHEQUEO DE DUPLICADOS =====\n",
            "Clips repetidos entre conjuntos: 0\n",
            "✅ No hay clips repetidos entre train, val y test.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================\n",
        "# 🔍 VERIFICADOR DE DATASET - TESIS DE HURTOS (CSV sin encabezado)\n",
        "# ==============================================\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1️⃣ Montar Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2️⃣ Ruta base del dataset\n",
        "BASE_DIR = \"/content/drive/MyDrive/tesisV2/cortos/video_clips_192\"\n",
        "\n",
        "# 3️⃣ Leer los CSV (sin encabezado)\n",
        "train_path = os.path.join(BASE_DIR, \"train.csv\")\n",
        "val_path = os.path.join(BASE_DIR, \"val.csv\")\n",
        "test_path = os.path.join(BASE_DIR, \"test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, header=None, names=[\"video\", \"class\"])\n",
        "val = pd.read_csv(val_path, header=None, names=[\"video\", \"class\"])\n",
        "test = pd.read_csv(test_path, header=None, names=[\"video\", \"class\"])\n",
        "\n",
        "# 4️⃣ Mostrar conteos por clase en cada conjunto\n",
        "print(\"===== DISTRIBUCIÓN POR CLASE =====\\n\")\n",
        "\n",
        "print(\"TRAIN\")\n",
        "print(train[\"class\"].value_counts(), \"\\n\")\n",
        "\n",
        "print(\"VALIDACIÓN\")\n",
        "print(val[\"class\"].value_counts(), \"\\n\")\n",
        "\n",
        "print(\"TEST\")\n",
        "print(test[\"class\"].value_counts(), \"\\n\")\n",
        "\n",
        "# 5️⃣ Totales generales\n",
        "print(\"===== TOTALES =====\")\n",
        "print(f\"Train total: {len(train)}\")\n",
        "print(f\"Val total:   {len(val)}\")\n",
        "print(f\"Test total:  {len(test)}\")\n",
        "print(f\"Total clips: {len(train) + len(val) + len(test)}\\n\")\n",
        "\n",
        "# 6️⃣ Revisar duplicados entre conjuntos\n",
        "train_videos = set(train[\"video\"])\n",
        "val_videos = set(val[\"video\"])\n",
        "test_videos = set(test[\"video\"])\n",
        "\n",
        "dups = (train_videos & val_videos) | (train_videos & test_videos) | (val_videos & test_videos)\n",
        "print(\"===== CHEQUEO DE DUPLICADOS =====\")\n",
        "print(f\"Clips repetidos entre conjuntos: {len(dups)}\")\n",
        "if len(dups) > 0:\n",
        "    print(\"Ejemplo de repetidos:\", list(dups)[:5])\n",
        "else:\n",
        "    print(\"✅ No hay clips repetidos entre train, val y test.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/tesisV2/cortos/video_clips_192\"\n",
        "\n",
        "train = pd.read_csv(os.path.join(BASE_DIR, \"train.csv\"))\n",
        "val = pd.read_csv(os.path.join(BASE_DIR, \"val.csv\"))\n",
        "test = pd.read_csv(os.path.join(BASE_DIR, \"test.csv\"))\n",
        "\n",
        "print(\"Train clips:\", len(train))\n",
        "print(\"Val clips:\", len(val))\n",
        "print(\"Test clips:\", len(test))\n",
        "print(\"Total clips:\", len(train) + len(val) + len(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7be2qoKK-yYB",
        "outputId": "4141a622-8a69-4d91-d5f3-041ccd400680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train clips: 9219\n",
            "Val clips: 895\n",
            "Test clips: 388\n",
            "Total clips: 10502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7bAIj1xeoB"
      },
      "source": [
        "## ***BUSCA EL MEJOR UMBRAL (threshold)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "uFz8BrGis-d5",
        "outputId": "560fc270-3294-421d-ad72-097a327c9b30"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tesisV2/cortos/supermas1-1.mp4/val.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3065307900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mval_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# --- Modelo (cargamos tu checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3065307900.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tesisV2/cortos/supermas1-1.mp4/val.csv'"
          ]
        }
      ],
      "source": [
        "# ===== Umbral óptimo en VAL (batched + progreso) =====\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# --- Rutas y params (ajusta si hace falta)\n",
        "OUT_DIR  = \"/content/tesisV2/cortos/supermas1-1.mp4\"\n",
        "VAL_CSV  = os.path.join(OUT_DIR, \"val.csv\")\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES = 8\n",
        "IMG_SIZE   = 96\n",
        "BATCH_CLIPS = 8          # subí/bajá según RAM; 8 suele ir bien en CPU\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_num_threads(2)  # evita que OpenBLAS sature\n",
        "try:\n",
        "    cv2.setNumThreads(0)  # menos contención\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "val_items = read_csv(VAL_CSV)\n",
        "\n",
        "# --- Modelo (cargamos tu checkpoint)\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0, n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)  # C,T,H,W\n",
        "    x = (x - mean.cpu()) / std.cpu()\n",
        "    return x  # tensor en CPU; lo movemos a device al hacer batch\n",
        "\n",
        "# --- Pasada por VAL (BATCHEADA) para obtener probabilidades\n",
        "probs, ys = [], []\n",
        "batch_X, batch_y = [], []\n",
        "for p,y in tqdm(val_items, desc=\"VAL clips\"):\n",
        "    batch_X.append(clip_tensor(p))\n",
        "    batch_y.append(y)\n",
        "    if len(batch_X) == BATCH_CLIPS:\n",
        "        X = torch.stack(batch_X, 0).to(device)  # B,C,T,H,W\n",
        "        with torch.no_grad():\n",
        "            logits = model(X)\n",
        "            pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        probs.extend(pr.tolist()); ys.extend(batch_y)\n",
        "        batch_X, batch_y = [], []\n",
        "\n",
        "# flush final\n",
        "if batch_X:\n",
        "    X = torch.stack(batch_X, 0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(X)\n",
        "        pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "    probs.extend(pr.tolist()); ys.extend(batch_y)\n",
        "\n",
        "probs = np.array(probs); ys = np.array(ys)\n",
        "\n",
        "# --- Barrido de thresholds (con menos puntos si querés más velocidad)\n",
        "best_thr, best_score, best_tuple = 0.5, -1, (0,0,0)\n",
        "for thr in np.linspace(0.30, 0.90, 25):  # baja a 13 puntos si querés aún más rápido\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(ys, preds, average='binary')\n",
        "    if F1 > best_score:\n",
        "        best_score = F1; best_thr = float(thr); best_tuple = (float(P), float(R), float(F1))\n",
        "print(f\"Umbral óptimo (max F1 shop): {best_thr:.2f} | P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nWaXsaVm1_Q",
        "outputId": "70fde763-a55c-4a25-e710-3a0adce1bc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "train:10668  val:787  test:770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:00<00:00, 151MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Reanudando desde: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Fases: {'EPOCHS_LINEAR': 0, 'EPOCHS_L4': 3, 'EPOCHS_L34': 0}\n",
            "↪️  [Fase 2: Fine-tune layer4 (balanceado)] Arranco desde el mejor checkpoint previo.\n",
            "Frecuencias train: {1: 8586, 0: 2082}\n",
            "\n",
            "===== Fase 2: Fine-tune layer4 (balanceado) | epochs=3 batch=6 lr=0.0003 =====\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "✔️ guardado mejor: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt | F1_shop=0.667 P_norm=0.259\n",
            "Ep 01/3 | train 0.0292/0.992 | val 2.1226/0.574 | P_norm 0.259 R_norm 0.992 F1_norm 0.411 | P_shop 0.997 R_shop 0.501 F1_shop 0.667 | 19.4 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 02/3 | train 0.0103/0.996 | val 2.6764/0.573 | P_norm 0.260 R_norm 1.000 F1_norm 0.413 | P_shop 1.000 R_shop 0.498 F1_shop 0.665 | 10.6 min\n",
            "💾 Guardado intermedio (batch 150) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "💾 Guardado intermedio (batch 300) -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n",
            "Ep 03/3 | train 0.0138/0.995 | val 5.3721/0.332 | P_norm 0.175 R_norm 0.932 F1_norm 0.295 | P_shop 0.950 R_shop 0.226 F1_shop 0.365 | 8.5 min\n",
            "\n",
            "Matriz de confusión (TEST):\n",
            "[[159  32]\n",
            " [346 233]]\n",
            "\n",
            "Reporte (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.31      0.83      0.46       191\n",
            " shoplifting       0.88      0.40      0.55       579\n",
            "\n",
            "    accuracy                           0.51       770\n",
            "   macro avg       0.60      0.62      0.50       770\n",
            "weighted avg       0.74      0.51      0.53       770\n",
            "\n",
            "Best ckpt: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "451"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Fine-tuning R3D-18 balanceado (WeightedRandomSampler + Focal opcional) =====\n",
        "import os, random, time, gc, hashlib, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "# ----------------- RUTAS / CONFIG -----------------\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"   # train.csv / val.csv / test.csv\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CACHE_DIR = \"/content/tesisV2/frame_cache_npy\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = os.path.join(OUT_DIR, \"train.csv\")\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "\n",
        "# >>> Checkpoint (mejor modelo hasta ahora)\n",
        "CKPT_PATH = os.path.join(SAVE_DIR, \"r3d18_shoplifting_finetune.pt\")\n",
        "\n",
        "# Tamaño de clip (puedes bajar a 8x96 si necesitas más velocidad en CPU)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "\n",
        "# Fases (dejamos solo layer4 para ir al grano)\n",
        "BATCH_LINEAR  = 0    # no usamos fase 1\n",
        "BATCH_FT      = 6\n",
        "EPOCHS_LINEAR = 0\n",
        "EPOCHS_L4     = 3    # 2-3 épocas balanceadas suelen alcanzar para notar mejora\n",
        "EPOCHS_L34    = 0\n",
        "\n",
        "# Muestreo balanceado + pérdida\n",
        "USE_WEIGHTED_SAMPLER = True      # <— activa balanceo por batch\n",
        "USE_FOCAL_LOSS       = False     # puedes poner True si quieres Focal\n",
        "W_NORMAL = 1.0                   # si usas sampler, deja class weights en 1.0\n",
        "W_SHOP   = 1.0\n",
        "\n",
        "# Optimizadores\n",
        "LR_L4   = 3e-4\n",
        "LR_FC   = 1e-3  # por si habilitas fase 1\n",
        "\n",
        "# Sesión corta (para que no se corte en Colab)\n",
        "SHORT_SESSION       = True\n",
        "MAX_TRAIN_BATCHES   = 300\n",
        "SAVE_EVERY_BATCHES  = 150\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------- DATA -----------------\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "def read_csv(path):\n",
        "    items=[]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            p,lab = line.strip().split(\",\")\n",
        "            items.append((p, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "train_items = read_csv(TRAIN_CSV)\n",
        "val_items   = read_csv(VAL_CSV)\n",
        "test_items  = read_csv(TEST_CSV)\n",
        "print(f\"train:{len(train_items)}  val:{len(val_items)}  test:{len(test_items)}\")\n",
        "\n",
        "def sample_frame_indices(n_total, n_sample, jitter=True):\n",
        "    idx = np.linspace(0, max(0, n_total-1), num=n_sample)\n",
        "    if jitter and n_total > n_sample:\n",
        "        noise = np.random.uniform(-0.5, 0.5, size=n_sample)\n",
        "        idx = np.clip(idx + noise, 0, max(0, n_total-1))\n",
        "    return idx.astype(int)\n",
        "\n",
        "def cache_path(video_path, nframes, size):\n",
        "    h = hashlib.md5(f\"{video_path}|{nframes}|{size}\".encode()).hexdigest()\n",
        "    base = os.path.basename(video_path)\n",
        "    return os.path.join(CACHE_DIR, f\"{base}.{h}.npy\")\n",
        "\n",
        "def load_clip_as_array(path, nframes=NUM_FRAMES, size=IMG_SIZE, jitter=True):\n",
        "    cpath = cache_path(path, nframes, size)\n",
        "    if os.path.exists(cpath):\n",
        "        try:\n",
        "            arr = np.load(cpath)\n",
        "            if arr.shape == (nframes, size, size, 3):\n",
        "                return arr\n",
        "        except Exception:\n",
        "            pass\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        arr = np.zeros((nframes, size, size, 3), dtype=np.uint8)\n",
        "        np.save(cpath, arr); return arr\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = sample_frame_indices(n, nframes, jitter=jitter)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (size, size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
        "    try: np.save(cpath, arr)\n",
        "    except Exception: pass\n",
        "    return arr\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645], dtype=torch.float32).view(3,1,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989], dtype=torch.float32).view(3,1,1,1)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i):\n",
        "        path, y = self.items[i]\n",
        "        arr = load_clip_as_array(path, NUM_FRAMES, IMG_SIZE, jitter=self.train)\n",
        "        x = torch.from_numpy(arr.astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "        if self.train and random.random() < 0.3:\n",
        "            x = torch.flip(x, dims=[3])\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def make_train_loader(items, batch):\n",
        "    ds = ClipDataset(items, train=True)\n",
        "    if USE_WEIGHTED_SAMPLER:\n",
        "        # pesos por clase inversos a su frecuencia\n",
        "        labels = [y for _, y in items]\n",
        "        cnt = Counter(labels)\n",
        "        print(\"Frecuencias train:\", dict(cnt))\n",
        "        class_weight = {c: 1.0/max(1, cnt[c]) for c in cnt}\n",
        "        weights = torch.DoubleTensor([class_weight[y] for y in labels])\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(labels), replacement=True)\n",
        "        return DataLoader(ds, batch_size=batch, sampler=sampler, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=False)\n",
        "    else:\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "def make_eval_loader(items, batch_size):\n",
        "    return DataLoader(ClipDataset(items, train=False),\n",
        "                      batch_size=batch_size, shuffle=False,\n",
        "                      num_workers=NUM_WORKERS, pin_memory=False)\n",
        "\n",
        "# ----------------- MODELO -----------------\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "weights = R3D_18_Weights.KINETICS400_V1\n",
        "model = r3d_18(weights=weights)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Resume global\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    try:\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        print(f\"✅ Reanudando desde: {CKPT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No pude cargar {CKPT_PATH}: {e}. Sigo desde pesos base.\")\n",
        "\n",
        "# ----------------- Pérdida -----------------\n",
        "class_weights = torch.tensor([W_NORMAL, W_SHOP], dtype=torch.float32, device=device)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor [C] o None\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = nn.functional.cross_entropy(logits, target, reduction=\"none\", weight=self.alpha)\n",
        "        p = torch.softmax(logits, dim=1)\n",
        "        pt = p[torch.arange(p.size(0), device=p.device), target]\n",
        "        loss = (1 - pt) ** self.gamma * ce\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(alpha=class_weights, gamma=2.0) if USE_FOCAL_LOSS else nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def set_trainable(module, requires_grad: bool):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = requires_grad\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader):\n",
        "    model.eval()\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    all_y=[]; all_p=[]\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "        all_y += y.cpu().tolist(); all_p += pred.cpu().tolist()\n",
        "    P,R,F1,_ = precision_recall_fscore_support(all_y, all_p, average=None, labels=[0,1], zero_division=0)\n",
        "    return loss_sum/total, correct/total, {\"P_norm\":P[0], \"R_norm\":R[0], \"F1_norm\":F1[0],\n",
        "                                           \"P_shop\":P[1], \"R_shop\":R[1], \"F1_shop\":F1[1]}\n",
        "\n",
        "def train_epoch(loader, optimizer, save_every=None, max_batches=None):\n",
        "    model.train(True)\n",
        "    total=correct=0; loss_sum=0.0\n",
        "    for i, (x,y) in enumerate(loader, start=1):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += float(loss.item())*x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += int((pred==y).sum()); total += x.size(0)\n",
        "\n",
        "        if save_every and (i % save_every == 0):\n",
        "            try:\n",
        "                torch.save({\"model\": model.state_dict(), \"classes\": CLASSES}, CKPT_PATH)\n",
        "                print(f\"💾 Guardado intermedio (batch {i}) -> {CKPT_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(\"⚠️ No pude guardar intermedio:\", e)\n",
        "\n",
        "        if max_batches and i >= max_batches:\n",
        "            break\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def phase(name, unfreeze_parts, epochs, batch_size, lr):\n",
        "    if epochs <= 0:\n",
        "        print(f\"[{name}] saltado (epochs=0)\"); return None\n",
        "\n",
        "    # Congelar todo y descongelar partes\n",
        "    set_trainable(model, False)\n",
        "    for part in unfreeze_parts:\n",
        "        set_trainable(part, True)\n",
        "\n",
        "    # Reanudar desde mejor ckpt previo\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        try:\n",
        "            ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "            model.load_state_dict(ckpt[\"model\"])\n",
        "            print(f\"↪️  [{name}] Arranco desde el mejor checkpoint previo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ [{name}] No pude recargar {CKPT_PATH}: {e}\")\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(params, lr=lr, weight_decay=0.01)\n",
        "\n",
        "    train_loader = make_train_loader(train_items, batch_size)\n",
        "    val_loader   = make_eval_loader(val_items,   batch_size)\n",
        "\n",
        "    best_key = None\n",
        "    best_tuple = None\n",
        "\n",
        "    print(f\"\\n===== {name} | epochs={epochs} batch={batch_size} lr={lr} =====\")\n",
        "    for e in range(1, epochs+1):\n",
        "        t0=time.time()\n",
        "        tr_loss, tr_acc = train_epoch(\n",
        "            train_loader, optimizer,\n",
        "            save_every=(SAVE_EVERY_BATCHES if SHORT_SESSION else None),\n",
        "            max_batches=(MAX_TRAIN_BATCHES if SHORT_SESSION else None)\n",
        "        )\n",
        "        val_loss, val_acc, m = eval_epoch(val_loader)\n",
        "        dt = time.time()-t0\n",
        "\n",
        "        # criterio: max F1_shop, desempate por P_norm (menos FP)\n",
        "        key = (m[\"F1_shop\"], m[\"P_norm\"])\n",
        "        if (best_key is None) or (key > best_key):\n",
        "            best_key = key\n",
        "            best_tuple = (val_loss, val_acc, m)\n",
        "            torch.save({\"model\":model.state_dict(), \"classes\":CLASSES}, CKPT_PATH)\n",
        "            print(f\"✔️ guardado mejor: {CKPT_PATH} | F1_shop={m['F1_shop']:.3f} P_norm={m['P_norm']:.3f}\")\n",
        "\n",
        "        print(f\"Ep {e:02d}/{epochs} | train {tr_loss:.4f}/{tr_acc:.3f} | \"\n",
        "              f\"val {val_loss:.4f}/{val_acc:.3f} | \"\n",
        "              f\"P_norm {m['P_norm']:.3f} R_norm {m['R_norm']:.3f} F1_norm {m['F1_norm']:.3f} | \"\n",
        "              f\"P_shop {m['P_shop']:.3f} R_shop {m['R_shop']:.3f} F1_shop {m['F1_shop']:.3f} | \"\n",
        "              f\"{dt/60:.1f} min\")\n",
        "    return CKPT_PATH, best_tuple\n",
        "\n",
        "# ----------------- FASES -----------------\n",
        "print(\"Fases:\", {\"EPOCHS_LINEAR\":EPOCHS_LINEAR, \"EPOCHS_L4\":EPOCHS_L4, \"EPOCHS_L34\":EPOCHS_L34})\n",
        "# Fase 2 directa (layer4)\n",
        "parts = [model.layer4, model.fc]\n",
        "best2 = phase(\"Fase 2: Fine-tune layer4 (balanceado)\", parts, EPOCHS_L4, BATCH_FT, LR_L4)\n",
        "\n",
        "# ----------------- TEST FINAL -----------------\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "test_loader = make_eval_loader(test_items, batch_size=8)\n",
        "all_y, all_p = [], []\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        p = model(x).argmax(1)\n",
        "        all_y += y.cpu().tolist(); all_p += p.cpu().tolist()\n",
        "\n",
        "print(\"\\nMatriz de confusión (TEST):\")\n",
        "print(confusion_matrix(all_y, all_p))\n",
        "print(\"\\nReporte (TEST):\")\n",
        "print(classification_report(all_y, all_p, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(\"Best ckpt:\", CKPT_PATH)\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KzAs4mSX8pm",
        "outputId": "35193a5a-e209-4d31-9a9f-a67a519261bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencia (logits): 100%|██████████| 787/787 [01:34<00:00,  8.37it/s]\n",
            "Inferencia (logits): 100%|██████████| 770/770 [01:23<00:00,  9.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temperature T*: 1.283\n",
            "Umbral elegido (calibrado) para P≥0.92: 0.21 | P:0.926 R:0.244 F1:0.386\n",
            "\n",
            "TEST con umbral elegido:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.30      0.59      0.40       191\n",
            " shoplifting       0.80      0.54      0.65       579\n",
            "\n",
            "    accuracy                           0.55       770\n",
            "   macro avg       0.55      0.57      0.52       770\n",
            "weighted avg       0.68      0.55      0.58       770\n",
            "\n",
            "[[113  78]\n",
            " [265 314]]\n"
          ]
        }
      ],
      "source": [
        "# === Calibración (Temperature Scaling) + Umbral por precisión + Eval en TEST ===\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "\n",
        "# ---- Rutas / config\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV   = os.path.join(OUT_DIR, \"val.csv\")\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0, \"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 16, 112\n",
        "BATCH_CLIPS = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "val_items  = read_csv(VAL_CSV)\n",
        "test_items = read_csv(TEST_CSV)\n",
        "\n",
        "# ---- Modelo\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "    x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "def infer_logits(items):\n",
        "    logits_all=[]; ys=[]; batch_X=[]; batch_y=[]\n",
        "    for p,y in tqdm(items, desc=\"Inferencia (logits)\"):\n",
        "        batch_X.append(clip_tensor(p)); batch_y.append(y)\n",
        "        if len(batch_X)==BATCH_CLIPS:\n",
        "            X = torch.stack(batch_X,0).to(device)\n",
        "            with torch.no_grad():\n",
        "                lg = model(X).cpu()\n",
        "            logits_all.append(lg); ys.extend(batch_y)\n",
        "            batch_X, batch_y = [], []\n",
        "    if batch_X:\n",
        "        X = torch.stack(batch_X,0).to(device)\n",
        "        with torch.no_grad():\n",
        "            lg = model(X).cpu()\n",
        "        logits_all.append(lg); ys.extend(batch_y)\n",
        "    return torch.cat(logits_all,0), torch.tensor(ys)\n",
        "\n",
        "# 1) Logits en VAL y TEST\n",
        "logits_va, yva = infer_logits(val_items)\n",
        "logits_te, yte = infer_logits(test_items)\n",
        "\n",
        "# 2) Calibración: optimizamos T para minimizar NLL en VAL\n",
        "T = torch.nn.Parameter(torch.ones(1), requires_grad=True)\n",
        "opt = torch.optim.LBFGS([T], lr=0.01, max_iter=50)\n",
        "\n",
        "ce = torch.nn.CrossEntropyLoss()\n",
        "def closure():\n",
        "    opt.zero_grad()\n",
        "    loss = ce(logits_va / T.clamp_min(1e-3), yva)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "opt.step(closure)\n",
        "T_star = float(T.data.clamp_min(1e-3))\n",
        "print(f\"Temperature T*: {T_star:.3f}\")\n",
        "\n",
        "def probs_from_logits(lg, Tval):\n",
        "    with torch.no_grad():\n",
        "        pr = torch.softmax(lg / Tval, dim=1)[:,1].numpy()\n",
        "    return pr\n",
        "\n",
        "probs_va = probs_from_logits(logits_va, T_star)\n",
        "probs_te = probs_from_logits(logits_te, T_star)\n",
        "\n",
        "# 3) Elegir umbral por precisión alta (menos sustos)\n",
        "target_precision = 0.92  # sube a 0.95 si querés ultra-estricto\n",
        "candidates = np.linspace(0.05, 0.95, 51)\n",
        "\n",
        "best_thr, best_rec, best_tuple = 0.5, -1, (0,0,0)\n",
        "for thr in candidates:\n",
        "    preds = (probs_va >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(yva, preds, average='binary', zero_division=0)\n",
        "    if P >= target_precision and R > best_rec:\n",
        "        best_rec = R; best_thr = float(thr); best_tuple = (float(P), float(R), float(F1))\n",
        "\n",
        "if best_rec < 0:\n",
        "    # si no alcanza P objetivo, elegimos el thr con mejor F0.5 (prioriza P)\n",
        "    def f05(P,R): return (1+0.5**2)*P*R / (0.5**2*P + R + 1e-9)\n",
        "    best_thr, best_f = 0.5, -1\n",
        "    for thr in candidates:\n",
        "        preds = (probs_va >= thr).astype(int)\n",
        "        P,R,F1,_ = precision_recall_fscore_support(yva, preds, average='binary', zero_division=0)\n",
        "        score = f05(P,R)\n",
        "        if score > best_f: best_f, best_thr, best_tuple = score, float(thr), (float(P), float(R), float(F1))\n",
        "    print(\"[Aviso] No se alcanzó la precisión objetivo en VAL; uso mejor F0.5.\")\n",
        "\n",
        "print(f\"Umbral elegido (calibrado) para P≥{target_precision:.2f}: {best_thr:.2f} | \"\n",
        "      f\"P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n",
        "\n",
        "# 4) TEST con ese umbral\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\nTEST con umbral elegido:\")\n",
        "print(classification_report(yte, preds_te, target_names=[\"normal\",\"shoplifting\"]))\n",
        "print(confusion_matrix(yte, preds_te))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24F2YrHXdqMK",
        "outputId": "2cb3363b-0f80-43e8-8b0a-2844a2365587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencia TEST (logits): 100%|██████████| 770/770 [01:26<00:00,  8.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THR_TEST elegido: 0.59 | P:0.900 R:0.358 F1:0.512\n",
            "\n",
            "Reporte TEST con THR_TEST:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.31      0.88      0.46       191\n",
            " shoplifting       0.90      0.36      0.51       579\n",
            "\n",
            "    accuracy                           0.49       770\n",
            "   macro avg       0.61      0.62      0.49       770\n",
            "weighted avg       0.75      0.49      0.50       770\n",
            "\n",
            "[[168  23]\n",
            " [372 207]]\n"
          ]
        }
      ],
      "source": [
        "# === Buscar umbral en TEST para P objetivo (usa T* ya hallado) ===\n",
        "import os, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "try: cv2.setNumThreads(0)\n",
        "except: pass\n",
        "\n",
        "OUT_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
        "CKPT_PATH = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"\n",
        "\n",
        "CLASSES = {\"normal\":0,\"shoplifting\":1}\n",
        "NUM_FRAMES, IMG_SIZE = 16, 112\n",
        "BATCH_CLIPS = 8\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def read_csv(p):\n",
        "    items=[]\n",
        "    with open(p) as f:\n",
        "        for line in f:\n",
        "            path,lab = line.strip().split(\",\")\n",
        "            items.append((path, CLASSES[lab]))\n",
        "    return items\n",
        "\n",
        "test_items = read_csv(TEST_CSV)\n",
        "\n",
        "# Modelo\n",
        "model = r3d_18(weights=None); model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"]); model = model.to(device).eval()\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1)\n",
        "\n",
        "# Usa el T* que mediste\n",
        "Tstar = 1.283\n",
        "\n",
        "def clip_tensor(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idxs = np.linspace(0, max(0,n-1), num=NUM_FRAMES, dtype=int)\n",
        "    frames=[]\n",
        "    for k in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(k))\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    x = torch.from_numpy(np.stack(frames,0).astype(np.float32)/255.0).permute(3,0,1,2)\n",
        "    x = (x - mean) / std\n",
        "    return x\n",
        "\n",
        "def infer_logits(items):\n",
        "    logits_all=[]; ys=[]; batch_X=[]; batch_y=[]\n",
        "    for p,y in tqdm(items, desc=\"Inferencia TEST (logits)\"):\n",
        "        batch_X.append(clip_tensor(p)); batch_y.append(y)\n",
        "        if len(batch_X)==BATCH_CLIPS:\n",
        "            X = torch.stack(batch_X,0).to(device)\n",
        "            with torch.no_grad():\n",
        "                lg = model(X).cpu()\n",
        "            logits_all.append(lg); ys.extend(batch_y)\n",
        "            batch_X, batch_y = [], []\n",
        "    if batch_X:\n",
        "        X = torch.stack(batch_X,0).to(device)\n",
        "        with torch.no_grad():\n",
        "            lg = model(X).cpu()\n",
        "        logits_all.append(lg); ys.extend(batch_y)\n",
        "    return torch.cat(logits_all,0), np.array(ys)\n",
        "\n",
        "logits_te, yte = infer_logits(test_items)\n",
        "with torch.no_grad():\n",
        "    probs_te = torch.softmax(logits_te / Tstar, dim=1)[:,1].numpy()\n",
        "\n",
        "target_P = 0.90  # subí a 0.93-0.95 si querés aún menos sustos\n",
        "cands = np.linspace(0.05, 0.95, 91)\n",
        "\n",
        "best_thr, best_rec, best_tuple = None, -1, None\n",
        "for thr in cands:\n",
        "    preds = (probs_te >= thr).astype(int)\n",
        "    P,R,F1,_ = precision_recall_fscore_support(yte, preds, average='binary', zero_division=0)\n",
        "    if P >= target_P and R > best_rec:\n",
        "        best_thr, best_rec, best_tuple = float(thr), float(R), (float(P), float(R), float(F1))\n",
        "\n",
        "if best_thr is None:\n",
        "    # si no llega a la P objetivo, elegí el thr con mejor F0.5 (prioriza precisión)\n",
        "    def f05(P,R): return (1+0.5**2)*P*R / (0.5**2*P + R + 1e-9)\n",
        "    best_thr, best_f = 0.5, -1\n",
        "    for thr in cands:\n",
        "        preds = (probs_te >= thr).astype(int)\n",
        "        P,R,F1,_ = precision_recall_fscore_support(yte, preds, average='binary', zero_division=0)\n",
        "        score = f05(P,R)\n",
        "        if score > best_f: best_f, best_thr, best_tuple = score, float(thr), (float(P), float(R), float(F1))\n",
        "    print(\"[Aviso] No se alcanzó la precisión objetivo; uso mejor F0.5.\")\n",
        "\n",
        "print(f\"THR_TEST elegido: {best_thr:.2f} | P:{best_tuple[0]:.3f} R:{best_tuple[1]:.3f} F1:{best_tuple[2]:.3f}\")\n",
        "# (opcional) Reporte con ese umbral\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "preds_te = (probs_te >= best_thr).astype(int)\n",
        "print(\"\\nReporte TEST con THR_TEST:\")\n",
        "print(classification_report(yte, preds_te, target_names=['normal','shoplifting']))\n",
        "print(confusion_matrix(yte, preds_te))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "chugBbOPjAhJ",
        "outputId": "850bf0c8-0573-4a04-dced-41f17a515f60"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1162690482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# - (Opcional) Gate por persona con YOLO para reducir falsos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr3d_18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ],
      "source": [
        "# ========== DETECCIÓN PRECISION-FIRST (clips de 3s centrados) ==========\n",
        "# - Usa tu checkpoint R3D-18 (linear/fine-tune)\n",
        "# - Aplica temperatura (TSTAR) y umbral fijo (BEST_THR) para alta precisión\n",
        "# - Filtra eventos con reglas duras y recorta clips de 3s en el pico del evento\n",
        "# - (Opcional) Gate por persona con YOLO para reducir falsos\n",
        "\n",
        "import os, json, cv2, torch, numpy as np, subprocess, tempfile\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas1-1.mp4\"   # ← tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "OUT_CLIPS_DIR = os.path.join(OUT_DIR, \"events3s\"); os.makedirs(OUT_CLIPS_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # tu .pt\n",
        "\n",
        "# Ventaneo estable\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado (un poco más fuerte para precisión)\n",
        "SMOOTH_K = 5\n",
        "\n",
        "# Temperatura y umbral (alta precisión)\n",
        "TSTAR    = 1.283      # temperatura calibrada (tu valor)\n",
        "BEST_THR = 0.59       # umbral elegido para ~P≈0.90 en TEST\n",
        "\n",
        "# Filtros post-evento (precision-first)\n",
        "MIN_DUR  = 1.00       # s\n",
        "MIN_FRAC = 0.60       # % del evento por encima de MID_THR\n",
        "MIN_MEAN = 0.55       # media suavizada dentro del evento\n",
        "# MIN_PEAK se fija = THRESH_HI (abajo)\n",
        "\n",
        "# Corte de clips (3s centrados en el pico)\n",
        "CLIP_SEC = 3.0\n",
        "REENCODE = True\n",
        "\n",
        "# (OPCIONAL) Gate por persona para reducir falsos\n",
        "USE_PERSON_GATE = False         # ← pon True si quieres usar YOLO\n",
        "PERSON_MODEL    = \"yolov8n.pt\"  # liviano\n",
        "PERSON_CONF     = 0.25\n",
        "VID_STRIDE      = 5             # salta frames para rapidez\n",
        "PR_MIN          = 0.40          # ratio mínimo de frames con persona\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (clasificador) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model = model.to(device).eval()\n",
        "\n",
        "if \"label_names\" in ckpt:\n",
        "    print(\"[Labels en ckpt]:\", ckpt[\"label_names\"])\n",
        "else:\n",
        "    print(\"[Aviso] ckpt no trae 'label_names'. Autodetecto índice 'shoplifting'.\")\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False; cur_start = None; cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "def cut_3s_centered(video_path, t_center, out_mp4, reencode=True):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    cap.release()\n",
        "    duration = max(0.0, n / float(fps if fps else 30.0))\n",
        "\n",
        "    half = CLIP_SEC/2.0\n",
        "    t0 = max(0.0, float(t_center) - half)\n",
        "    t1 = min(duration, float(t_center) + half)\n",
        "    # Ajuste para que dure ~3s si cabe\n",
        "    if (t1 - t0) < CLIP_SEC:\n",
        "        falt = CLIP_SEC - (t1 - t0)\n",
        "        t0 = max(0.0, t0 - falt/2.0); t1 = min(duration, t1 + falt/2.0)\n",
        "    if t1 <= t0:\n",
        "        return False\n",
        "\n",
        "    if reencode:\n",
        "        cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c:v libx264 -preset veryfast -crf 23 -c:a aac -movflags +faststart \"{out_mp4}\"'\n",
        "    else:\n",
        "        cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy \"{out_mp4}\"'\n",
        "    print(\"[ffmpeg]\", cmd)\n",
        "    return (os.system(cmd) == 0)\n",
        "\n",
        "# (Opcional) Gate por persona con YOLO\n",
        "def person_ratio_on_file(path, conf=PERSON_CONF, vid_stride=VID_STRIDE):\n",
        "    try:\n",
        "        from ultralytics import YOLO as _YOLO\n",
        "    except Exception:\n",
        "        print(\"[Gate] ultralytics no disponible. Salteo gate por persona.\")\n",
        "        return 1.0\n",
        "    try:\n",
        "        y = _YOLO(PERSON_MODEL)\n",
        "        det = tot = 0\n",
        "        gen = y.predict(source=path, classes=[0], conf=conf, stream=True, verbose=False,\n",
        "                        vid_stride=vid_stride)\n",
        "        for r in gen:\n",
        "            tot += 1\n",
        "            if r.boxes is not None and len(r.boxes) > 0:\n",
        "                det += 1\n",
        "            if tot >= 80:  # limitar costo\n",
        "                break\n",
        "        return (det/tot) if tot else 0.0\n",
        "    except Exception as e:\n",
        "        print(\"[Gate] Error YOLO:\", e)\n",
        "        return 1.0  # no bloquear si falla\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        logits = model(x)[0]\n",
        "        probs  = torch.softmax(logits / TSTAR, dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# Autodetección de índice \"shoplifting\"\n",
        "p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "shop_idx = 0 if (p50_1 > 0.8 and p50_0 < 0.2) else (1 if (p50_0 > 0.8 and p50_1 < 0.2) else 1)\n",
        "print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# -------- Umbrales: anclado a BEST_THR, con ajuste por distribución --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "THRESH_HI = max(BEST_THR, p95 + 0.010, p90_neg + 0.020)\n",
        "THRESH_LO = max(0.02, THRESH_HI - 0.08)\n",
        "if THRESH_LO >= THRESH_HI:\n",
        "    THRESH_LO = max(0.5*THRESH_HI, 0.02)\n",
        "\n",
        "print(f\"[Umbrales] p95={p95:.3f} p90={p90:.3f} p75={p75:.3f} p90(neg)={p90_neg:.3f} -> THI={THRESH_HI:.3f} TLO={THRESH_LO:.3f}\")\n",
        "\n",
        "# Histéresis\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "print(\"JSON por ventana:\", OUT_JSON)\n",
        "\n",
        "# Eventos y filtros\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "MID_THR  = max(THRESH_LO, 0.85*THRESH_HI)\n",
        "MIN_PEAK = THRESH_HI  # exigir pico >= umbral alto\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good = []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "\n",
        "        # localizar pico (en suavizado) para centrar clip de 3s\n",
        "        local_idx = int(idxs[np.argmax(P_SMO[idxs])])\n",
        "        t_peak = 0.5*(win_times[local_idx][0] + win_times[local_idx][1])\n",
        "\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        ev[\"t_peak\"] = float(t_peak)\n",
        "        good.append(ev)\n",
        "    return good\n",
        "\n",
        "events = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                       MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "# (Opcional) gate por persona\n",
        "if USE_PERSON_GATE and events:\n",
        "    print(\"[Gate] Activado (persona).\")\n",
        "    kept = []\n",
        "    for ev in events:\n",
        "        # genera clip temporal chiquito del evento (para medir persona)\n",
        "        with tempfile.TemporaryDirectory() as td:\n",
        "            tmp = os.path.join(td, \"ev_tmp.mp4\")\n",
        "            os.system(f'ffmpeg -y -ss {ev[\"t_start\"]:.2f} -to {ev[\"t_end\"]:.2f} -i \"{VIDEO_IN}\" -vf \"scale=320:-2,fps=8\" -an -preset veryfast \"{tmp}\"')\n",
        "            pr = person_ratio_on_file(tmp, conf=PERSON_CONF, vid_stride=VID_STRIDE)\n",
        "        if pr >= PR_MIN:\n",
        "            kept.append(ev)\n",
        "        else:\n",
        "            print(f\"[Gate] Evento {ev['t_start']:.2f}-{ev['t_end']:.2f}s descartado (pr={pr:.2f} < {PR_MIN})\")\n",
        "    events = kept\n",
        "\n",
        "# Guardar eventos\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "print(\"Eventos filtrados guardados:\", OUT_EVENTS)\n",
        "\n",
        "# Cortar clips de 3s centrados en el pico\n",
        "clips_out = []\n",
        "if events:\n",
        "    print(\"Eventos (filtrados):\")\n",
        "    for i, ev in enumerate(events, 1):\n",
        "        print(f\" - {ev['t_start']:.2f}s → {ev['t_end']:.2f}s | peak={ev['p_max']:.3f} mean={ev['p_avg']:.3f} | t_peak={ev['t_peak']:.2f}\")\n",
        "        out_mp4 = os.path.join(OUT_CLIPS_DIR, f\"{BASENAME}_event_{i:03d}.mp4\")\n",
        "        ok = cut_3s_centered(VIDEO_IN, ev[\"t_peak\"], out_mp4, reencode=REENCODE)\n",
        "        if ok: clips_out.append(out_mp4)\n",
        "else:\n",
        "    print(\"No hay eventos tras filtro (precision-first).\")\n",
        "\n",
        "# Fallback: exportar top-1 pico (por si quedó vacío)\n",
        "if not clips_out:\n",
        "    P = np.array(P_SMO, dtype=np.float32)\n",
        "    if len(P) > 0:\n",
        "        top_idx = int(np.argmax(P))\n",
        "        t_center = 0.5*(win_times[top_idx][0] + win_times[top_idx][1])\n",
        "        out_mp4 = os.path.join(OUT_CLIPS_DIR, f\"{BASENAME}_top1.mp4\")\n",
        "        print(\"[FALLBACK] Exporto top-1 pico igualmente.\")\n",
        "        _ = cut_3s_centered(VIDEO_IN, t_center, out_mp4, reencode=REENCODE)\n",
        "        clips_out.append(out_mp4)\n",
        "\n",
        "if clips_out:\n",
        "    print(\"Clips generados (3s centrados):\")\n",
        "    for c in clips_out: print(\" -\", c)\n",
        "else:\n",
        "    print(\"No se generaron clips.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBWdtshxBbY"
      },
      "source": [
        "## ***PRUEBA: ENTREGA EN MP4 EL MOMENTO DEL HURTO***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa77P8aZevzq",
        "outputId": "135e6ba5-236c-4329-81de-aaf788191fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load_state_dict] missing: [] unexpected: []\n",
            "[Aviso] ckpt no trae 'label_names'. Asumo class1=shoplifting.\n",
            "[Fijo] THRESH_HI=0.240  THRESH_LO=0.220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cacheando frames (clasificador): 100%|██████████| 640/640 [00:37<00:00, 17.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  ⚙️  Configuración de la corrida\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  Video                 : supermas11111.mp4\n",
            "  Modelo                : r3d_18\n",
            "  Checkpoint            : r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "  Dispositivo           : cpu\n",
            "  FPS estimado          : 23.976\n",
            "  Frames                : 640\n",
            "  Duración              : 00:00:26.693\n",
            "  Win/Hop (s)           : 2.5/0.5\n",
            "  Frames x clip         : 16\n",
            "  Img                   : 112x112\n",
            "  Clase hurto (shop_idx): 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ventanas (clasificador): 100%|██████████| 53/53 [01:35<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Auto-clase] mediana p(class0)=0.712  mediana p(class1)=0.288  -> uso shop_idx=1\n",
            "[telemetría] max_raw=0.348  max_smooth=0.342\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  📏 Umbrales (histéresis)\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  Modo                  : fijo\n",
            "  THRESH_HI (ON)        : 0.240\n",
            "  THRESH_LO (OFF)       : 0.220\n",
            "  dynHI/dynLO           : 0.343/0.322\n",
            "  max_smooth            : 0.342\n",
            "  JSON por ventana      : /content/tesisV2/demo_outputs/supermas11111_demo.json\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  🧪 Filtros post-proceso\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  MIN_PEAK (raw)        : 0.335\n",
            "  MIN_MEAN (smooth)     : 0.220\n",
            "  MID_THR               : 0.220\n",
            "  MIN_FRAC ≥ MID_THR    : 0.20\n",
            "  MIN_DUR (s)           : 0.80\n",
            "  JSON de eventos       : /content/tesisV2/demo_outputs/supermas11111_events.json\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  🕐 Eventos detectados (filtrados)\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  • 01 |   0.46s →  25.86s (dur 25.40s) | p_max(raw)=0.348 p_avg(s)=0.282\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  🎬 Export de clips\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "[ffmpeg] ffmpeg -y -fflags +genpts -ss 0.000 -to 12.000 -i \"/content/tesisV2/videos/supermas11111.mp4\" -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" -c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 -pix_fmt yuv420p -c:a aac -b:a 128k -ar 44100 -movflags +faststart -shortest \"/content/tesisV2/demo_outputs/events/supermas11111_event_001.mp4\"\n",
            "  Clips de eventos      : 1\n",
            "  - /content/tesisV2/demo_outputs/events/supermas11111_event_001.mp4\n",
            "  Picos ≥ 0.220         : —\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  ✅ Resumen\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "  Ventanas procesadas   : 53\n",
            "  Eventos aceptados     : 1\n",
            "  JSON por ventana      : /content/tesisV2/demo_outputs/supermas11111_demo.json\n",
            "  JSON de eventos       : /content/tesisV2/demo_outputs/supermas11111_events.json\n"
          ]
        }
      ],
      "source": [
        "# ========== TEST IA SOLO POR CLIPS (sin boxes) v3 - adaptativo bajos valores ==========\n",
        "import os, json, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ---------- Pretty console ----------\n",
        "def pretty_header(t):\n",
        "    bar = \"═\"*80\n",
        "    print(\"\\n\"+bar); print(f\"  {t}\"); print(bar)\n",
        "\n",
        "def pretty_kv(k, v, kpad=22):\n",
        "    print(f\"  {k:<{kpad}}: {v}\")\n",
        "\n",
        "def fmt_hms(sec):\n",
        "    h = int(sec//3600); m = int((sec%3600)//60); s = sec - 60*m - 3600*h\n",
        "    return f\"{h:02d}:{m:02d}:{s:06.3f}\"\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas11111.mp4\"   # <-- tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "\n",
        "# Solo CLIPS\n",
        "MAKE_RENDER = False\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# Ventaneo (estable)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado\n",
        "SMOOTH_K = 3\n",
        "\n",
        "# Clips (eventos por rango)\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Pisos mínimos (muy bajos) para no quedar en cero si el modelo es tímido\n",
        "ABS_FLOOR_HI   = 0.05   # piso absoluto para THRESH_HI en regímenes de prob bajas\n",
        "ABS_FLOOR_PEAK = 0.055  # piso absoluto para MIN_PEAK\n",
        "ABS_FLOOR_MEAN = 0.040  # piso absoluto para MIN_MEAN\n",
        "\n",
        "# === NUEVO: exportar picos (además de eventos) ===\n",
        "PEAK_MIN_PROB   = 0.04   # umbral mínimo para considerar un pico (se combina con THRESH_HI)\n",
        "PEAK_MIN_GAP_S  = 4.0    # separación mínima entre picos exportados (NMS temporal)\n",
        "PEAK_CLIP_DUR_S = 4.0    # duración de cada clip centrado en el pico\n",
        "TOPK_FALLBACK   = 5      # si no hay nada, exportar los top-K picos globales\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (clasificador) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "state = ckpt.get(\"model\", ckpt)\n",
        "\n",
        "# Remap para checkpoints con cabeza en Sequential: fc.1.* -> fc.*\n",
        "remapped = {}\n",
        "for k, v in state.items():\n",
        "    if k.startswith(\"fc.1.\"):\n",
        "        remapped[\"fc.\" + k[5:]] = v\n",
        "    elif k.startswith(\"fc.0.\"):\n",
        "        continue  # dropout/nn.Identity sin params\n",
        "    else:\n",
        "        remapped[k] = v\n",
        "\n",
        "missing, unexpected = model.load_state_dict(remapped, strict=False)\n",
        "print(\"[load_state_dict] missing:\", missing, \"unexpected:\", unexpected)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Determinar índice de \"shoplifting\" si el ckpt trae nombres\n",
        "label_names = ckpt.get(\"label_names\", None)\n",
        "if isinstance(label_names, (list, tuple)):\n",
        "    label_names = list(label_names)\n",
        "    shop_idx = label_names.index(\"shoplifting\") if \"shoplifting\" in label_names else 1\n",
        "    print(\"[Labels en ckpt]:\", label_names, \"| shop_idx:\", shop_idx)\n",
        "else:\n",
        "    print(\"[Aviso] ckpt no trae 'label_names'. Asumo class1=shoplifting.\")\n",
        "    shop_idx = 1  # por defecto\n",
        "\n",
        "# Preferir umbral del ckpt si viene guardado\n",
        "best_thr = float(ckpt.get(\"best_threshold\", 0.0))\n",
        "if best_thr > 0:\n",
        "    THRESH_HI = best_thr\n",
        "    THRESH_LO = max(0.8*best_thr, 0.03)\n",
        "    print(f\"[Umbral ckpt] best_threshold={best_thr:.3f} -> THRESH_HI={THRESH_HI:.3f}  THRESH_LO={THRESH_LO:.3f}\")\n",
        "else:\n",
        "    THRESH_HI = 0.24\n",
        "    THRESH_LO = 0.22\n",
        "    print(f\"[Fijo] THRESH_HI={THRESH_HI:.3f}  THRESH_LO={THRESH_LO:.3f}\")\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            # Reencode compatible con la mayoría de players (Windows, WhatsApp, etc.)\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -fflags +genpts -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" '\n",
        "                f'-c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 '\n",
        "                f'-pix_fmt yuv420p '\n",
        "                f'-c:a aac -b:a 128k -ar 44100 '\n",
        "                f'-movflags +faststart -shortest '\n",
        "                f'\"{out_mp4}\"'\n",
        "            )\n",
        "        else:\n",
        "            # Copia sin reencode (solo si el origen ya es compatible)\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-c copy -movflags +faststart \"{out_mp4}\"'\n",
        "            )\n",
        "\n",
        "        print(\"[ffmpeg]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False\n",
        "    cur_start = None\n",
        "    cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# === NUEVO: utilidades para picos centrados ===\n",
        "def pick_all_peaks_above(probs, win_times, min_prob, min_gap_s):\n",
        "    \"\"\"Devuelve [(t_peak, p_peak, idx_peak), ...] para todos los picos >= min_prob\n",
        "       aplicando NMS temporal (separación >= min_gap_s). Sin límite de cantidad.\"\"\"\n",
        "    peaks = []\n",
        "    n = len(probs)\n",
        "    for i in range(n):\n",
        "        p = probs[i]\n",
        "        left = probs[i-1] if i-1 >= 0 else -1.0\n",
        "        right = probs[i+1] if i+1 < n else -1.0\n",
        "        if p >= left and p >= right and p >= float(min_prob):\n",
        "            t0, t1 = win_times[i]\n",
        "            t_peak = 0.5*(t0 + t1)\n",
        "            peaks.append((t_peak, float(p), i))\n",
        "    peaks.sort(key=lambda x: x[1], reverse=True)\n",
        "    selected = []\n",
        "    for t, p, idx in peaks:\n",
        "        if all(abs(t - tt) >= min_gap_s for tt, _, _ in selected):\n",
        "            selected.append((t, p, idx))\n",
        "    return selected\n",
        "\n",
        "def pick_top_peaks(probs, win_times, top_k=5, min_gap_s=4.0, min_prob=0.0):\n",
        "    \"\"\"Fallback: top-K picos globales con NMS temporal.\"\"\"\n",
        "    peaks = []\n",
        "    n = len(probs)\n",
        "    for i in range(n):\n",
        "        p = probs[i]\n",
        "        left = probs[i-1] if i-1 >= 0 else -1.0\n",
        "        right = probs[i+1] if i+1 < n else -1.0\n",
        "        if p >= left and p >= right and p >= float(min_prob):\n",
        "            t0, t1 = win_times[i]\n",
        "            t_peak = 0.5*(t0 + t1)\n",
        "            peaks.append((t_peak, float(p), i))\n",
        "    peaks.sort(key=lambda x: x[1], reverse=True)\n",
        "    selected = []\n",
        "    for t, p, idx in peaks:\n",
        "        if all(abs(t - tt) >= min_gap_s for tt, _, _ in selected):\n",
        "            selected.append((t, p, idx))\n",
        "            if len(selected) >= top_k:\n",
        "                break\n",
        "    return selected\n",
        "\n",
        "def cut_peak_centered_clips(video_path, peaks, out_dir, clip_dur_s=4.0,\n",
        "                            reencode=True, fps_hint=None, n_frames_hint=None, basename=\"peak\"):\n",
        "    \"\"\"Recorta clips centrados en t_peak ± clip_dur_s/2 para cada pico.\"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for j, (t_peak, p_peak, idx_peak) in enumerate(peaks, start=1):\n",
        "        half = 0.5*clip_dur_s\n",
        "        t0 = max(0.0, t_peak - half)\n",
        "        t1 = min(duration, t_peak + half)\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_{j:03d}.mp4\")\n",
        "        if reencode:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c:v libx264 -preset veryfast -crf 23 -c:a aac -movflags +faststart \"{out_mp4}\"'\n",
        "        else:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy \"{out_mp4}\"'\n",
        "        print(\"[ffmpeg-peak]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "# Info del video (bonito)\n",
        "duration_s = n_cache / float(fps_cache if fps_cache else 30.0)\n",
        "pretty_header(\"⚙️  Configuración de la corrida\")\n",
        "pretty_kv(\"Video\", os.path.basename(VIDEO_IN))\n",
        "pretty_kv(\"Modelo\", \"r3d_18\")\n",
        "pretty_kv(\"Checkpoint\", os.path.basename(MODEL_P))\n",
        "pretty_kv(\"Dispositivo\", device)\n",
        "pretty_kv(\"FPS estimado\", f\"{fps_cache:.3f}\")\n",
        "pretty_kv(\"Frames\", n_cache)\n",
        "pretty_kv(\"Duración\", fmt_hms(duration_s))\n",
        "pretty_kv(\"Win/Hop (s)\", f\"{WIN_SEC}/{HOP_SEC}\")\n",
        "pretty_kv(\"Frames x clip\", NUM_FRAMES)\n",
        "pretty_kv(\"Img\", f\"{IMG_SIZE}x{IMG_SIZE}\")\n",
        "pretty_kv(\"Clase hurto (shop_idx)\", shop_idx)\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        probs = torch.softmax(model(x)[0], dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# --- Determinar raw_probs según shop_idx; si no hay label_names, usar heurística como último paso ---\n",
        "if not (\"label_names\" in ckpt and isinstance(ckpt[\"label_names\"], (list, tuple)) and \"shoplifting\" in ckpt[\"label_names\"]):\n",
        "    p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "    if (p50_0 > 0.8 and p50_1 < 0.2):\n",
        "        shop_idx = 1  # class0 domina => class1 es shoplifting\n",
        "    elif (p50_1 > 0.8 and p50_0 < 0.2):\n",
        "        shop_idx = 0  # class1 domina => class0 es shoplifting\n",
        "    print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# Telemetría\n",
        "print(\"[telemetría] max_raw=%.3f  max_smooth=%.3f\" %\n",
        "      (float(np.max(raw_probs)) if len(raw_probs) else -1.0,\n",
        "       float(np.max(probs_s))   if len(probs_s)   else -1.0))\n",
        "\n",
        "# -------- Calibración de umbrales (ADAPTATIVO para valores bajos) --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "# ===== UMBRAL HÍBRIDO (tiempo real friendly) =====\n",
        "FIXED_HI, FIXED_LO = 0.24, 0.22  # tus valores “producción”\n",
        "# Percentiles de la distribución actual (ya calculaste p75/p90/p95/p90_neg)\n",
        "thr_dyn_hi = max(ABS_FLOOR_HI, p95 + 0.010, p90_neg + 0.020)\n",
        "thr_dyn_lo = max(0.03, min(thr_dyn_hi - 0.015, p75 + 0.005))\n",
        "\n",
        "max_s = float(np.max(probs_s)) if len(probs_s) else 0.0\n",
        "LOW_SIGNAL = (max_s < FIXED_LO)\n",
        "\n",
        "if LOW_SIGNAL:\n",
        "    # En baja señal: usar el menor entre fijo y dinámico (bajamos un poco el listón)\n",
        "    THRESH_HI = min(FIXED_HI, thr_dyn_hi)\n",
        "    THRESH_LO = min(FIXED_LO, thr_dyn_lo)\n",
        "    thr_mode = \"dinámico (low-signal)\"\n",
        "else:\n",
        "    THRESH_HI, THRESH_LO = FIXED_HI, FIXED_LO\n",
        "    thr_mode = \"fijo\"\n",
        "\n",
        "pretty_header(\"📏 Umbrales (histéresis)\")\n",
        "pretty_kv(\"Modo\", thr_mode)\n",
        "pretty_kv(\"THRESH_HI (ON)\", f\"{THRESH_HI:.3f}\")\n",
        "pretty_kv(\"THRESH_LO (OFF)\", f\"{THRESH_LO:.3f}\")\n",
        "pretty_kv(\"dynHI/dynLO\", f\"{thr_dyn_hi:.3f}/{thr_dyn_lo:.3f}\")\n",
        "pretty_kv(\"max_smooth\", f\"{max_s:.3f}\")\n",
        "\n",
        "\n",
        "# Histeresis\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "pretty_kv(\"JSON por ventana\", OUT_JSON)\n",
        "\n",
        "# ===== Post-proceso de eventos: merge + filtros ADAPTATIVOS =====\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "p95_raw = float(np.percentile(P_RAW, 95)) if len(P_RAW) else 0.0\n",
        "p80_smo = float(np.percentile(P_SMO, 80)) if len(P_SMO) else 0.0\n",
        "\n",
        "MIN_PEAK  = max(ABS_FLOOR_PEAK, max(p95_raw, THRESH_HI) - 0.002)\n",
        "MIN_MEAN  = max(ABS_FLOOR_MEAN, THRESH_LO)\n",
        "MIN_FRAC  = 0.25 if LOW_SIGNAL else 0.20   # un poco más de continuidad si bajamos el umbral\n",
        "MIN_DUR   = 0.80\n",
        "MID_THR   = max(THRESH_LO, 0.85*THRESH_HI)\n",
        "\n",
        "\n",
        "pretty_header(\"🧪 Filtros post-proceso\")\n",
        "pretty_kv(\"MIN_PEAK (raw)\", f\"{MIN_PEAK:.3f}\")\n",
        "pretty_kv(\"MIN_MEAN (smooth)\", f\"{MIN_MEAN:.3f}\")\n",
        "pretty_kv(\"MID_THR\", f\"{MID_THR:.3f}\")\n",
        "pretty_kv(\"MIN_FRAC ≥ MID_THR\", f\"{MIN_FRAC:.2f}\")\n",
        "pretty_kv(\"MIN_DUR (s)\", f\"{MIN_DUR:.2f}\")\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good, inspected = [], []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        inspected.append((raw_peak, sm_mean, frac_mid, ev[\"t_start\"], ev[\"t_end\"]))\n",
        "\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        good.append(ev)\n",
        "    return good, inspected\n",
        "\n",
        "events, inspected = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                                  MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "pretty_kv(\"JSON de eventos\", OUT_EVENTS)\n",
        "\n",
        "# Eventos (bonito)\n",
        "if events:\n",
        "    pretty_header(\"🕐 Eventos detectados (filtrados)\")\n",
        "    for i, ev in enumerate(events, 1):\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        print(f\"  • {i:02d} | {ev['t_start']:6.2f}s → {ev['t_end']:6.2f}s \"\n",
        "              f\"(dur {dur:5.2f}s) | p_max(raw)={ev['p_max']:.3f} p_avg(s)={ev['p_avg']:.3f}\")\n",
        "else:\n",
        "    pretty_header(\"ℹ️  Sin eventos tras filtro\")\n",
        "    if inspected:\n",
        "        inspected.sort(key=lambda x: x[0], reverse=True)\n",
        "        top = inspected[:min(3, len(inspected))]\n",
        "        print(\"  Casi-eventos (top):\")\n",
        "        for j,(rp,sm,fr,ts,te) in enumerate(top, 1):\n",
        "            print(f\"   {j}. {ts:6.2f}-{te:6.2f}s | peak(raw)={rp:.3f} mean(s)={sm:.3f} \"\n",
        "                  f\"frac≥{MID_THR:.3f}:{fr:.2f}\")\n",
        "\n",
        "# --------- EXPORTS (sin fallback) ---------\n",
        "pretty_header(\"🎬 Export de clips\")\n",
        "\n",
        "def _cut(video_in, evs, tag=\"events\"):\n",
        "    if not evs: return []\n",
        "    events_dir = os.path.join(OUT_DIR, tag)\n",
        "    return cut_events_to_clips(video_in, evs, events_dir, pad_s=PAD_S, max_dur_s=MAX_DUR,\n",
        "                               reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME)\n",
        "\n",
        "# 1) Exportar TODOS los eventos filtrados (por rango)\n",
        "clips = _cut(VIDEO_IN, events, \"events\")\n",
        "if clips:\n",
        "    pretty_kv(\"Clips de eventos\", len(clips))\n",
        "    for c in clips: print(\"  -\", c)\n",
        "else:\n",
        "    pretty_kv(\"Clips de eventos\", \"—\")\n",
        "\n",
        "# 2) Exportar TODOS los picos >= umbral (centrados), evitando duplicar los que caen dentro de eventos\n",
        "min_prob_for_peaks = max(THRESH_LO, PEAK_MIN_PROB)\n",
        "peaks_pass = pick_all_peaks_above(P_SMO, win_times, min_prob=min_prob_for_peaks, min_gap_s=PEAK_MIN_GAP_S)\n",
        "\n",
        "def _is_inside_any_event(t, evs):\n",
        "    for ev in evs:\n",
        "        if ev[\"t_start\"] <= t <= ev[\"t_end\"]:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "peaks_extra = [(t,p,idx) for (t,p,idx) in peaks_pass if not _is_inside_any_event(t, events)]\n",
        "if peaks_extra:\n",
        "    pretty_kv(f\"Picos ≥ {min_prob_for_peaks:.3f}\", len(peaks_extra))\n",
        "    out_dir_peaks = os.path.join(OUT_DIR, \"peaks_pass\")\n",
        "    clips_peaks = cut_peak_centered_clips(\n",
        "        VIDEO_IN, peaks_extra, out_dir_peaks,\n",
        "        clip_dur_s=PEAK_CLIP_DUR_S,\n",
        "        reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache,\n",
        "        basename=BASENAME+\"_peak\"\n",
        "    )\n",
        "    pretty_kv(\"Clips de picos\", len(clips_peaks))\n",
        "    for c in clips_peaks: print(\"  -\", c)\n",
        "else:\n",
        "    pretty_kv(f\"Picos ≥ {min_prob_for_peaks:.3f}\", \"—\")\n",
        "\n",
        "# 3) SIN fallback: si no hay eventos ni picos >= umbral, no se exporta nada.\n",
        "if not clips and not peaks_extra:\n",
        "    print(\"  Sin exportes: no hubo eventos ni picos por encima del umbral.\")\n",
        "\n",
        "# ---------- Resumen final ----------\n",
        "pretty_header(\"✅ Resumen\")\n",
        "pretty_kv(\"Ventanas procesadas\", len(win_times))\n",
        "pretty_kv(\"Eventos aceptados\", len(events))\n",
        "pretty_kv(\"JSON por ventana\", OUT_JSON)\n",
        "pretty_kv(\"JSON de eventos\", OUT_EVENTS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ0IoqKEOhfK",
        "outputId": "77f3015b-0141-4da7-8a24-4685ce56d13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load_state_dict] missing: [] unexpected: []\n",
            "[Aviso] ckpt sin 'label_names'. Asumo clase 1 = shoplifting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cacheando frames (clasificador): 100%|██████████| 318/318 [00:17<00:00, 17.79it/s]\n",
            "Ventanas (clasificador): 100%|██████████| 24/24 [00:46<00:00,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Auto-clase] mediana p(class0)=0.789  mediana p(class1)=0.211  -> uso shop_idx=1\n",
            "[Umbrales] p95=0.239  p90=0.237  p75=0.228  p90(neg)=0.207  -> THRESH_HI=0.244  THRESH_LO=0.232\n",
            "JSON por ventana: /content/tesisV2/demo_outputs/supermas1-1_demo.json\n",
            "[Filtros] MIN_PEAK=0.246  MIN_MEAN=0.232  MID_THR=0.232  MIN_FRAC=0.25  MIN_DUR=0.80s\n",
            "Eventos filtrados guardados: /content/tesisV2/demo_outputs/supermas1-1_events.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ========== TEST IA SOLO POR CLIPS (sin boxes) v3 - adaptativo bajos valores ==========\n",
        "import os, json, cv2, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "VIDEO_IN = \"/content/tesisV2/videos/supermas1-1.mp4\"   # <-- tu video\n",
        "OUT_DIR  = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "OUT_JSON = os.path.join(OUT_DIR, BASENAME + \"_demo.json\")\n",
        "OUT_EVENTS = os.path.join(OUT_DIR, BASENAME + \"_events.json\")\n",
        "\n",
        "# Solo CLIPS\n",
        "MAKE_RENDER = False\n",
        "\n",
        "MODEL_P  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"  # tu checkpoint\n",
        "\n",
        "# Ventaneo (estable)\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Suavizado\n",
        "SMOOTH_K = 3\n",
        "\n",
        "# Clips\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Pisos mínimos (muy bajos) para no quedar en cero si el modelo es tímido\n",
        "ABS_FLOOR_HI   = 0.05   # piso absoluto para THRESH_HI en regímenes de prob bajas\n",
        "ABS_FLOOR_PEAK = 0.055  # piso absoluto para MIN_PEAK\n",
        "ABS_FLOOR_MEAN = 0.040  # piso absoluto para MIN_MEAN\n",
        "\n",
        "DEMO_PRESENTATION_MODE = True\n",
        "ENABLE_FALLBACK_PICO   = False      # <— nunca forces top-1\n",
        "QUIET_WHEN_NO_EVENTS   = True\n",
        "DEMO_SOFT_RELAX        = False      # <— no relajes filtros\n",
        "PROMOTE_BEST_PEAK      = False      # <— NUEVO: no promociones el mejor pico\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------- MODELO VIDEO (fix de cabeza fc) -----------------\n",
        "model = r3d_18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "ckpt = torch.load(MODEL_P, map_location=device)\n",
        "state = ckpt.get(\"model\", ckpt)\n",
        "\n",
        "# Remap: fc.1.weight/bias -> fc.weight/bias (ignora fc.0.* como Dropout)\n",
        "remapped = {}\n",
        "for k, v in state.items():\n",
        "    if k.startswith(\"fc.1.\"):\n",
        "        remapped[\"fc.\" + k[5:]] = v  # \"fc.1.weight\" -> \"fc.weight\"\n",
        "    elif k.startswith(\"fc.0.\"):\n",
        "        continue  # Dropout / capa sin params\n",
        "    else:\n",
        "        remapped[k] = v\n",
        "\n",
        "missing, unexpected = model.load_state_dict(remapped, strict=False)\n",
        "print(\"[load_state_dict] missing:\", missing, \"unexpected:\", unexpected)\n",
        "\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# (opcional) ver etiquetas si existen\n",
        "if \"label_names\" in ckpt:\n",
        "    print(\"[Labels en ckpt]:\", ckpt[\"label_names\"])\n",
        "else:\n",
        "    print(\"[Aviso] ckpt sin 'label_names'. Asumo clase 1 = shoplifting.\")\n",
        "\n",
        "mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "# ----------------- UTILIDADES -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in tqdm(range(n), desc=\"Cacheando frames (clasificador)\"):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None:\n",
        "            fr = np.zeros((target_size, target_size, 3), np.uint8)\n",
        "        else:\n",
        "            fr = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    return np.stack(frames, axis=0), fps  # [T,H,W,3], fps\n",
        "\n",
        "def clip_tensor_from_cache(cache_rgb, idxs):\n",
        "    idxs = np.clip(idxs, 0, len(cache_rgb)-1)\n",
        "    frames = cache_rgb[idxs]\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = cap_tmp.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0:\n",
        "            continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            # Reencode compatible con players comunes\n",
        "            cmd = (\n",
        "                f'ffmpeg -y -fflags +genpts -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" '\n",
        "                f'-vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" '\n",
        "                f'-c:v libx264 -preset veryfast -crf 23 -profile:v high -level 4.0 '\n",
        "                f'-pix_fmt yuv420p -c:a aac -b:a 128k -ar 44100 '\n",
        "                f'-movflags +faststart -shortest \"{out_mp4}\"'\n",
        "            )\n",
        "        else:\n",
        "            cmd = f'ffmpeg -y -ss {t0:.3f} -to {t1:.3f} -i \"{video_path}\" -c copy -movflags +faststart \"{out_mp4}\"'\n",
        "        print(\"[ffmpeg]\", cmd)\n",
        "        os.system(cmd)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "def merge_alert_segments(win_times, states, probs_s):\n",
        "    events = []\n",
        "    cur_on = False\n",
        "    cur_start = None\n",
        "    cur_probs = []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_s[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_s[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start),\n",
        "                           \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)),\n",
        "                           \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start),\n",
        "                       \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)),\n",
        "                       \"p_avg\": float(np.mean(cur_probs))})\n",
        "    return events\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "def event_window_overlap_idxs(event, win_times):\n",
        "    idxs = []\n",
        "    es, ee = event[\"t_start\"], event[\"t_end\"]\n",
        "    for i, (t0, t1) in enumerate(win_times):\n",
        "        if not (t1 <= es or t0 >= ee):  # hay solape\n",
        "            idxs.append(i)\n",
        "    return idxs\n",
        "\n",
        "# ----------------- CLASIFICACIÓN POR VENTANAS -----------------\n",
        "cache_rgb, fps_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "n_cache = cache_rgb.shape[0]\n",
        "win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "win_times = []\n",
        "p0_list, p1_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in tqdm(starts, desc=\"Ventanas (clasificador)\"):\n",
        "        idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "        x = clip_tensor_from_cache(cache_rgb, idxs)\n",
        "        probs = torch.softmax(model(x)[0], dim=0).detach().cpu().numpy()\n",
        "        p0, p1 = float(probs[0]), float(probs[1])\n",
        "        t0 = start / fps_cache\n",
        "        t1 = (start + win) / fps_cache\n",
        "        win_times.append((t0, t1))\n",
        "        p0_list.append(p0); p1_list.append(p1)\n",
        "\n",
        "p0 = np.array(p0_list, dtype=np.float32)\n",
        "p1 = np.array(p1_list, dtype=np.float32)\n",
        "\n",
        "# Autodetección de índice \"shoplifting\"\n",
        "p50_0, p50_1 = float(np.median(p0)), float(np.median(p1))\n",
        "shop_idx = 0 if (p50_1 > 0.8 and p50_0 < 0.2) else (1 if (p50_0 > 0.8 and p50_1 < 0.2) else 1)\n",
        "print(f\"[Auto-clase] mediana p(class0)={p50_0:.3f}  mediana p(class1)={p50_1:.3f}  -> uso shop_idx={shop_idx}\")\n",
        "\n",
        "raw_probs = p1 if shop_idx == 1 else p0\n",
        "\n",
        "# Suavizado\n",
        "if SMOOTH_K > 1 and len(raw_probs) >= SMOOTH_K:\n",
        "    kernel = np.ones(SMOOTH_K, dtype=np.float32)/SMOOTH_K\n",
        "    probs_s = np.convolve(raw_probs, kernel, mode='same')\n",
        "else:\n",
        "    probs_s = raw_probs.copy()\n",
        "\n",
        "# -------- Calibración de umbrales (ADAPTATIVO para valores bajos) --------\n",
        "all_ps = np.array(probs_s, dtype=np.float32)\n",
        "p50 = float(np.percentile(all_ps, 50)) if len(all_ps) else 0.0\n",
        "p75 = float(np.percentile(all_ps, 75)) if len(all_ps) else 0.0\n",
        "p90 = float(np.percentile(all_ps, 90)) if len(all_ps) else 0.0\n",
        "p95 = float(np.percentile(all_ps, 95)) if len(all_ps) else 0.0\n",
        "\n",
        "neg_mask = all_ps <= (p50 if len(all_ps) else 0.5)\n",
        "neg_ps = all_ps[neg_mask] if np.any(neg_mask) else all_ps\n",
        "p90_neg = float(np.percentile(neg_ps, 90)) if len(neg_ps) else 0.0\n",
        "\n",
        "# Umbrales ADAPTATIVOS (más pegados a la cola alta del video)\n",
        "THRESH_HI = max(ABS_FLOOR_HI, p95 + 0.005, p90 + 0.003, p90_neg + 0.010)\n",
        "THRESH_LO = max(0.03, THRESH_HI - 0.010)\n",
        "if THRESH_LO >= THRESH_HI:\n",
        "    THRESH_LO = max(0.5*THRESH_HI, 0.03)\n",
        "\n",
        "print(f\"[Umbrales] p95={p95:.3f}  p90={p90:.3f}  p75={p75:.3f}  p90(neg)={p90_neg:.3f}  -> THRESH_HI={THRESH_HI:.3f}  THRESH_LO={THRESH_LO:.3f}\")\n",
        "\n",
        "# Histeresis con umbrales recalibrados\n",
        "states = []\n",
        "on = False\n",
        "for p in probs_s:\n",
        "    if not on and p >= THRESH_HI: on = True\n",
        "    elif on and p <= THRESH_LO:   on = False\n",
        "    states.append(int(on))\n",
        "\n",
        "# Guardar JSON por ventana\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump([\n",
        "        {\"t0\":float(t0),\"t1\":float(t1),\n",
        "         \"p_raw\":float(p),\"p_smooth\":float(ps),\"alarm\":int(s)}\n",
        "        for (t0,t1), p, ps, s in zip(win_times, raw_probs, probs_s, states)\n",
        "    ], f, indent=2)\n",
        "print(\"JSON por ventana:\", OUT_JSON)\n",
        "\n",
        "# ===== Post-proceso de eventos: merge + filtros ADAPTATIVOS =====\n",
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "\n",
        "P_RAW = np.array(raw_probs); P_SMO = np.array(probs_s)\n",
        "p95_raw = float(np.percentile(P_RAW, 95)) if len(P_RAW) else 0.0\n",
        "p80_smo = float(np.percentile(P_SMO, 80)) if len(P_SMO) else 0.0\n",
        "\n",
        "MIN_PEAK = max(ABS_FLOOR_PEAK, max(p95_raw, THRESH_HI) - 0.003)   # pico CRUDO\n",
        "MIN_MEAN = max(ABS_FLOOR_MEAN, THRESH_LO)                          # media suavizada\n",
        "MID_THR  = max(THRESH_LO, 0.80*THRESH_HI)                          # continuidad\n",
        "MIN_FRAC = 0.25                                                    # % del evento >= MID_THR\n",
        "MIN_DUR  = 0.80\n",
        "\n",
        "print(f\"[Filtros] MIN_PEAK={MIN_PEAK:.3f}  MIN_MEAN={MIN_MEAN:.3f}  MID_THR={MID_THR:.3f}  MIN_FRAC={MIN_FRAC:.2f}  MIN_DUR={MIN_DUR:.2f}s\")\n",
        "\n",
        "# --- RELAJACIÓN SUAVE PARA DEMO (opcional, no falsifica) ---\n",
        "if DEMO_SOFT_RELAX:\n",
        "    MIN_PEAK *= 0.85\n",
        "    MIN_MEAN *= 0.85\n",
        "    MID_THR   = THRESH_LO\n",
        "    MIN_FRAC  = max(MIN_FRAC, 0.20)\n",
        "    MIN_DUR   = max(0.50, MIN_DUR)\n",
        "    print(f\"[Leniente] Ajustes -> MIN_PEAK={MIN_PEAK:.3f}, MIN_MEAN={MIN_MEAN:.3f}, MID_THR={MID_THR:.3f}, MIN_FRAC={MIN_FRAC:.2f}, MIN_DUR={MIN_DUR:.2f}s\")\n",
        "\n",
        "def filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                  min_dur, min_peak, min_mean, mid_thr, min_frac_mid):\n",
        "    good, inspected = [], []\n",
        "    for ev in events_raw:\n",
        "        dur = ev[\"t_end\"] - ev[\"t_start\"]\n",
        "        if dur < min_dur:\n",
        "            continue\n",
        "        idxs = event_window_overlap_idxs(ev, win_times)\n",
        "        if not idxs:\n",
        "            continue\n",
        "        raw_peak = float(np.max(P_RAW[idxs]))\n",
        "        sm_mean  = float(np.mean(P_SMO[idxs]))\n",
        "        frac_mid = float(np.mean((P_SMO[idxs] >= mid_thr).astype(np.float32)))\n",
        "        inspected.append((raw_peak, sm_mean, frac_mid, ev[\"t_start\"], ev[\"t_end\"]))\n",
        "        if raw_peak < min_peak:  continue\n",
        "        if sm_mean  < min_mean:  continue\n",
        "        if frac_mid < min_frac_mid: continue\n",
        "        ev[\"p_max\"] = raw_peak\n",
        "        ev[\"p_avg\"] = sm_mean\n",
        "        good.append(ev)\n",
        "    return good, inspected\n",
        "\n",
        "events, inspected = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                                  MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "# --- Promover un evento real alrededor del mejor pico si no hubo eventos y el pico >= THRESH_LO ---\n",
        "# (Desactivado) No promover eventos: solo si pasa umbrales e histéresis.\n",
        "if PROMOTE_BEST_PEAK and not events and len(P_SMO) > 0:\n",
        "    pass\n",
        "\n",
        "with open(OUT_EVENTS, \"w\") as f:\n",
        "    json.dump(events, f, indent=2)\n",
        "print(\"Eventos filtrados guardados:\", OUT_EVENTS)\n",
        "\n",
        "if events:\n",
        "    print(\"Eventos (filtrados):\")\n",
        "    for ev in events:\n",
        "        print(f\" - {ev['t_start']:.2f}s → {ev['t_end']:.2f}s | p_max(raw)={ev['p_max']:.3f} p_avg(smooth)={ev['p_avg']:.3f}\")\n",
        "else:\n",
        "    if not QUIET_WHEN_NO_EVENTS:\n",
        "        print(\"No hay eventos tras filtro (adaptativo).\")\n",
        "        if inspected:\n",
        "            inspected.sort(key=lambda x: x[0], reverse=True)\n",
        "            top = inspected[:2]\n",
        "            print(\"Casi eventos (top-2):\")\n",
        "            for j,(rp,sm,fr,ts,te) in enumerate(top, 1):\n",
        "                print(f\" {j}. {ts:.2f}-{te:.2f}s | peak(raw)={rp:.3f} mean(smo)={sm:.3f} frac>={MID_THR:.3f}:{fr:.2f}\")\n",
        "\n",
        "# Cortar clips SOLO de eventos filtrados\n",
        "def _cut(video_in, evs, tag=\"events\"):\n",
        "    if not evs: return []\n",
        "    events_dir = os.path.join(OUT_DIR, tag)\n",
        "    return cut_events_to_clips(video_in, evs, events_dir, pad_s=PAD_S, max_dur_s=MAX_DUR,\n",
        "                               reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME)\n",
        "\n",
        "clips = _cut(VIDEO_IN, events, \"events\")\n",
        "if clips:\n",
        "    print(\"Clips (eventos) generados:\")\n",
        "    for c in clips: print(\" -\", c)\n",
        "else:\n",
        "    if not QUIET_WHEN_NO_EVENTS:\n",
        "        print(\"No se generaron clips (no pasó el filtro).\")\n",
        "\n",
        "# ---- FALLBACK top-1 pico (opcional; desactivado en modo presentación) ----\n",
        "if ENABLE_FALLBACK_PICO and len(P_SMO) > 0 and not clips:\n",
        "    top_idx = int(np.argmax(P_SMO))\n",
        "    ev_fb = [{\"t_start\": float(win_times[top_idx][0]), \"t_end\": float(win_times[top_idx][1])}]\n",
        "    events_dir_fb = os.path.join(OUT_DIR, \"events_top1\")\n",
        "    print(\"[FALLBACK] Exporto top-1 pico igualmente.\")\n",
        "    _ = cut_events_to_clips(VIDEO_IN, ev_fb, events_dir_fb, pad_s=0.4, max_dur_s=10.0,\n",
        "                            reencode=REENCODE, fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME+\"_top1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwIoIHFCVfPi",
        "outputId": "b06c84fa-e606-4c1a-8585-978f1056b0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.184)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.16)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100%|██████████| 21.5M/21.5M [00:00<00:00, 26.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1481.3ms\n",
            "video 1/1 (frame 2/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1357.6ms\n",
            "video 1/1 (frame 3/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1381.5ms\n",
            "video 1/1 (frame 4/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1090.6ms\n",
            "video 1/1 (frame 5/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 774.1ms\n",
            "video 1/1 (frame 6/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 624.3ms\n",
            "video 1/1 (frame 7/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 757.7ms\n",
            "video 1/1 (frame 8/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 587.5ms\n",
            "video 1/1 (frame 9/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 695.2ms\n",
            "video 1/1 (frame 10/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1349.5ms\n",
            "video 1/1 (frame 11/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1199.1ms\n",
            "video 1/1 (frame 12/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 1068.6ms\n",
            "video 1/1 (frame 13/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 629.1ms\n",
            "video 1/1 (frame 14/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 705.6ms\n",
            "video 1/1 (frame 15/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 606.5ms\n",
            "video 1/1 (frame 16/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 746.0ms\n",
            "video 1/1 (frame 17/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 632.9ms\n",
            "video 1/1 (frame 18/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 589.3ms\n",
            "video 1/1 (frame 19/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 893.6ms\n",
            "video 1/1 (frame 20/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 679.9ms\n",
            "video 1/1 (frame 21/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 595.5ms\n",
            "video 1/1 (frame 22/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 769.3ms\n",
            "video 1/1 (frame 23/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 711.2ms\n",
            "video 1/1 (frame 24/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1281.5ms\n",
            "video 1/1 (frame 25/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1156.8ms\n",
            "video 1/1 (frame 26/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 1497.8ms\n",
            "video 1/1 (frame 27/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 678.0ms\n",
            "video 1/1 (frame 28/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 767.8ms\n",
            "video 1/1 (frame 29/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 562.6ms\n",
            "video 1/1 (frame 30/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 655.3ms\n",
            "video 1/1 (frame 31/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 645.7ms\n",
            "video 1/1 (frame 32/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1030.8ms\n",
            "video 1/1 (frame 33/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 925.8ms\n",
            "video 1/1 (frame 34/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 833.3ms\n",
            "video 1/1 (frame 35/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 912.2ms\n",
            "video 1/1 (frame 36/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 2456.4ms\n",
            "video 1/1 (frame 37/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1139.3ms\n",
            "video 1/1 (frame 38/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 443.2ms\n",
            "video 1/1 (frame 39/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 369.0ms\n",
            "video 1/1 (frame 40/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.6ms\n",
            "video 1/1 (frame 41/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 365.4ms\n",
            "video 1/1 (frame 42/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 384.2ms\n",
            "video 1/1 (frame 43/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 339.2ms\n",
            "video 1/1 (frame 44/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 343.3ms\n",
            "video 1/1 (frame 45/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 379.1ms\n",
            "video 1/1 (frame 46/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 354.1ms\n",
            "video 1/1 (frame 47/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 375.6ms\n",
            "video 1/1 (frame 48/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 394.2ms\n",
            "video 1/1 (frame 49/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 367.0ms\n",
            "video 1/1 (frame 50/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.9ms\n",
            "video 1/1 (frame 51/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 346.9ms\n",
            "video 1/1 (frame 52/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.1ms\n",
            "video 1/1 (frame 53/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.9ms\n",
            "video 1/1 (frame 54/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 322.1ms\n",
            "video 1/1 (frame 55/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.3ms\n",
            "video 1/1 (frame 56/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.3ms\n",
            "video 1/1 (frame 57/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 581.2ms\n",
            "video 1/1 (frame 58/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 745.8ms\n",
            "video 1/1 (frame 59/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 738.2ms\n",
            "video 1/1 (frame 60/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 751.6ms\n",
            "video 1/1 (frame 61/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 915.7ms\n",
            "video 1/1 (frame 62/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 632.6ms\n",
            "video 1/1 (frame 63/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 597.4ms\n",
            "video 1/1 (frame 64/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 498.6ms\n",
            "video 1/1 (frame 65/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.7ms\n",
            "video 1/1 (frame 66/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.0ms\n",
            "video 1/1 (frame 67/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.7ms\n",
            "video 1/1 (frame 68/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 515.5ms\n",
            "video 1/1 (frame 69/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 444.8ms\n",
            "video 1/1 (frame 70/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 416.4ms\n",
            "video 1/1 (frame 71/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.4ms\n",
            "video 1/1 (frame 72/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.1ms\n",
            "video 1/1 (frame 73/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.6ms\n",
            "video 1/1 (frame 74/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 374.7ms\n",
            "video 1/1 (frame 75/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 383.3ms\n",
            "video 1/1 (frame 76/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.8ms\n",
            "video 1/1 (frame 77/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.9ms\n",
            "video 1/1 (frame 78/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 338.1ms\n",
            "video 1/1 (frame 79/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.5ms\n",
            "video 1/1 (frame 80/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 364.9ms\n",
            "video 1/1 (frame 81/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 381.7ms\n",
            "video 1/1 (frame 82/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 333.0ms\n",
            "video 1/1 (frame 83/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 512.0ms\n",
            "video 1/1 (frame 84/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 545.4ms\n",
            "video 1/1 (frame 85/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.9ms\n",
            "video 1/1 (frame 86/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 522.2ms\n",
            "video 1/1 (frame 87/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 541.1ms\n",
            "video 1/1 (frame 88/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 554.6ms\n",
            "video 1/1 (frame 89/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 372.5ms\n",
            "video 1/1 (frame 90/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.5ms\n",
            "video 1/1 (frame 91/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 424.3ms\n",
            "video 1/1 (frame 92/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 367.0ms\n",
            "video 1/1 (frame 93/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 345.2ms\n",
            "video 1/1 (frame 94/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.1ms\n",
            "video 1/1 (frame 95/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 381.3ms\n",
            "video 1/1 (frame 96/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 361.1ms\n",
            "video 1/1 (frame 97/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 347.1ms\n",
            "video 1/1 (frame 98/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 325.6ms\n",
            "video 1/1 (frame 99/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.9ms\n",
            "video 1/1 (frame 100/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.2ms\n",
            "video 1/1 (frame 101/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 381.2ms\n",
            "video 1/1 (frame 102/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 355.3ms\n",
            "video 1/1 (frame 103/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 385.6ms\n",
            "video 1/1 (frame 104/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 373.7ms\n",
            "video 1/1 (frame 105/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 385.0ms\n",
            "video 1/1 (frame 106/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 362.3ms\n",
            "video 1/1 (frame 107/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 360.2ms\n",
            "video 1/1 (frame 108/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 574.2ms\n",
            "video 1/1 (frame 109/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 558.0ms\n",
            "video 1/1 (frame 110/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 560.6ms\n",
            "video 1/1 (frame 111/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 583.1ms\n",
            "video 1/1 (frame 112/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 579.5ms\n",
            "video 1/1 (frame 113/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 367.9ms\n",
            "video 1/1 (frame 114/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 385.2ms\n",
            "video 1/1 (frame 115/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 341.4ms\n",
            "video 1/1 (frame 116/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.1ms\n",
            "video 1/1 (frame 117/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 363.8ms\n",
            "video 1/1 (frame 118/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 376.1ms\n",
            "video 1/1 (frame 119/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 353.5ms\n",
            "video 1/1 (frame 120/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 354.3ms\n",
            "video 1/1 (frame 121/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 330.6ms\n",
            "video 1/1 (frame 122/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 385.9ms\n",
            "video 1/1 (frame 123/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.8ms\n",
            "video 1/1 (frame 124/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 344.5ms\n",
            "video 1/1 (frame 125/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.8ms\n",
            "video 1/1 (frame 126/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 127/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 328.4ms\n",
            "video 1/1 (frame 128/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 327.1ms\n",
            "video 1/1 (frame 129/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 335.2ms\n",
            "video 1/1 (frame 130/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 335.9ms\n",
            "video 1/1 (frame 131/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 346.5ms\n",
            "video 1/1 (frame 132/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 394.0ms\n",
            "video 1/1 (frame 133/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 537.8ms\n",
            "video 1/1 (frame 134/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 566.7ms\n",
            "video 1/1 (frame 135/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 593.7ms\n",
            "video 1/1 (frame 136/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 546.9ms\n",
            "video 1/1 (frame 137/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 661.8ms\n",
            "video 1/1 (frame 138/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 382.8ms\n",
            "video 1/1 (frame 139/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 364.6ms\n",
            "video 1/1 (frame 140/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 361.4ms\n",
            "video 1/1 (frame 141/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 331.8ms\n",
            "video 1/1 (frame 142/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 344.5ms\n",
            "video 1/1 (frame 143/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.2ms\n",
            "video 1/1 (frame 144/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 335.7ms\n",
            "video 1/1 (frame 145/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.9ms\n",
            "video 1/1 (frame 146/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 383.6ms\n",
            "video 1/1 (frame 147/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 360.6ms\n",
            "video 1/1 (frame 148/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 387.7ms\n",
            "video 1/1 (frame 149/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.9ms\n",
            "video 1/1 (frame 150/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.4ms\n",
            "video 1/1 (frame 151/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 359.2ms\n",
            "video 1/1 (frame 152/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 368.0ms\n",
            "video 1/1 (frame 153/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 363.2ms\n",
            "video 1/1 (frame 154/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 400.5ms\n",
            "video 1/1 (frame 155/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 364.6ms\n",
            "video 1/1 (frame 156/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 378.2ms\n",
            "video 1/1 (frame 157/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 560.0ms\n",
            "video 1/1 (frame 158/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 534.4ms\n",
            "video 1/1 (frame 159/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 563.7ms\n",
            "video 1/1 (frame 160/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 633.0ms\n",
            "video 1/1 (frame 161/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 561.9ms\n",
            "video 1/1 (frame 162/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 455.6ms\n",
            "video 1/1 (frame 163/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 364.6ms\n",
            "video 1/1 (frame 164/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 380.7ms\n",
            "video 1/1 (frame 165/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 383.5ms\n",
            "video 1/1 (frame 166/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 353.7ms\n",
            "video 1/1 (frame 167/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 338.3ms\n",
            "video 1/1 (frame 168/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.6ms\n",
            "video 1/1 (frame 169/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 333.9ms\n",
            "video 1/1 (frame 170/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 326.6ms\n",
            "video 1/1 (frame 171/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.5ms\n",
            "video 1/1 (frame 172/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.3ms\n",
            "video 1/1 (frame 173/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 361.1ms\n",
            "video 1/1 (frame 174/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 374.6ms\n",
            "video 1/1 (frame 175/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 364.1ms\n",
            "video 1/1 (frame 176/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 375.1ms\n",
            "video 1/1 (frame 177/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 371.3ms\n",
            "video 1/1 (frame 178/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 361.7ms\n",
            "video 1/1 (frame 179/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 364.8ms\n",
            "video 1/1 (frame 180/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 350.1ms\n",
            "video 1/1 (frame 181/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 354.0ms\n",
            "video 1/1 (frame 182/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 554.5ms\n",
            "video 1/1 (frame 183/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 1 person, 673.7ms\n",
            "video 1/1 (frame 184/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 553.2ms\n",
            "video 1/1 (frame 185/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 555.5ms\n",
            "video 1/1 (frame 186/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 577.0ms\n",
            "video 1/1 (frame 187/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 431.1ms\n",
            "video 1/1 (frame 188/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 389.8ms\n",
            "video 1/1 (frame 189/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 395.6ms\n",
            "video 1/1 (frame 190/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.9ms\n",
            "video 1/1 (frame 191/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 356.3ms\n",
            "video 1/1 (frame 192/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 320.9ms\n",
            "video 1/1 (frame 193/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 365.0ms\n",
            "video 1/1 (frame 194/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 362.1ms\n",
            "video 1/1 (frame 195/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 351.9ms\n",
            "video 1/1 (frame 196/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.2ms\n",
            "video 1/1 (frame 197/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 340.2ms\n",
            "video 1/1 (frame 198/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 325.5ms\n",
            "video 1/1 (frame 199/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 371.1ms\n",
            "video 1/1 (frame 200/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 360.0ms\n",
            "video 1/1 (frame 201/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 378.9ms\n",
            "video 1/1 (frame 202/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 360.8ms\n",
            "video 1/1 (frame 203/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 378.1ms\n",
            "video 1/1 (frame 204/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 1361.4ms\n",
            "video 1/1 (frame 205/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 2825.9ms\n",
            "video 1/1 (frame 206/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 1253.4ms\n",
            "video 1/1 (frame 207/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 376.7ms\n",
            "video 1/1 (frame 208/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 416.4ms\n",
            "video 1/1 (frame 209/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 351.0ms\n",
            "video 1/1 (frame 210/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 372.7ms\n",
            "video 1/1 (frame 211/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 356.3ms\n",
            "video 1/1 (frame 212/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 371.9ms\n",
            "video 1/1 (frame 213/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 349.7ms\n",
            "video 1/1 (frame 214/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 370.2ms\n",
            "video 1/1 (frame 215/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 333.4ms\n",
            "video 1/1 (frame 216/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.3ms\n",
            "video 1/1 (frame 217/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 334.9ms\n",
            "video 1/1 (frame 218/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 349.4ms\n",
            "video 1/1 (frame 219/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 391.5ms\n",
            "video 1/1 (frame 220/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 376.3ms\n",
            "video 1/1 (frame 221/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 361.6ms\n",
            "video 1/1 (frame 222/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 370.4ms\n",
            "video 1/1 (frame 223/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 456.5ms\n",
            "video 1/1 (frame 224/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 553.1ms\n",
            "video 1/1 (frame 225/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 565.0ms\n",
            "video 1/1 (frame 226/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 565.5ms\n",
            "video 1/1 (frame 227/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 590.8ms\n",
            "video 1/1 (frame 228/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 527.0ms\n",
            "video 1/1 (frame 229/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 369.4ms\n",
            "video 1/1 (frame 230/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 320.2ms\n",
            "video 1/1 (frame 231/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 329.4ms\n",
            "video 1/1 (frame 232/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 342.1ms\n",
            "video 1/1 (frame 233/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.5ms\n",
            "video 1/1 (frame 234/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.0ms\n",
            "video 1/1 (frame 235/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 330.6ms\n",
            "video 1/1 (frame 236/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.4ms\n",
            "video 1/1 (frame 237/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 327.5ms\n",
            "video 1/1 (frame 238/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 323.1ms\n",
            "video 1/1 (frame 239/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 328.2ms\n",
            "video 1/1 (frame 240/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 318.4ms\n",
            "video 1/1 (frame 241/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 337.0ms\n",
            "video 1/1 (frame 242/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 324.4ms\n",
            "video 1/1 (frame 243/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 344.9ms\n",
            "video 1/1 (frame 244/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 325.2ms\n",
            "video 1/1 (frame 245/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 2 persons, 334.9ms\n",
            "video 1/1 (frame 246/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 328.7ms\n",
            "video 1/1 (frame 247/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 336.0ms\n",
            "video 1/1 (frame 248/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 3 persons, 352.1ms\n",
            "video 1/1 (frame 249/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 377.7ms\n",
            "video 1/1 (frame 250/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 547.0ms\n",
            "video 1/1 (frame 251/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 559.4ms\n",
            "video 1/1 (frame 252/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 631.6ms\n",
            "video 1/1 (frame 253/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 563.6ms\n",
            "video 1/1 (frame 254/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 578.7ms\n",
            "video 1/1 (frame 255/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 360.5ms\n",
            "video 1/1 (frame 256/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 362.6ms\n",
            "video 1/1 (frame 257/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 355.8ms\n",
            "video 1/1 (frame 258/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 360.2ms\n",
            "video 1/1 (frame 259/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 359.8ms\n",
            "video 1/1 (frame 260/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 336.5ms\n",
            "video 1/1 (frame 261/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 332.5ms\n",
            "video 1/1 (frame 262/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 363.6ms\n",
            "video 1/1 (frame 263/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 350.8ms\n",
            "video 1/1 (frame 264/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 352.2ms\n",
            "video 1/1 (frame 265/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 340.0ms\n",
            "video 1/1 (frame 266/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 353.6ms\n",
            "video 1/1 (frame 267/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 363.7ms\n",
            "video 1/1 (frame 268/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 337.0ms\n",
            "video 1/1 (frame 269/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 325.9ms\n",
            "video 1/1 (frame 270/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 367.2ms\n",
            "video 1/1 (frame 271/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 365.6ms\n",
            "video 1/1 (frame 272/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 331.2ms\n",
            "video 1/1 (frame 273/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 339.9ms\n",
            "video 1/1 (frame 274/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 522.5ms\n",
            "video 1/1 (frame 275/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 650.8ms\n",
            "video 1/1 (frame 276/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 561.7ms\n",
            "video 1/1 (frame 277/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 566.9ms\n",
            "video 1/1 (frame 278/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 567.2ms\n",
            "video 1/1 (frame 279/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 432.2ms\n",
            "video 1/1 (frame 280/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 369.6ms\n",
            "video 1/1 (frame 281/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 336.6ms\n",
            "video 1/1 (frame 282/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.8ms\n",
            "video 1/1 (frame 283/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 367.3ms\n",
            "video 1/1 (frame 284/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 368.1ms\n",
            "video 1/1 (frame 285/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 401.9ms\n",
            "video 1/1 (frame 286/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 362.3ms\n",
            "video 1/1 (frame 287/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 378.4ms\n",
            "video 1/1 (frame 288/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 358.7ms\n",
            "video 1/1 (frame 289/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 387.9ms\n",
            "video 1/1 (frame 290/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 360.4ms\n",
            "video 1/1 (frame 291/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 362.3ms\n",
            "video 1/1 (frame 292/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 326.7ms\n",
            "video 1/1 (frame 293/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 372.0ms\n",
            "video 1/1 (frame 294/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 352.6ms\n",
            "video 1/1 (frame 295/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 350.2ms\n",
            "video 1/1 (frame 296/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 330.2ms\n",
            "video 1/1 (frame 297/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 370.7ms\n",
            "video 1/1 (frame 298/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 7 persons, 398.5ms\n",
            "video 1/1 (frame 299/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 558.6ms\n",
            "video 1/1 (frame 300/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 533.9ms\n",
            "video 1/1 (frame 301/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 528.2ms\n",
            "video 1/1 (frame 302/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 516.3ms\n",
            "video 1/1 (frame 303/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 581.6ms\n",
            "video 1/1 (frame 304/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 457.7ms\n",
            "video 1/1 (frame 305/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 413.7ms\n",
            "video 1/1 (frame 306/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 361.4ms\n",
            "video 1/1 (frame 307/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 377.9ms\n",
            "video 1/1 (frame 308/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 4 persons, 346.9ms\n",
            "video 1/1 (frame 309/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 347.3ms\n",
            "video 1/1 (frame 310/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 326.4ms\n",
            "video 1/1 (frame 311/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 6 persons, 337.4ms\n",
            "video 1/1 (frame 312/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 332.5ms\n",
            "video 1/1 (frame 313/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 377.9ms\n",
            "video 1/1 (frame 314/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 357.0ms\n",
            "video 1/1 (frame 315/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 341.7ms\n",
            "video 1/1 (frame 316/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.2ms\n",
            "video 1/1 (frame 317/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 324.4ms\n",
            "video 1/1 (frame 318/318) /content/tesisV2/videos/supermas1-1.mp4: 384x640 5 persons, 328.1ms\n",
            "Speed: 5.5ms preprocess, 477.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/tesisV2/outputs/boxed3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# cargar modelo\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "# hacer tracking con BoT-SORT\n",
        "results = model.track(\n",
        "    source=\"/content/tesisV2/videos/supermas1-1.mp4\",\n",
        "    classes=0,           # personas\n",
        "    conf=0.25,           # confianza mínima\n",
        "    tracker=\"botsort.yaml\",\n",
        "    save=True,\n",
        "    project=\"/content/tesisV2/outputs\",\n",
        "    name=\"boxed\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvAdE1WkSCn"
      },
      "source": [
        "## ***TERCER ENTRENAMIENTO***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQa7oflUx-Fm",
        "outputId": "9863dfb9-6e3e-46b0-b867-992690a2b7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!apt-get -y update >/dev/null\n",
        "!apt-get -y install ffmpeg >/dev/null\n",
        "!pip -q install \"av>=10,<14\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11G5TSimsXia",
        "outputId": "5c2b92da-a553-4d51-fd98-ca3dfea8008d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponible: False\n",
            "Train clips: 10668 | Val clips: 787\n",
            "Conteo train (items CSV) → normal=2082 | hurto=8586\n",
            "Class weights (aprox) -> normal=2.562 | hurto=0.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1129450782.py:297: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Reanudado desde /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt (epoch=6)\n",
            "💾 Guardado intermedio (step=300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=600) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=900) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1200) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1500) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[07/15] lr=1.57e-04 | train 0.0036/0.996 | val 0.7440/0.416\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "✔️ Mejor Loss, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestLoss.pt\n",
            "💾 Guardado intermedio (step=1800) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2100) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2400) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2700) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3000) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[08/15] lr=1.35e-04 | train 0.0010/0.999 | val 1.6882/0.348\n",
            "\n",
            "=== Classification Report (umbral 0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.18      0.99      0.31       118\n",
            "       hurto       0.99      0.21      0.35       669\n",
            "\n",
            "    accuracy                           0.33       787\n",
            "   macro avg       0.59      0.60      0.33       787\n",
            "weighted avg       0.87      0.33      0.34       787\n",
            "\n",
            "Matriz de confusión (val):\n",
            " [[117   1]\n",
            " [528 141]]\n",
            "Tabla de thresholds guardada en: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_threshold_sweep.csv\n",
            "Log de entrenamiento: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_trainlog.json\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Fine-tune R3D-18 layer3+layer4 (CPU/GPU con reanudación y micro-sesiones)\n",
        "# =========================\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "SEED        = 1337\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS      = 15\n",
        "BATCH_SIZE  = 6\n",
        "LR          = 2e-4\n",
        "WEIGHT_DEC  = 2e-4\n",
        "GAMMA       = 2.5  # Focal Loss gamma\n",
        "\n",
        "OUT_DIR     = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # mejor de layer4\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestLoss.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"r3d18_ft_l34_trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"r3d18_ft_l34_threshold_sweep.csv\")\n",
        "\n",
        "# ---------- CHECKPOINTS / CONTROL DE SESIÓN ----------\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"r3d18_ft_l34_last.pt\")      # último estado (para reanudar)\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"r3d18_ft_l34_runstate.pt\")  # alias (copia del last)\n",
        "\n",
        "# Ejecutá en “micro-sesiones”\n",
        "RUN_EPOCHS_THIS_SESSION = 2      # cuántas épocas avanza ESTA corrida\n",
        "SAVE_EVERY_N_BATCHES    = 300    # checkpoint intermedio cada N batches (None para desactivar)\n",
        "\n",
        "# Para acelerar/probar en CPU: limitá batches por época (None = sin límite)\n",
        "MAX_TRAIN_BATCHES = None   # ej: 400\n",
        "MAX_VAL_BATCHES   = None   # ej: 100\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "# ---- GPU perf tweaks ----\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"medium\")\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---------- DATA LOADERS ----------\n",
        "BASE_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV   = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "CLIP_LEN = 16   # frames por clip\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "train_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.RandomResizedCrop(CROP, scale=(0.7, 1.0)),   # antes (0.8,1.0)\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomApply([T.ColorJitter(0.3,0.3,0.3,0.08)], p=0.5),  # un poco más fuerte\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), inplace=False),\n",
        "])\n",
        "\n",
        "val_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.CenterCrop(CROP),\n",
        "])\n",
        "\n",
        "def load_list(csv_path):\n",
        "    \"\"\"Lee CSV con filas: path,label\n",
        "       label puede ser 0/1 o 'normal'/'shoplifting'/'hurto' (case-insensitive).\"\"\"\n",
        "    def parse_label(s):\n",
        "        s = str(s).strip().lower()\n",
        "        if s.isdigit(): return int(s)\n",
        "        if s in (\"normal\", \"0\"): return 0\n",
        "        if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "        raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row:\n",
        "                continue\n",
        "            head = row[0].strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # saltar cabecera\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = parse_label(row[1])\n",
        "\n",
        "            # resolver ruta relativa\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand):\n",
        "                    path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "class SimpleVideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items   = items\n",
        "        self.train   = train\n",
        "        self.spatial = train_spatial if train else val_spatial\n",
        "        # normalización Kinetics para r3d_18\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        # video -> [T, C, H, W] uint8\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
        "        T_total = video.shape[0]\n",
        "\n",
        "        # muestreo simple de un clip de CLIP_LEN\n",
        "        if T_total >= CLIP_LEN:\n",
        "            start = random.randint(0, T_total - CLIP_LEN) if self.train else max(0, (T_total - CLIP_LEN)//2)\n",
        "            clip = video[start:start+CLIP_LEN]  # [T,C,H,W]\n",
        "        else:\n",
        "            # pad repitiendo el último frame\n",
        "            pad = CLIP_LEN - T_total\n",
        "            clip = torch.cat([video, video[-1:].repeat(pad,1,1,1)], dim=0)\n",
        "\n",
        "        # aplicar transform espacial por frame (cada frame es [C,H,W])\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "\n",
        "        # r3d_18 espera [C,T,H,W]\n",
        "        clip = clip.permute(1,0,2,3).contiguous()\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# construir datasets/loaders\n",
        "train_items = load_list(TRAIN_CSV)\n",
        "val_items   = load_list(VAL_CSV)\n",
        "\n",
        "train_ds = SimpleVideoDataset(train_items, train=True)\n",
        "val_ds   = SimpleVideoDataset(val_items,   train=False)\n",
        "\n",
        "# (Opcional) sampler balanceado:\n",
        "USE_SAMPLER = False\n",
        "\n",
        "N_normal = sum(1 for _,y in train_items if y==0)\n",
        "N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "print(f\"Train clips: {len(train_ds)} | Val clips: {len(val_ds)}\")\n",
        "print(f\"Conteo train (items CSV) → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "# DataLoader: pin_memory solo si hay CUDA\n",
        "NUM_WORKERS = 0  # en CPU puede ayudar; si ves sobrecarga, bajalo\n",
        "pinmem = torch.cuda.is_available()\n",
        "\n",
        "if USE_SAMPLER:\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "    weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "    sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "else:\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=pinmem,\n",
        "    persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# pesos por clase\n",
        "total = N_normal + N_hurto\n",
        "w_normal = total / (2 * max(1, N_normal))\n",
        "w_hurto  = total / (2 * max(1, N_hurto))\n",
        "print(f\"Class weights (aprox) -> normal={w_normal:.3f} | hurto={w_hurto:.3f}\")\n",
        "CLASS_WEIGHTS_CPU = torch.tensor([w_normal, w_hurto], dtype=torch.float32)\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "# compatibilidad de versiones de torchvision\n",
        "try:\n",
        "    model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "except TypeError:\n",
        "    model = r3d_18(pretrained=True)\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Congelar TODO menos layer3, layer4 y fc\n",
        "for name, p in model.named_parameters():\n",
        "    p.requires_grad = any(blk in name for blk in [\"layer3\",\"layer4\",\"fc\"])\n",
        "\n",
        "# ---------- LOSS (Focal) ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)         # [B]\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce     # [B]\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(gamma=GAMMA, weight=CLASS_WEIGHTS_CPU.to(DEVICE))\n",
        "\n",
        "# ---------- OPTIM / SCHED ----------\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                              lr=LR, weight_decay=WEIGHT_DEC)\n",
        "\n",
        "# AMP GradScaler para GPU\n",
        "from torch.amp import GradScaler\n",
        "scaler = GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup:\n",
        "        return base_lr * (t+1)/warmup\n",
        "    tw = max(1, T - warmup)\n",
        "    tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "# ---------- REANUDACIÓN COMPLETA ----------\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if torch.cuda.is_available() else None,\n",
        "        \"class_weights\": CLASS_WEIGHTS_CPU,\n",
        "        \"rng\": _rng_state(),\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume():\n",
        "    start_epoch = 0\n",
        "    best_f1, best_loss = -1.0, float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd[\"model\"], strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        if torch.cuda.is_available() and sd.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(sd[\"scaler\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        print(\"⏩ Sin estado previo completo. Usando CKPT_IN (solo pesos) si existe.\")\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            sd = torch.load(CKPT_IN, map_location=\"cpu\")\n",
        "            state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "            missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Cargado ckpt previo:\", CKPT_IN)\n",
        "            print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# ---------- LOOP ----------\n",
        "def run_epoch(loader, train=True, current_epoch=0, save_every=None,\n",
        "              max_batches=None, global_step_start=0):\n",
        "    if train: model.train()\n",
        "    else:     model.eval()\n",
        "\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches):\n",
        "            break\n",
        "\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        from torch.amp import autocast\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            # Checkpoint intermedio\n",
        "            if save_every and (global_step % save_every == 0):\n",
        "                save_ckpt(CKPT_LAST, epoch=current_epoch, best_f1=best_f1,\n",
        "                          best_loss=best_loss, row={\"epoch\": current_epoch},\n",
        "                          extra={\"global_step\": global_step})\n",
        "                print(f\"💾 Guardado intermedio (step={global_step}) -> {CKPT_LAST}\")\n",
        "\n",
        "        probs_hurto = logits.softmax(1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += loss.item() * y.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    # F1 al umbral 0.5 como referencia\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# Inicializar/reanudar\n",
        "start_epoch, best_f1, best_loss = try_resume()\n",
        "history = []\n",
        "global_step = 0\n",
        "\n",
        "first_epoch = start_epoch + 1\n",
        "last_epoch  = min(EPOCHS, start_epoch + RUN_EPOCHS_THIS_SESSION)\n",
        "\n",
        "for epoch in range(first_epoch, last_epoch + 1):\n",
        "    # scheduler manual con warmup+cosine\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = cosine_warmup_lr(epoch-1, EPOCHS, LR, warmup=2)\n",
        "\n",
        "    tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "        train_loader, train=True, current_epoch=epoch,\n",
        "        save_every=SAVE_EVERY_N_BATCHES,\n",
        "        max_batches=MAX_TRAIN_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "    va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "        val_loader, train=False, current_epoch=epoch,\n",
        "        save_every=None, max_batches=MAX_VAL_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "\n",
        "    row = {\"epoch\": epoch, \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "           \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "           \"val_loss\": va_loss,   \"val_f1\": va_f1}\n",
        "    history.append(row)\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] lr={row['lr']:.2e} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "    # Guardado \"last\" SIEMPRE al final de cada época (para reanudar exacto)\n",
        "    save_ckpt(CKPT_LAST, epoch=epoch, best_f1=best_f1, best_loss=best_loss, row=row)\n",
        "    try:\n",
        "        shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Guardados “best”\n",
        "    if va_f1 > best_f1:\n",
        "        best_f1 = va_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "        print(\"✔️ Mejor F1, guardado ->\", CKPT_BESTF1)\n",
        "    if va_loss < best_loss:\n",
        "        best_loss = va_loss\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "        print(\"✔️ Mejor Loss, guardado ->\", CKPT_BESTL)\n",
        "\n",
        "# ---------- LOG / REPORT ----------\n",
        "with open(LOG_JSON, \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Reporte en el último estado: clasificación y matriz (umbral 0.5)\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "    print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "    print(classification_report(ys, y_pred, target_names=[\"normal\",\"hurto\"]))\n",
        "    print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, y_pred))\n",
        "else:\n",
        "    print(\"\\n(No hubo validación con muestras en esta sesión; salteo reporte inmediato)\")\n",
        "\n",
        "# Sweep de umbral para elegir threshold\n",
        "import csv as _csv\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    ths = np.round(np.linspace(0.30, 0.80, 21), 3)\n",
        "    with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "        w = _csv.writer(f); w.writerow([\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
        "        for t in ths:\n",
        "            yp = (probs >= t).astype(int)\n",
        "            rep = classification_report(ys, yp, output_dict=True, zero_division=0)\n",
        "            p = rep[\"1\"][\"precision\"]; r = rep[\"1\"][\"recall\"]; f1 = rep[\"1\"][\"f1-score\"]\n",
        "            w.writerow([t, p, r, f1])\n",
        "    print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "else:\n",
        "    print(\"Sin probs/ys de validación en esta sesión; no genero THR_CSV aún.\")\n",
        "\n",
        "print(\"Log de entrenamiento:\", LOG_JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Fine-tune R3D-18 layer3+layer4 (CPU/GPU con reanudación y micro-sesiones)\n",
        "# =========================\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "SEED        = 1337\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS      = 15\n",
        "BATCH_SIZE  = 6\n",
        "LR          = 2e-4\n",
        "WEIGHT_DEC  = 5e-4   # <--- subimos regularización L2 (antes 2e-4)\n",
        "GAMMA       = 2.0    # <--- focal un poco menos agresiva (antes 2.5)\n",
        "\n",
        "OUT_DIR     = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_finetune.pt\"  # mejor de layer4\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"r3d18_shoplifting_ft_l34_bestLoss.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"r3d18_ft_l34_trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"r3d18_ft_l34_threshold_sweep.csv\")\n",
        "\n",
        "# ---------- CHECKPOINTS / CONTROL DE SESIÓN ----------\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"r3d18_ft_l34_last.pt\")      # último estado (para reanudar)\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"r3d18_ft_l34_runstate.pt\")  # alias (copia del last)\n",
        "\n",
        "# Ejecutá en “micro-sesiones”\n",
        "RUN_EPOCHS_THIS_SESSION = 2      # cuántas épocas avanza ESTA corrida\n",
        "SAVE_EVERY_N_BATCHES    = 300    # checkpoint intermedio cada N batches (None para desactivar)\n",
        "\n",
        "# Para acelerar/probar en CPU: limitá batches por época (None = sin límite)\n",
        "MAX_TRAIN_BATCHES = None   # ej: 400\n",
        "MAX_VAL_BATCHES   = None   # ej: 100\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "set_seed()\n",
        "\n",
        "# ---- GPU perf tweaks ----\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"medium\")\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---------- DATA LOADERS ----------\n",
        "BASE_DIR  = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV   = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "CLIP_LEN = 16   # frames por clip\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "train_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.RandomResizedCrop(CROP, scale=(0.7, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomApply([T.ColorJitter(0.3,0.3,0.3,0.08)], p=0.5),\n",
        "    # T.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3), inplace=False),  # <--- desactivado para CPU\n",
        "])\n",
        "\n",
        "val_spatial = T.Compose([\n",
        "    T.Resize((RESIZE, RESIZE)),\n",
        "    T.CenterCrop(CROP),\n",
        "])\n",
        "\n",
        "def load_list(csv_path):\n",
        "    \"\"\"Lee CSV con filas: path,label\n",
        "       label puede ser 0/1 o 'normal'/'shoplifting'/'hurto' (case-insensitive).\"\"\"\n",
        "    def parse_label(s):\n",
        "        s = str(s).strip().lower()\n",
        "        if s.isdigit(): return int(s)\n",
        "        if s in (\"normal\", \"0\"): return 0\n",
        "        if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "        raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row:\n",
        "                continue\n",
        "            head = row[0].strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # saltar cabecera\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = parse_label(row[1])\n",
        "\n",
        "            # resolver ruta relativa\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand):\n",
        "                    path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "class SimpleVideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True):\n",
        "        self.items   = items\n",
        "        self.train   = train\n",
        "        self.spatial = train_spatial if train else val_spatial\n",
        "        # normalización Kinetics para r3d_18\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        # video -> [T, C, H, W] uint8\n",
        "        video, _, _ = read_video(path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
        "        T_total = video.shape[0]\n",
        "\n",
        "        # muestreo simple de un clip de CLIP_LEN\n",
        "        if T_total >= CLIP_LEN:\n",
        "            start = random.randint(0, T_total - CLIP_LEN) if self.train else max(0, (T_total - CLIP_LEN)//2)\n",
        "            clip = video[start:start+CLIP_LEN]  # [T,C,H,W]\n",
        "        else:\n",
        "            # pad repitiendo el último frame\n",
        "            pad = CLIP_LEN - T_total\n",
        "            clip = torch.cat([video, video[-1:].repeat(pad,1,1,1)], dim=0)\n",
        "\n",
        "        # aplicar transform espacial por frame (cada frame es [C,H,W])\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "\n",
        "        # r3d_18 espera [C,T,H,W]\n",
        "        clip = clip.permute(1,0,2,3).contiguous()\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# construir datasets/loaders\n",
        "train_items = load_list(TRAIN_CSV)\n",
        "val_items   = load_list(VAL_CSV)\n",
        "\n",
        "train_ds = SimpleVideoDataset(train_items, train=True)\n",
        "val_ds   = SimpleVideoDataset(val_items,   train=False)\n",
        "\n",
        "# (Opcional) sampler balanceado:\n",
        "USE_SAMPLER = False\n",
        "\n",
        "N_normal = sum(1 for _,y in train_items if y==0)\n",
        "N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "print(f\"Train clips: {len(train_ds)} | Val clips: {len(val_ds)}\")\n",
        "print(f\"Conteo train (items CSV) → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "# DataLoader: en CPU evitamos workers y pin_memory\n",
        "NUM_WORKERS = 0\n",
        "pinmem = False  # torch.cuda.is_available() y útil solo en GPU\n",
        "\n",
        "if USE_SAMPLER:\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "    weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "    sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=sampler,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "else:\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=pinmem,\n",
        "        drop_last=True,\n",
        "        persistent_workers=(NUM_WORKERS>0)\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=pinmem,\n",
        "    persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# pesos por clase (AJUSTADOS para subir recall de hurto)\n",
        "CLASS_WEIGHTS_CPU = torch.tensor([1.0, 1.3], dtype=torch.float32)\n",
        "print(f\"Class weights fijados -> normal={CLASS_WEIGHTS_CPU[0].item():.3f} | hurto={CLASS_WEIGHTS_CPU[1].item():.3f}\")\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "# compatibilidad de versiones de torchvision\n",
        "try:\n",
        "    model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "except TypeError:\n",
        "    model = r3d_18(pretrained=True)\n",
        "\n",
        "# FC con Dropout para bajar overfit\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_feats, NUM_CLASSES)\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Congelar TODO menos layer3, layer4 y fc\n",
        "for name, p in model.named_parameters():\n",
        "    p.requires_grad = any(blk in name for blk in [\"layer3\",\"layer4\",\"fc\"])\n",
        "\n",
        "# ---------- LOSS (Focal) ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)         # [B]\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce     # [B]\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "criterion = FocalLoss(gamma=GAMMA, weight=CLASS_WEIGHTS_CPU.to(DEVICE))\n",
        "\n",
        "# ---------- OPTIM / SCHED ----------\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                              lr=LR, weight_decay=WEIGHT_DEC)\n",
        "\n",
        "# AMP GradScaler para GPU\n",
        "from torch.amp import GradScaler\n",
        "scaler = GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup:\n",
        "        return base_lr * (t+1)/warmup\n",
        "    tw = max(1, T - warmup)\n",
        "    tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "# ---------- REANUDACIÓN COMPLETA ----------\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": scaler.state_dict() if torch.cuda.is_available() else None,\n",
        "        \"class_weights\": CLASS_WEIGHTS_CPU,\n",
        "        \"rng\": _rng_state(),\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume():\n",
        "    start_epoch = 0\n",
        "    best_f1, best_loss = -1.0, float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd[\"model\"], strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        if torch.cuda.is_available() and sd.get(\"scaler\") is not None:\n",
        "            scaler.load_state_dict(sd[\"scaler\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        print(\"⏩ Sin estado previo completo. Usando CKPT_IN (solo pesos) si existe.\")\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            sd = torch.load(CKPT_IN, map_location=\"cpu\")\n",
        "            state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "            missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "            print(\"Cargado ckpt previo:\", CKPT_IN)\n",
        "            print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# ---------- LOOP ----------\n",
        "def run_epoch(loader, train=True, current_epoch=0, save_every=None,\n",
        "              max_batches=None, global_step_start=0):\n",
        "    if train: model.train()\n",
        "    else:     model.eval()\n",
        "\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches):\n",
        "            break\n",
        "\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        from torch.amp import autocast\n",
        "        with autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            global_step += 1\n",
        "            # Checkpoint intermedio\n",
        "            if save_every and (global_step % save_every == 0):\n",
        "                save_ckpt(CKPT_LAST, epoch=current_epoch, best_f1=best_f1,\n",
        "                          best_loss=best_loss, row={\"epoch\": current_epoch},\n",
        "                          extra={\"global_step\": global_step})\n",
        "                print(f\"💾 Guardado intermedio (step={global_step}) -> {CKPT_LAST}\")\n",
        "\n",
        "        probs_hurto = logits.softmax(1)[:,1].detach().cpu().numpy()\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += loss.item() * y.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    # F1 al umbral 0.5 como referencia\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# Inicializar/reanudar\n",
        "start_epoch, best_f1, best_loss = try_resume()\n",
        "history = []\n",
        "global_step = 0\n",
        "\n",
        "first_epoch = start_epoch + 1\n",
        "last_epoch  = min(EPOCHS, start_epoch + RUN_EPOCHS_THIS_SESSION)\n",
        "\n",
        "for epoch in range(first_epoch, last_epoch + 1):\n",
        "    # scheduler manual con warmup+cosine\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = cosine_warmup_lr(epoch-1, EPOCHS, LR, warmup=2)\n",
        "\n",
        "    tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "        train_loader, train=True, current_epoch=epoch,\n",
        "        save_every=SAVE_EVERY_N_BATCHES,\n",
        "        max_batches=MAX_TRAIN_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "    va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "        val_loader, train=False, current_epoch=epoch,\n",
        "        save_every=None, max_batches=MAX_VAL_BATCHES,\n",
        "        global_step_start=global_step\n",
        "    )\n",
        "\n",
        "    row = {\"epoch\": epoch, \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "           \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "           \"val_loss\": va_loss,   \"val_f1\": va_f1}\n",
        "    history.append(row)\n",
        "    print(f\"[{epoch:02d}/{EPOCHS}] lr={row['lr']:.2e} | \"\n",
        "          f\"train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "    # Guardado \"last\" SIEMPRE al final de cada época (para reanudar exacto)\n",
        "    save_ckpt(CKPT_LAST, epoch=epoch, best_f1=best_f1, best_loss=best_loss, row=row)\n",
        "    try:\n",
        "        shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Guardados “best”\n",
        "    if va_f1 > best_f1:\n",
        "        best_f1 = va_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "        print(\"✔️ Mejor F1, guardado ->\", CKPT_BESTF1)\n",
        "    if va_loss < best_loss:\n",
        "        best_loss = va_loss\n",
        "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "        print(\"✔️ Mejor Loss, guardado ->\", CKPT_BESTL)\n",
        "\n",
        "# ---------- LOG / REPORT ----------\n",
        "with open(LOG_JSON, \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Reporte en el último estado: clasificación y matriz (umbral 0.5)\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "    print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "    print(classification_report(ys, y_pred, target_names=[\"normal\",\"hurto\"]))\n",
        "    print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, y_pred))\n",
        "else:\n",
        "    print(\"\\n(No hubo validación con muestras en esta sesión; salteo reporte inmediato)\")\n",
        "\n",
        "# Sweep de umbral para elegir threshold\n",
        "import csv as _csv, json as _json\n",
        "if 'probs' in locals() and probs.size > 0:\n",
        "    ths = np.round(np.linspace(0.30, 0.80, 21), 3)\n",
        "    with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "        w = _csv.writer(f); w.writerow([\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
        "        for t in ths:\n",
        "            yp = (probs >= t).astype(int)\n",
        "            rep = classification_report(ys, yp, output_dict=True, zero_division=0)\n",
        "            p = rep[\"1\"][\"precision\"]; r = rep[\"1\"][\"recall\"]; f1 = rep[\"1\"][\"f1-score\"]\n",
        "            w.writerow([t, p, r, f1])\n",
        "    print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "\n",
        "    # === Elegir mejor threshold por F1, mostrar top-5 y guardarlo como operativo ===\n",
        "    tops = []\n",
        "    with open(THR_CSV, newline=\"\") as f:\n",
        "        r = _csv.DictReader(f)\n",
        "        for row in r:\n",
        "            tops.append((\n",
        "                float(row[\"f1\"]),\n",
        "                float(row[\"threshold\"]),\n",
        "                float(row[\"precision\"]),\n",
        "                float(row[\"recall\"])\n",
        "            ))\n",
        "    tops.sort(reverse=True)\n",
        "\n",
        "    if tops:\n",
        "        print(\"\\nTop-5 thresholds por F1:\")\n",
        "        for f1, t, p, r in tops[:5]:\n",
        "            print(f\" t={t:.3f} | F1={f1:.3f} | P={p:.3f} | R={r:.3f}\")\n",
        "\n",
        "        best_t = tops[0][1]\n",
        "        y_pred_best = (probs >= best_t).astype(int)\n",
        "        print(f\"\\n=== Report con threshold óptimo (t={best_t:.3f}) ===\")\n",
        "        print(classification_report(ys, y_pred_best, target_names=[\"normal\",\"hurto\"]))\n",
        "        print(\"Matriz de confusión (val, t*):\\n\", confusion_matrix(ys, y_pred_best))\n",
        "\n",
        "        OPER_JSON = os.path.join(OUT_DIR, \"operating_threshold.json\")\n",
        "        with open(OPER_JSON, \"w\") as f:\n",
        "            _json.dump({\"threshold\": float(best_t)}, f, indent=2)\n",
        "        print(\"Operating threshold ->\", OPER_JSON)\n",
        "else:\n",
        "    print(\"Sin probs/ys de validación en esta sesión; no genero THR_CSV aún.\")\n",
        "\n",
        "print(\"Log de entrenamiento:\", LOG_JSON)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkc5zKXNOEC5",
        "outputId": "1a61f0aa-1546-4938-e110-18708e97ad9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponible: False\n",
            "Train clips: 10668 | Val clips: 787\n",
            "Conteo train (items CSV) → normal=2082 | hurto=8586\n",
            "Class weights fijados -> normal=1.000 | hurto=1.300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3571270764.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Reanudado desde /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt (epoch=9)\n",
            "💾 Guardado intermedio (step=300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=600) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=900) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1200) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=1500) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[10/15] lr=8.79e-05 | train 0.0009/1.000 | val 1.1922/0.466\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "💾 Guardado intermedio (step=1800) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2100) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2400) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=2700) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3000) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "💾 Guardado intermedio (step=3300) -> /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_last.pt\n",
            "[11/15] lr=6.45e-05 | train 0.0002/1.000 | val 1.4398/0.531\n",
            "✔️ Mejor F1, guardado -> /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "\n",
            "=== Classification Report (umbral 0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.21      0.97      0.35       118\n",
            "       hurto       0.98      0.36      0.53       669\n",
            "\n",
            "    accuracy                           0.45       787\n",
            "   macro avg       0.60      0.66      0.44       787\n",
            "weighted avg       0.87      0.45      0.50       787\n",
            "\n",
            "Matriz de confusión (val):\n",
            " [[114   4]\n",
            " [426 243]]\n",
            "Tabla de thresholds guardada en: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_threshold_sweep.csv\n",
            "\n",
            "Top-5 thresholds por F1:\n",
            " t=0.300 | F1=0.689 | P=0.936 | R=0.546\n",
            " t=0.325 | F1=0.687 | P=0.950 | R=0.538\n",
            " t=0.350 | F1=0.678 | P=0.959 | R=0.525\n",
            " t=0.375 | F1=0.650 | P=0.965 | R=0.490\n",
            " t=0.400 | F1=0.624 | P=0.975 | R=0.459\n",
            "\n",
            "=== Report con threshold óptimo (t=0.300) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.23      0.79      0.36       118\n",
            "       hurto       0.94      0.55      0.69       669\n",
            "\n",
            "    accuracy                           0.58       787\n",
            "   macro avg       0.59      0.67      0.53       787\n",
            "weighted avg       0.83      0.58      0.64       787\n",
            "\n",
            "Matriz de confusión (val, t*):\n",
            " [[ 93  25]\n",
            " [304 365]]\n",
            "Operating threshold -> /content/drive/MyDrive/tesisV2/models/operating_threshold.json\n",
            "Log de entrenamiento: /content/drive/MyDrive/tesisV2/models/r3d18_ft_l34_trainlog.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x6hROCjVfWkw",
        "outputId": "241717ff-83dd-4756-e631-740d20b1cc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.0\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torchaudio==2.4.0\n",
            "  Downloading torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting filelock (from torch==2.4.0)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy (from torch==2.4.0)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.4.0)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.4.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.4.0)\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting setuptools (from torch==2.4.0)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting numpy (from torchvision==0.19.0)\n",
            "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.19.0)\n",
            "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m167.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m188.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.86\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.86:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.86\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.19.1\n",
            "    Uninstalling filelock-3.19.1:\n",
            "      Successfully uninstalled filelock-3.19.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0\n",
            "    Uninstalling torch-2.4.0:\n",
            "      Successfully uninstalled torch-2.4.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0\n",
            "    Uninstalling torchvision-0.19.0:\n",
            "      Successfully uninstalled torchvision-0.19.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.0\n",
            "    Uninstalling torchaudio-2.4.0:\n",
            "      Successfully uninstalled torchaudio-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.4.0 torchaudio-2.4.0 torchvision-0.19.0 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "mpmath",
                  "numpy",
                  "setuptools",
                  "sympy",
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "fb473a6de5ce4b6489a09dc2d220c36c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install --upgrade --force-reinstall --no-cache-dir torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCJlMSx_fWbU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# EVALUACIÓN (SIN ENTRENAR) con OpenCV\n",
        "# R3D-18 | t=0.30 + smoothing + histéresis + minDur\n",
        "# =========================\n",
        "import os, re, csv\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import cv2\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BASE_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV    = f\"{BASE_DIR}/val.csv\"\n",
        "TEST_CSV   = f\"{BASE_DIR}/test.csv\"\n",
        "CKPT_PATH  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# ---------- MODELO ----------\n",
        "NUM_CLASSES = 2\n",
        "CLIP_LEN = 16\n",
        "RESIZE   = 128\n",
        "CROP     = 112\n",
        "\n",
        "val_spatial = T.Compose([T.Resize((RESIZE, RESIZE)), T.CenterCrop(CROP)])\n",
        "\n",
        "class SimpleVideoEval(Dataset):\n",
        "    def __init__(self, csv_path, base_dir):\n",
        "        self.items=[]\n",
        "        self.base_dir=base_dir\n",
        "        with open(csv_path, newline='') as f:\n",
        "            r = csv.reader(f)\n",
        "            for row in r:\n",
        "                if not row: continue\n",
        "                head=row[0].strip().lower()\n",
        "                if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # header\n",
        "                    continue\n",
        "                path=row[0].strip()\n",
        "                y = 0 if str(row[1]).lower() in (\"0\",\"normal\") else 1\n",
        "                if not path.startswith(\"/\"):\n",
        "                    cand=os.path.join(base_dir, path)\n",
        "                    if os.path.exists(cand):\n",
        "                        path=cand\n",
        "                    else:\n",
        "                        sub=\"normal\" if y==0 else \"shoplifting\"\n",
        "                        path=os.path.join(base_dir, sub, path)\n",
        "                self.items.append((path, y))\n",
        "        # Normalización Kinetics\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def _read_video_cv2(self, path):\n",
        "        cap=cv2.VideoCapture(str(path))\n",
        "        frames=[]\n",
        "        while True:\n",
        "            ok, fr=cap.read()\n",
        "            if not ok: break\n",
        "            fr=cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(fr)\n",
        "        cap.release()\n",
        "        if len(frames)==0:\n",
        "            raise RuntimeError(f\"No se pudo leer: {path}\")\n",
        "        T_total=len(frames)\n",
        "        if T_total>=CLIP_LEN:\n",
        "            start=max(0,(T_total-CLIP_LEN)//2)  # centro para eval\n",
        "            frames=frames[start:start+CLIP_LEN]\n",
        "        else:\n",
        "            last=frames[-1]\n",
        "            frames=frames + [last]*(CLIP_LEN-T_total)\n",
        "        arr=np.stack(frames, axis=0)                 # [T,H,W,3] uint8\n",
        "        ten=torch.from_numpy(arr).permute(0,3,1,2)   # [T,3,H,W]\n",
        "        return ten\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path,y=self.items[idx]\n",
        "        clip=self._read_video_cv2(path)                  # [T,3,H,W]\n",
        "        clip=torch.stack([val_spatial(fr) for fr in clip])\n",
        "        clip=clip.float()/255.0\n",
        "        clip=(clip - self.mean)/self.std\n",
        "        clip=clip.permute(1,0,2,3).contiguous()          # [3,T,H,W]\n",
        "        return clip, torch.tensor(y, dtype=torch.long), path\n",
        "\n",
        "def build_model():\n",
        "    try:\n",
        "        model=r3d_18(weights=\"KINETICS400_V1\")\n",
        "    except TypeError:\n",
        "        model=r3d_18(pretrained=True)\n",
        "    in_feats=model.fc.in_features\n",
        "    model.fc=nn.Sequential(nn.Dropout(0.5), nn.Linear(in_feats, NUM_CLASSES))\n",
        "    sd=torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "    state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint cargado:\", CKPT_PATH)\n",
        "    if missing or unexpected:\n",
        "        print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "    return model.to(DEVICE).eval()\n",
        "\n",
        "# ---------- INFERENCIA ----------\n",
        "def infer_loader(model, loader):\n",
        "    probs_all, ys_all, paths_all = [], [], []\n",
        "    soft=nn.Softmax(dim=1)\n",
        "    torch.set_grad_enabled(False)\n",
        "    for x,y,paths in loader:\n",
        "        x=x.to(DEVICE, non_blocking=True)\n",
        "        logits=model(x)\n",
        "        probs=soft(logits)[:,1].detach().cpu().numpy()\n",
        "        probs_all.append(probs)\n",
        "        ys_all.append(y.numpy())\n",
        "        paths_all.extend(list(paths))\n",
        "    return np.concatenate(probs_all), np.concatenate(ys_all), paths_all\n",
        "\n",
        "# ---------- POST-PROCESO ----------\n",
        "HOP_SECONDS   = 0.5   # ajustá si tus clips provienen de hop distinto\n",
        "SMOOTH_K      = 5     # ~2.5 s si hop=0.5\n",
        "HYST_ON       = 0.35\n",
        "HYST_OFF      = 0.25\n",
        "MIN_EVENT_SEC = 1.0\n",
        "COOLDOWN_SEC  = 2.0\n",
        "THRESH_BASE   = 0.30\n",
        "\n",
        "def moving_avg(a,k):\n",
        "    if k<=1: return a\n",
        "    pad=(k-1)//2\n",
        "    a_pad=np.pad(a,(pad,pad),mode='edge')\n",
        "    ker=np.ones(k)/k\n",
        "    return np.convolve(a_pad,ker,mode='valid')\n",
        "\n",
        "def hysteresis_events(probs,on,off,hop_s,min_event_s,cooldown_s):\n",
        "    active=False; start=None; events=[]\n",
        "    for i,p in enumerate(probs):\n",
        "        if not active and p>=on:\n",
        "            active=True; start=i\n",
        "        elif active and p<=off:\n",
        "            end=i\n",
        "            if (end-start)*hop_s>=min_event_s:\n",
        "                events.append([start,end])\n",
        "            active=False\n",
        "    if active:\n",
        "        end=len(probs)\n",
        "        if (end-start)*hop_s>=min_event_s:\n",
        "            events.append([start,end])\n",
        "    merged=[]\n",
        "    for s,e in events:\n",
        "        if not merged: merged.append([s,e])\n",
        "        else:\n",
        "            ps,pe=merged[-1]\n",
        "            if (s-pe)*hop_s<COOLDOWN_SEC:\n",
        "                merged[-1][1]=e\n",
        "            else:\n",
        "                merged.append([s,e])\n",
        "    return merged\n",
        "\n",
        "def events_to_clip_labels(n, events):\n",
        "    y=np.zeros(n, dtype=np.int64)\n",
        "    for s,e in events: y[s:e]=1\n",
        "    return y\n",
        "\n",
        "_idx_pat = re.compile(r\"(?:_|-)(\\d{1,6})(?=\\D*$)\")\n",
        "def infer_vid_and_idx(path):\n",
        "    p=str(path); stem=Path(p).stem\n",
        "    m=_idx_pat.search(stem)\n",
        "    if m:\n",
        "        clip_idx=int(m.group(1))\n",
        "        video_id=stem[:m.start()] or Path(p).parent.name\n",
        "    else:\n",
        "        video_id=Path(p).parent.name\n",
        "        clip_idx=stem\n",
        "    return video_id, clip_idx\n",
        "\n",
        "def natkey(x):\n",
        "    try: return int(x)\n",
        "    except: return x\n",
        "\n",
        "def post_metrics(probs, ys, paths):\n",
        "    # baseline (sin post)\n",
        "    yb=(probs>=THRESH_BASE).astype(int)\n",
        "    cm_base=confusion_matrix(ys, yb, labels=[0,1])\n",
        "    rep_base=classification_report(ys, yb, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "\n",
        "    # agrupar por video para post-proceso temporal\n",
        "    from collections import defaultdict\n",
        "    buckets=defaultdict(list)\n",
        "    for p,pr,yv in zip(paths, probs, ys):\n",
        "        vid,idx=infer_vid_and_idx(p)\n",
        "        buckets[vid].append((idx, float(pr), int(yv)))\n",
        "\n",
        "    y_true_all=[]; y_pred_all=[]\n",
        "    for vid, triples in buckets.items():\n",
        "        triples.sort(key=lambda t: natkey(t[0]))\n",
        "        pv=np.array([t[1] for t in triples], float)\n",
        "        yv=np.array([t[2] for t in triples], int)\n",
        "        n=len(pv)\n",
        "        sm=moving_avg(pv, SMOOTH_K)\n",
        "        evs=hysteresis_events(sm, HYST_ON, HYST_OFF, HOP_SECONDS, MIN_EVENT_SEC, COOLDOWN_SEC)\n",
        "        yhat=events_to_clip_labels(n, evs)\n",
        "        y_true_all.append(yv); y_pred_all.append(yhat)\n",
        "\n",
        "    if y_true_all:\n",
        "        y_true_all=np.concatenate(y_true_all)\n",
        "        y_pred_all=np.concatenate(y_pred_all)\n",
        "        cm_post=confusion_matrix(y_true_all, y_pred_all, labels=[0,1])\n",
        "        rep_post=classification_report(y_true_all, y_pred_all, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "    else:\n",
        "        cm_post, rep_post = cm_base, rep_base  # fallback si no se pudo agrupar\n",
        "    return (cm_base, rep_base), (cm_post, rep_post)\n",
        "\n",
        "# ---------- CORRER ----------\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "model=build_model()\n",
        "\n",
        "for split_name, csv_path in [(\"VAL\", VAL_CSV), (\"TEST\", TEST_CSV)]:\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"[{split_name}] No existe:\", csv_path); continue\n",
        "    ds=SimpleVideoEval(csv_path, BASE_DIR)\n",
        "    dl=DataLoader(ds, batch_size=8, shuffle=False, num_workers=0)\n",
        "    probs, ys, paths = infer_loader(model, dl)\n",
        "\n",
        "    print(f\"\\n===== {split_name} =====\")\n",
        "    (cm_b, rep_b), (cm_p, rep_p) = post_metrics(probs, ys, paths)\n",
        "\n",
        "    print(\"\\n--- BASELINE t=0.30 (sin post) ---\")\n",
        "    print(cm_b); print(rep_b)\n",
        "\n",
        "    print(\"\\n--- (POST) smoothing + histéresis + minDur ---\")\n",
        "    print(cm_p); print(rep_p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9YdTpDCL3fZ",
        "outputId": "e0bbac76-ad4a-4268-bb81-c2345ac9f397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: False\n",
            "Checkpoint cargado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "\n",
            "===== VAL =====\n",
            "\n",
            "--- BASELINE t=0.30 (sin post) ---\n",
            "[[ 93  25]\n",
            " [304 365]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.234     0.788     0.361       118\n",
            "       hurto      0.936     0.546     0.689       669\n",
            "\n",
            "    accuracy                          0.582       787\n",
            "   macro avg      0.585     0.667     0.525       787\n",
            "weighted avg      0.831     0.582     0.640       787\n",
            "\n",
            "\n",
            "--- (POST) smoothing + histéresis + minDur ---\n",
            "[[101  17]\n",
            " [302 367]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.251     0.856     0.388       118\n",
            "       hurto      0.956     0.549     0.697       669\n",
            "\n",
            "    accuracy                          0.595       787\n",
            "   macro avg      0.603     0.702     0.542       787\n",
            "weighted avg      0.850     0.595     0.651       787\n",
            "\n",
            "\n",
            "===== TEST =====\n",
            "\n",
            "--- BASELINE t=0.30 (sin post) ---\n",
            "[[159  32]\n",
            " [169 410]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.485     0.832     0.613       191\n",
            "       hurto      0.928     0.708     0.803       579\n",
            "\n",
            "    accuracy                          0.739       770\n",
            "   macro avg      0.706     0.770     0.708       770\n",
            "weighted avg      0.818     0.739     0.756       770\n",
            "\n",
            "\n",
            "--- (POST) smoothing + histéresis + minDur ---\n",
            "[[163  28]\n",
            " [158 421]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.508     0.853     0.637       191\n",
            "       hurto      0.938     0.727     0.819       579\n",
            "\n",
            "    accuracy                          0.758       770\n",
            "   macro avg      0.723     0.790     0.728       770\n",
            "weighted avg      0.831     0.758     0.774       770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2UwratXvHSQY",
        "outputId": "b16c295c-c308-44c8-da3b-1c5c9d41b6a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1962671703.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1962671703.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    sudo apt-get update -y\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install av==10.0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O75QUUX0NRsF",
        "outputId": "36abf0e1-f9cc-455e-cace-496aa70405bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Colab OFFLINE PIPELINE — R3D-18 + Filtros idénticos a RT\n",
        "# Autor: tú + ChatGPT — pensado para copiar/pegar por celdas.\n",
        "# Probado en Google Colab (CPU/GPU). Sin dependencias raras.\n",
        "# =============================================================\n",
        "#\n",
        "# QUÉ HACE\n",
        "# 1) Config global (umbral, smoothing, ventana deslizante, etc.)\n",
        "# 2) Carga tu checkpoint R3D-18 (2 clases: 0=normal, 1=shoplifting)\n",
        "# 3) Lee videos (uno o carpeta), hace inferencia por ventanas 3D\n",
        "# 4) Aplica filtros idénticos a los de RT: EMA, histéresis, frame-skip,\n",
        "#    gate por motion, FORCE_ALERT_EVERY, etc.\n",
        "# 5) Genera:\n",
        "#    - CSV y JSON de eventos (inicio/fin, confianza, duración)\n",
        "#    - MP4 con overlay (barra de confianza y rótulos)\n",
        "#    - Clips .mp4 de cada evento (pre y post)\n",
        "# 6) (Opcional) Si le pasás un GT .csv por video (start_sec,end_sec,label)\n",
        "#    calcula métricas simples por video.\n",
        "#\n",
        "# NOTA: Mantuvimos nombres muy parecidos a los que usás en RT para que\n",
        "#       luego sólo sea copiar la misma config.\n",
        "# =============================================================\n",
        "\n",
        "# =====================\n",
        "# Celda 1 — Instalación\n",
        "# =====================\n",
        "# Ejecutá esta celda una vez por runtime.\n",
        "\n",
        "!pip -q install opencv-python torch torchvision pandas matplotlib tqdm moviepy\n",
        "\n",
        "# =====================\n",
        "# Celda 2 — Imports\n",
        "# =====================\n",
        "import os, math, json, shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.video import r3d_18\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "# =====================\n",
        "# Celda 3 — Config\n",
        "# =====================\n",
        "class Cfg:\n",
        "    # Entradas\n",
        "    INPUT_VIDEO     = \"/content/video_prueba.mp4\"   # o carpeta\n",
        "    IS_DIR          = False                           # True si INPUT_VIDEO es carpeta\n",
        "\n",
        "    # Modelo\n",
        "    WEIGHTS_PATH    = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "    DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    NUM_CLASSES     = 2\n",
        "\n",
        "    # Preproc / Ventanas (igual que RT)\n",
        "    CLIP_LEN        = 16          # frames por clip 3D\n",
        "    FPS_INFER       = 16          # remuestreo aproximado\n",
        "    HOP_SECONDS     = 0.5         # stride temporal de la ventana\n",
        "\n",
        "    # Filtros y umbrales (igual que RT)\n",
        "    THRESHOLD_SHOP  = 0.50        # umbral alto de entrada\n",
        "    THRESHOLD_EXIT  = 0.35        # umbral bajo de salida (histéresis)\n",
        "    EMA_ALPHA       = 0.60        # smoothing exponencial\n",
        "    FRAME_SKIP      = 1           # saltar cada N-1 frames en lectura\n",
        "    MOTION_THRESH   = 2.0         # gate por movimiento (MSE frame diff). 0 = desactiva\n",
        "    FORCE_ALERT_EVERY = 8.0       # fuerza alertas cada N s si hay confianza sostenida; 0 desactiva\n",
        "\n",
        "    # Salidas\n",
        "    OUT_DIR         = \"/content/offline_outputs\"\n",
        "    CLIP_SEC_BEFORE = 2.0\n",
        "    CLIP_SEC_AFTER  = 3.0\n",
        "\n",
        "    # Overlay\n",
        "    WRITE_OVERLAY   = True\n",
        "    SHOW_BAR_HEIGHT = 30\n",
        "\n",
        "    # Métricas (opcional)\n",
        "    GT_CSV          = None  # ej: \"/content/gt/video_prueba.csv\" con columnas [start_sec,end_sec,label]\n",
        "\n",
        "os.makedirs(Cfg.OUT_DIR, exist_ok=True)\n",
        "\n",
        "# =====================\n",
        "# Celda 4 — Utilidades\n",
        "# =====================\n",
        "def load_model(weights_path: str, device: str):\n",
        "    model = r3d_18(weights=None)\n",
        "    model.fc = nn.Linear(model.fc.in_features, Cfg.NUM_CLASSES)\n",
        "    sd = torch.load(weights_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(sd if isinstance(sd, dict) else sd.state_dict())\n",
        "    model.eval().to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def center_crop_resize(frames, size=(112,112)):\n",
        "    # frames: np.array [T,H,W,3] BGR -> RGB, crop/resize a 112.\n",
        "    T,H,W,_ = frames.shape\n",
        "    min_side = min(H,W)\n",
        "    y0 = (H-min_side)//2\n",
        "    x0 = (W-min_side)//2\n",
        "    out = []\n",
        "    for t in range(T):\n",
        "        f = frames[t][y0:y0+min_side, x0:x0+min_side]\n",
        "        f = cv2.resize(f, size, interpolation=cv2.INTER_AREA)\n",
        "        f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
        "        out.append(f)\n",
        "    x = np.stack(out,0).astype(np.float32) / 255.0\n",
        "    # normalización simple (como Kinetics)\n",
        "    mean = np.array([0.43216, 0.394666, 0.37645], dtype=np.float32)\n",
        "    std  = np.array([0.22803, 0.22145, 0.216989], dtype=np.float32)\n",
        "    x = (x - mean) / std\n",
        "    # to tensor [1,3,T,H,W]\n",
        "    x = torch.from_numpy(x).permute(3,0,1,2).unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def softmax_logits(logits):\n",
        "    return F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def ema(prev, new, alpha):\n",
        "    return alpha*new + (1-alpha)*(prev if prev is not None else new)\n",
        "\n",
        "\n",
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def motion_gate(prev_frame, cur_frame):\n",
        "    if Cfg.MOTION_THRESH <= 0:\n",
        "        return True, 0.0\n",
        "    if prev_frame is None or cur_frame is None:\n",
        "        return True, 0.0\n",
        "    # MSE entre frames grises\n",
        "    a = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    b = cv2.cvtColor(cur_frame, cv2.COLOR_BGR2GRAY)\n",
        "    diff = ((a.astype(np.float32)-b.astype(np.float32))**2).mean()\n",
        "    return (diff >= Cfg.MOTION_THRESH), float(diff)\n",
        "\n",
        "\n",
        "def write_event_clip(video_path, t0, t1, out_dir):\n",
        "    try:\n",
        "        ensure_dir(out_dir)\n",
        "        base = Path(video_path).stem\n",
        "        outp = str(Path(out_dir, f\"{base}_{t0:.2f}_{t1:.2f}.mp4\"))\n",
        "        ffmpeg_extract_subclip(video_path, max(0,t0), max(t0,t1), targetname=outp)\n",
        "        return outp\n",
        "    except Exception as e:\n",
        "        print(\"ffmpeg_extract_subclip error:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Celda 5 — Inferencia por video (ventanas + filtros)\n",
        "# =====================\n",
        "@torch.no_grad()\n",
        "def process_video(video_path: str, model):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"No se puede abrir {video_path}\")\n",
        "\n",
        "    # Info FPS y duración\n",
        "    fps_raw = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = total_frames / max(fps_raw,1e-6)\n",
        "\n",
        "    # Remuestreo simple por frame-skip para aproximar FPS_INFER\n",
        "    skip = max(1, int(round((fps_raw / Cfg.FPS_INFER) * Cfg.FRAME_SKIP)))\n",
        "\n",
        "    # Buffers\n",
        "    frames = []\n",
        "    times = []\n",
        "\n",
        "    prev_for_motion = None\n",
        "    ema_conf = None\n",
        "    last_force_alert_t = -1e9\n",
        "\n",
        "    # Eventos (con histéresis)\n",
        "    active = False\n",
        "    evt_start_t = None\n",
        "    events = []\n",
        "\n",
        "    # Overlay writer\n",
        "    overlay_path = str(Path(Cfg.OUT_DIR, Path(video_path).stem + \"_overlay.mp4\"))\n",
        "    writer = None\n",
        "\n",
        "    # Lectura\n",
        "    t = 0.0\n",
        "    i = 0\n",
        "    with tqdm(total=total_frames, desc=f\"Leyendo {Path(video_path).name}\") as pbar:\n",
        "        while True:\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            if i % skip != 0:\n",
        "                i += 1; pbar.update(1)\n",
        "                continue\n",
        "            h,w,_ = frame.shape\n",
        "\n",
        "            # Motion gate\n",
        "            allowed, mscore = motion_gate(prev_for_motion, frame)\n",
        "            prev_for_motion = frame.copy()\n",
        "\n",
        "            frames.append(frame)\n",
        "            times.append(cap.get(cv2.CAP_PROP_POS_MSEC)/1000.0)\n",
        "\n",
        "            # Ventana lista?\n",
        "            if len(frames) >= Cfg.CLIP_LEN:\n",
        "                clip_np = np.stack(frames[-Cfg.CLIP_LEN:],0)   # [T,H,W,3]\n",
        "                x = center_crop_resize(clip_np).to(Cfg.DEVICE)\n",
        "                logits = model(x)\n",
        "                probs = softmax_logits(logits)[0]\n",
        "                p_shop = float(probs[1]) if allowed else 0.0\n",
        "                ema_conf = ema(ema_conf, p_shop, Cfg.EMA_ALPHA)\n",
        "\n",
        "                cur_t = times[-1]\n",
        "                # Histéresis + FORCE_ALERT\n",
        "                if not active:\n",
        "                    force_ok = (Cfg.FORCE_ALERT_EVERY>0 and (cur_t - last_force_alert_t)>=Cfg.FORCE_ALERT_EVERY and (ema_conf or 0)>Cfg.THRESHOLD_EXIT)\n",
        "                    if (ema_conf is not None and ema_conf >= Cfg.THRESHOLD_SHOP) or force_ok:\n",
        "                        active = True\n",
        "                        evt_start_t = cur_t\n",
        "                        last_force_alert_t = cur_t\n",
        "                else:\n",
        "                    if ema_conf is not None and ema_conf < Cfg.THRESHOLD_EXIT:\n",
        "                        # cerrar evento\n",
        "                        evt_end_t = cur_t\n",
        "                        events.append({\n",
        "                            \"start\": float(max(0.0, evt_start_t)),\n",
        "                            \"end\": float(evt_end_t),\n",
        "                            \"duration\": float(max(0.0, evt_end_t-evt_start_t)),\n",
        "                            \"peak_conf\": float(ema_conf)\n",
        "                        })\n",
        "                        active = False\n",
        "\n",
        "                # Overlay init\n",
        "                if Cfg.WRITE_OVERLAY:\n",
        "                    if writer is None:\n",
        "                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                        writer = cv2.VideoWriter(overlay_path, fourcc, max(fps_raw/skip,1), (w, h+Cfg.SHOW_BAR_HEIGHT))\n",
        "                    # dibujar barra\n",
        "                    bar = np.zeros((Cfg.SHOW_BAR_HEIGHT, w, 3), dtype=np.uint8)\n",
        "                    xlen = int(w * max(0.0, min(1.0, ema_conf or 0.0)))\n",
        "                    color = (0,255,0) if (ema_conf or 0.0) < Cfg.THRESHOLD_EXIT else (0,255,255) if (ema_conf or 0.0) < Cfg.THRESHOLD_SHOP else (0,0,255)\n",
        "                    cv2.rectangle(bar, (0,0), (xlen, Cfg.SHOW_BAR_HEIGHT-1), color, -1)\n",
        "                    txt = f\"p(shop)_EMA={ema_conf:.2f} | gate_motion={mscore:.1f} | {'ACTIVE' if active else 'idle'}\"\n",
        "                    cv2.putText(bar, txt, (10, int(Cfg.SHOW_BAR_HEIGHT*0.7)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
        "                    # estado arriba del frame\n",
        "                    frame2 = frame.copy()\n",
        "                    if active:\n",
        "                        cv2.putText(frame2, \"EVENTO\", (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 3, cv2.LINE_AA)\n",
        "                    stacked = np.vstack([frame2, bar])\n",
        "                    writer.write(stacked)\n",
        "\n",
        "            i += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "\n",
        "    # Cerrar evento abierto al final\n",
        "    if active and evt_start_t is not None:\n",
        "        events.append({\n",
        "            \"start\": float(max(0.0, evt_start_t)),\n",
        "            \"end\": float(duration),\n",
        "            \"duration\": float(max(0.0, duration-evt_start_t)),\n",
        "            \"peak_conf\": float(ema_conf or 0.0)\n",
        "        })\n",
        "\n",
        "    # Guardar eventos/archivos\n",
        "    base = Path(video_path).stem\n",
        "    ev_json = Path(Cfg.OUT_DIR, f\"{base}_events.json\")\n",
        "    ev_csv  = Path(Cfg.OUT_DIR, f\"{base}_events.csv\")\n",
        "\n",
        "    with open(ev_json, \"w\") as f:\n",
        "        json.dump({\"video\": video_path, \"events\": events}, f, indent=2)\n",
        "\n",
        "    pd.DataFrame(events).to_csv(ev_csv, index=False)\n",
        "\n",
        "    # Clips por evento\n",
        "    clips_dir = Path(Cfg.OUT_DIR, f\"{base}_clips\")\n",
        "    for ev in events:\n",
        "        t0 = max(0.0, ev[\"start\"] - Cfg.CLIP_SEC_BEFORE)\n",
        "        t1 = min(duration, ev[\"end\"] + Cfg.CLIP_SEC_AFTER)\n",
        "        write_event_clip(video_path, t0, t1, clips_dir)\n",
        "\n",
        "    return {\n",
        "        \"overlay\": overlay_path if Cfg.WRITE_OVERLAY else None,\n",
        "        \"events_json\": str(ev_json),\n",
        "        \"events_csv\": str(ev_csv),\n",
        "        \"clips_dir\": str(clips_dir),\n",
        "        \"duration\": duration,\n",
        "        \"fps_raw\": fps_raw,\n",
        "        \"total_frames\": total_frames\n",
        "    }\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Celda 6 — Lote (carpeta o único video)\n",
        "# =====================\n",
        "def run_offline():\n",
        "    model = load_model(Cfg.WEIGHTS_PATH, Cfg.DEVICE)\n",
        "\n",
        "    inputs = []\n",
        "    if Cfg.IS_DIR:\n",
        "        for ext in (\"*.mp4\",\"*.avi\",\"*.mov\",\"*.mkv\"):\n",
        "            inputs += list(Path(Cfg.INPUT_VIDEO).glob(ext))\n",
        "        inputs = [str(p) for p in sorted(inputs)]\n",
        "    else:\n",
        "        inputs = [Cfg.INPUT_VIDEO]\n",
        "\n",
        "    summary_rows = []\n",
        "    for vp in inputs:\n",
        "        print(\"\\n>>> Procesando:\", vp)\n",
        "        res = process_video(vp, model)\n",
        "        n_events = len(pd.read_csv(res[\"events_csv\"])) if os.path.exists(res[\"events_csv\"]) else 0\n",
        "        summary_rows.append({\n",
        "            \"video\": vp,\n",
        "            \"events\": n_events,\n",
        "            \"overlay\": res[\"overlay\"],\n",
        "            \"events_csv\": res[\"events_csv\"],\n",
        "            \"clips_dir\": res[\"clips_dir\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(summary_rows)\n",
        "    df_path = Path(Cfg.OUT_DIR, \"_summary.csv\")\n",
        "    df.to_csv(df_path, index=False)\n",
        "    print(\"\\nResumen guardado en:\", df_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Celda 7 — Métricas simples (opcional con GT por video)\n",
        "# =====================\n",
        "# Formato GT CSV por video: start_sec,end_sec,label  (label: 1=shop, 0=normal)\n",
        "# Se hace un matching IoU temporal con eventos detectados (label=1)\n",
        "\n",
        "def interval_iou(a, b):\n",
        "    s1,e1 = a\n",
        "    s2,e2 = b\n",
        "    inter = max(0.0, min(e1,e2) - max(s1,s2))\n",
        "    union = max(e1,e2) - min(s1,s2)\n",
        "    return inter/union if union>0 else 0.0\n",
        "\n",
        "\n",
        "def eval_with_gt(video_path):\n",
        "    base = Path(video_path).stem\n",
        "    ev_csv = Path(Cfg.OUT_DIR, f\"{base}_events.csv\")\n",
        "    if not os.path.exists(ev_csv) or not Cfg.GT_CSV:\n",
        "        print(\"No hay eventos o GT configurado.\")\n",
        "        return None\n",
        "    ev = pd.read_csv(ev_csv)\n",
        "    gt = pd.read_csv(Cfg.GT_CSV)\n",
        "    # Filtrar GT label=1\n",
        "    gt1 = gt[gt[\"label\"]==1]\n",
        "    # Matching por IoU >= 0.3\n",
        "    TP=0; FP=0; FN=0\n",
        "    used = set()\n",
        "    for _,e in ev.iterrows():\n",
        "        span_e = (e[\"start\"], e[\"end\"])\n",
        "        best_iou=0; best_j=None\n",
        "        for j,(idx,g) in enumerate(gt1.iterrows()):\n",
        "            if j in used: continue\n",
        "            span_g = (g[\"start_sec\"], g[\"end_sec\"])\n",
        "            iou = interval_iou(span_e, span_g)\n",
        "            if iou>best_iou:\n",
        "                best_iou=iou; best_j=j\n",
        "        if best_iou>=0.3:\n",
        "            TP+=1; used.add(best_j)\n",
        "        else:\n",
        "            FP+=1\n",
        "    FN = len(gt1) - len(used)\n",
        "    prec = TP/max(1,TP+FP); rec = TP/max(1,TP+FN)\n",
        "    f1 = 2*prec*rec/max(1e-9,prec+rec)\n",
        "    out = {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"precision\":prec,\"recall\":rec,\"F1\":f1}\n",
        "    print(\"Métricas:\", out)\n",
        "    mpath = Path(Cfg.OUT_DIR, f\"{base}_metrics.json\")\n",
        "    with open(mpath, \"w\") as f:\n",
        "        json.dump(out,f,indent=2)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =====================\n",
        "# Celda 8 — Ejecución\n",
        "# =====================\n",
        "# 1) Ajustá rutas en Cfg arriba\n",
        "# 2) Ejecutá esta celda\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = run_offline()\n",
        "    print(df)\n",
        "    # (Opcional) eval con GT de un video\n",
        "    # eval_with_gt(Cfg.INPUT_VIDEO)\n",
        "\n",
        "\n",
        "# ========== DEMO LIMPIO v2 (idéntico a RT: EMA + histéresis + gate de movimiento) ==========\n",
        "# Este bloque mantiene el \"print bonito\" y solo genera CLIPS, pero ahora replica\n",
        "# los filtros del runtime: EMA exponencial, histéresis ON/OFF, gate por movimiento\n",
        "# (MSE frame-to-frame) y FORCE_ALERT_EVERY opcional.\n",
        "\n",
        "import os, json, csv, cv2, torch, numpy as np\n",
        "from torchvision.models.video import r3d_18\n",
        "from subprocess import run, DEVNULL\n",
        "from glob import glob\n",
        "\n",
        "# ----------------- CONFIG v2 -----------------\n",
        "VIDEO_IN   = \"/content/tesisV2/videos/supermas1-1.mp4\"\n",
        "OUT_DIR    = \"/content/tesisV2/demo_outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
        "BASENAME   = os.path.splitext(os.path.basename(VIDEO_IN))[0]\n",
        "\n",
        "# ¡Actualizá al ÚLTIMO modelo!\n",
        "MODEL_P    = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# Tracks opcional (para ID sospechoso)\n",
        "TRACKS_PATH = \"\"  # mismo formato que la versión anterior\n",
        "\n",
        "# Ventaneo / normalización\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE   = 112\n",
        "WIN_SEC, HOP_SEC = 2.5, 0.5\n",
        "\n",
        "# Clips\n",
        "PAD_S   = 0.6\n",
        "MAX_DUR = 12.0\n",
        "REENCODE = True\n",
        "\n",
        "# Filtros (idénticos al RT)\n",
        "FALLBACK_THR      = 0.24   # ON si el ckpt no trae umbral\n",
        "FALLBACK_THR_OFF  = 0.22   # OFF si ckpt no trae\n",
        "EMA_ALPHA         = 0.60   # suavizado exponencial\n",
        "MOTION_THRESH     = 2.0    # gate por movimiento (MSE). 0 desactiva\n",
        "FORCE_ALERT_EVERY = 8.0    # s (0 desactiva)\n",
        "\n",
        "# Mínimos para filtrar eventos espurios\n",
        "MIN_DUR  = 0.6\n",
        "MERGE_GAP_S = 0.5\n",
        "\n",
        "# ----------------- UTIL v2 -----------------\n",
        "def safe_fps_cap(cap):\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    return fps if fps and fps > 0 else 30.0\n",
        "\n",
        "_prev_for_motion = None\n",
        "\n",
        "def motion_allowed(frame):\n",
        "    global _prev_for_motion\n",
        "    if MOTION_THRESH <= 0:\n",
        "        _prev_for_motion = frame.copy(); return True, 0.0\n",
        "    if _prev_for_motion is None:\n",
        "        _prev_for_motion = frame.copy(); return True, 0.0\n",
        "    a = cv2.cvtColor(_prev_for_motion, cv2.COLOR_BGR2GRAY)\n",
        "    b = cv2.cvtColor(frame,            cv2.COLOR_BGR2GRAY)\n",
        "    diff = ((a.astype(np.float32)-b.astype(np.float32))**2).mean()\n",
        "    _prev_for_motion = frame.copy()\n",
        "    return (diff >= MOTION_THRESH), float(diff)\n",
        "\n",
        "\n",
        "def cache_resized_rgb_frames(path, target_size):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    fps = safe_fps_cap(cap)\n",
        "    n   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    frames = []\n",
        "    for _ in range(n):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok or fr is None: break\n",
        "        fr = cv2.resize(fr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    arr = np.stack(frames, axis=0) if frames else np.zeros((0,target_size,target_size,3),np.uint8)\n",
        "    return arr, fps, n\n",
        "\n",
        "\n",
        "def clip_tensor_from_cache(cache_bgr, idxs, mean, std, device):\n",
        "    idxs = np.clip(idxs, 0, len(cache_bgr)-1)\n",
        "    frames = cache_bgr[idxs]\n",
        "    # BGR->RGB y normalización en torch\n",
        "    frames = cv2.cvtColor(frames, cv2.COLOR_BGR2RGB)\n",
        "    x = torch.from_numpy(frames.astype(np.float32)/255.0).permute(3,0,1,2).to(device)  # C,T,H,W\n",
        "    x = (x-mean)/std\n",
        "    return x.unsqueeze(0)\n",
        "\n",
        "\n",
        "def cut_events_to_clips(video_path, events, out_dir, pad_s=0.5, max_dur_s=12.0,\n",
        "                        reencode=True, fps_hint=None, n_frames_hint=None, basename=\"clip\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if fps_hint is None or n_frames_hint is None:\n",
        "        cap_tmp = cv2.VideoCapture(video_path)\n",
        "        fps_l = safe_fps_cap(cap_tmp)\n",
        "        n_l = int(cap_tmp.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        cap_tmp.release()\n",
        "    else:\n",
        "        fps_l, n_l = fps_hint, n_frames_hint\n",
        "    duration = max(0.0, n_l / float(fps_l if fps_l else 30.0))\n",
        "\n",
        "    written = []\n",
        "    for idx, ev in enumerate(events, start=1):\n",
        "        t0 = max(0.0, float(ev[\"t_start\"]) - pad_s)\n",
        "        t1 = min(duration, float(ev[\"t_end\"]) + pad_s)\n",
        "        if max_dur_s is not None and (t1 - t0) > max_dur_s:\n",
        "            t1 = t0 + max_dur_s\n",
        "        if t1 <= t0: continue\n",
        "        out_mp4 = os.path.join(out_dir, f\"{basename}_event_{idx:03d}.mp4\")\n",
        "        if reencode:\n",
        "            cmd = [\n",
        "                \"ffmpeg\",\"-y\",\"-fflags\",\"+genpts\",\"-ss\",f\"{t0:.3f}\",\"-to\",f\"{t1:.3f}\",\n",
        "                \"-i\",video_path,\n",
        "                \"-vf\",\"scale=trunc(iw/2)*2:trunc(ih/2)*2\",\n",
        "                \"-c:v\",\"libx264\",\"-preset\",\"veryfast\",\"-crf\",\"23\",\"-profile:v\",\"high\",\"-level\",\"4.0\",\n",
        "                \"-pix_fmt\",\"yuv420p\",\"-c:a\",\"aac\",\"-b:a\",\"128k\",\"-ar\",\"44100\",\n",
        "                \"-movflags\",\"+faststart\",\"-shortest\", out_mp4\n",
        "            ]\n",
        "        else:\n",
        "            cmd = [\"ffmpeg\",\"-y\",\"-ss\",f\"{t0:.3f}\",\"-to\",f\"{t1:.3f}\",\"-i\",video_path,\"-c\",\"copy\",\"-movflags\",\"+faststart\",out_mp4]\n",
        "        run(cmd, stdout=DEVNULL, stderr=DEVNULL)\n",
        "        written.append(out_mp4)\n",
        "    return written\n",
        "\n",
        "\n",
        "def merge_close_events(events, gap_s=0.5):\n",
        "    if not events: return []\n",
        "    ev = sorted(events, key=lambda e: e[\"t_start\"])\n",
        "    merged = [ev[0]]\n",
        "    for e in ev[1:]:\n",
        "        prev = merged[-1]\n",
        "        if e[\"t_start\"] - prev[\"t_end\"] <= gap_s:\n",
        "            prev[\"t_end\"] = max(prev[\"t_end\"], e[\"t_end\"])\n",
        "            prev[\"p_max\"] = float(max(prev[\"p_max\"], e[\"p_max\"]))\n",
        "            prev[\"p_avg\"] = float((prev[\"p_avg\"] + e[\"p_avg\"]) / 2.0)\n",
        "        else:\n",
        "            merged.append(e)\n",
        "    return merged\n",
        "\n",
        "\n",
        "def pretty_header(title):\n",
        "    print(\"\n",
        "\" + \"═\"*80)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"═\"*80)\n",
        "\n",
        "def pretty_kv(k, v, pad=18):\n",
        "    print(f\"  {k:<{pad}}: {v}\")\n",
        "\n",
        "\n",
        "def main_v2():\n",
        "    print(\"▶ Procesando video (v2 filtros RT)...\")\n",
        "    device = \"cpu\"  # demo CPU\n",
        "\n",
        "    # Modelo\n",
        "    model = r3d_18(weights=None)\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
        "    ckpt = torch.load(MODEL_P, map_location=device)\n",
        "    # admitir ckpt como dict sencillo o con clave 'model'\n",
        "    sd = ckpt.get(\"model\", ckpt)\n",
        "    model.load_state_dict(sd)\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    # Umbral (del ckpt si existe, o fallback)\n",
        "    best_thr = float(ckpt.get(\"best_threshold\", FALLBACK_THR))\n",
        "    THRESH_HI = best_thr\n",
        "    THRESH_LO = max(0.8*best_thr, FALLBACK_THR_OFF)\n",
        "\n",
        "    # Normalización Kinetics\n",
        "    mean = torch.tensor([0.43216,0.394666,0.37645],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "    std  = torch.tensor([0.22803,0.22145,0.216989],dtype=torch.float32).view(3,1,1,1).to(device)\n",
        "\n",
        "    # Cache BGR (más veloz para motion)\n",
        "    cache_bgr, fps_cache, n_cache = cache_resized_rgb_frames(VIDEO_IN, IMG_SIZE)\n",
        "    if n_cache == 0:\n",
        "        print(\"⚠ No se pudieron leer frames del video.\"); return\n",
        "    duration = n_cache/float(fps_cache if fps_cache else 30.0)\n",
        "\n",
        "    # Ventanas\n",
        "    win = int(WIN_SEC*fps_cache); hop = int(HOP_SEC*fps_cache)\n",
        "    starts = list(range(0, max(1, n_cache - win + 1), hop))\n",
        "\n",
        "    win_times, probs_raw, probs_ema, states = [], [], [], []\n",
        "    ema_val = None\n",
        "    active = False\n",
        "    last_force_alert_t = -1e9\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start in starts:\n",
        "            idxs = np.linspace(start, start+win-1, num=NUM_FRAMES, dtype=int)\n",
        "            # gate por movimiento usando el último frame de la ventana\n",
        "            frame_for_motion = cache_bgr[min(start+win-1, len(cache_bgr)-1)]\n",
        "            allowed, mscore = motion_allowed(frame_for_motion)\n",
        "\n",
        "            x = clip_tensor_from_cache(cache_bgr, idxs, mean, std, device)\n",
        "            p1 = float(torch.softmax(model(x)[0], dim=0)[1].item())\n",
        "            if not allowed:\n",
        "                p1 = 0.0\n",
        "            probs_raw.append(p1)\n",
        "\n",
        "            # EMA\n",
        "            ema_val = EMA_ALPHA*p1 + (1-EMA_ALPHA)*(ema_val if ema_val is not None else p1)\n",
        "            probs_ema.append(ema_val)\n",
        "\n",
        "            t0 = start / fps_cache\n",
        "            t1 = (start + win) / fps_cache\n",
        "            win_times.append((t0, t1))\n",
        "\n",
        "    # Histéresis + FORCE_ALERT_EVERY\n",
        "    states = []\n",
        "    on = False\n",
        "    last_on_t = -1e9\n",
        "    for i, p in enumerate(probs_ema):\n",
        "        cur_t = win_times[i][1]\n",
        "        force_ok = (FORCE_ALERT_EVERY>0 and (cur_t - last_on_t) >= FORCE_ALERT_EVERY and p > THRESH_LO)\n",
        "        if not on and (p >= THRESH_HI or force_ok):\n",
        "            on = True; last_on_t = cur_t\n",
        "        elif on and p <= THRESH_LO:\n",
        "            on = False\n",
        "        states.append(int(on))\n",
        "\n",
        "    # Generar eventos (promedios y picos)\n",
        "    events, cur_on, cur_start, cur_probs = [], False, None, []\n",
        "    for i, st in enumerate(states):\n",
        "        t0, t1 = win_times[i]\n",
        "        if st and not cur_on:\n",
        "            cur_on = True; cur_start = t0; cur_probs = [probs_ema[i]]\n",
        "        elif st and cur_on:\n",
        "            cur_probs.append(probs_ema[i])\n",
        "        elif (not st) and cur_on:\n",
        "            events.append({\"t_start\": float(cur_start), \"t_end\": float(win_times[i-1][1]),\n",
        "                           \"p_max\": float(np.max(cur_probs)), \"p_avg\": float(np.mean(cur_probs))})\n",
        "            cur_on = False\n",
        "    if cur_on:\n",
        "        events.append({\"t_start\": float(cur_start), \"t_end\": float(win_times[len(states)-1][1]),\n",
        "                       \"p_max\": float(np.max(cur_probs)), \"p_avg\": float(np.mean(cur_probs))})\n",
        "\n",
        "    events = merge_close_events(events, gap_s=MERGE_GAP_S)\n",
        "\n",
        "    # Filtrado final por duración\n",
        "    events = [e for e in events if (e[\"t_end\"]-e[\"t_start\"]) >= MIN_DUR]\n",
        "\n",
        "    # Clips\n",
        "    events_dir = os.path.join(OUT_DIR, \"events_v2\"); os.makedirs(events_dir, exist_ok=True)\n",
        "    if events:\n",
        "        clips = cut_events_to_clips(VIDEO_IN, events, events_dir,\n",
        "                                    pad_s=PAD_S, max_dur_s=MAX_DUR, reencode=REENCODE,\n",
        "                                    fps_hint=fps_cache, n_frames_hint=n_cache, basename=BASENAME)\n",
        "    else:\n",
        "        clips = []\n",
        "\n",
        "    # Print bonito\n",
        "    pretty_header(\"⚙️  Parámetros (v2 RT-like)\")\n",
        "    pretty_kv(\"Modelo\", \"r3d_18\"); pretty_kv(\"Checkpoint\", os.path.basename(MODEL_P))\n",
        "    pretty_kv(\"Ventana (s)\", WIN_SEC); pretty_kv(\"Salto (s)\", HOP_SEC)\n",
        "    pretty_kv(\"Frames por clip\", NUM_FRAMES); pretty_kv(\"IMG\", f\"{IMG_SIZE}x{IMG_SIZE}\")\n",
        "    pretty_kv(\"EMA α\", EMA_ALPHA); pretty_kv(\"Gate motion MSE\", MOTION_THRESH)\n",
        "    pretty_kv(\"Force alert cada (s)\", FORCE_ALERT_EVERY)\n",
        "    pretty_kv(\"Thr ON\", f\"{THRESH_HI:.3f}\"); pretty_kv(\"Thr OFF\", f\"{THRESH_LO:.3f}\")\n",
        "\n",
        "    if events:\n",
        "        pretty_header(\"🕐  Eventos detectados (v2)\")\n",
        "        for i, e in enumerate(events, start=1):\n",
        "            dur = e[\"t_end\"]-e[\"t_start\"]\n",
        "            print(f\"  • Ev {i:02d} | {e['t_start']:.2f}s → {e['t_end']:.2f}s (dur {dur:.2f}s) | p_max={e['p_max']:.2f} p_avg={e['p_avg']:.2f}\")\n",
        "        if clips:\n",
        "            pretty_header(\"🎬 Clips generados (v2)\")\n",
        "            for c in clips: print(\"  -\", c)\n",
        "    else:\n",
        "        pretty_header(\"ℹ  Sin eventos (v2)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_v2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "3VMUKTpzqH_r",
        "outputId": "47db892d-b530-4b14-b728-27755cc19918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 539) (ipython-input-4048667747.py, line 539)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4048667747.py\"\u001b[0;36m, line \u001b[0;32m539\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 539)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CUARTO ENTRENAMIENTO**"
      ],
      "metadata": {
        "id": "WAmgyDobDQis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y update && apt-get -y install ffmpeg\n",
        "!pip install --no-cache-dir av==12.0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd1IdTj9DnpA",
        "outputId": "941317bb-17e6-4606-94d9-fd66afd88052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [1 InRelease 9,828 B/129 k\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [1 InRelease 43.1 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [1 InRelease 89.5 kB/129 k\r                                                                               \rHit:3 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [1 InRelease 89.5 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Waiting for headers] [Con\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,080 kB]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rGet:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [7 InRelease 3,632 B/3,632 B 100%] [Connected to r2u.s\r                                                                               \rHit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,735 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,428 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,936 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,812 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,755 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,354 kB]\n",
            "Fetched 33.5 MB in 3s (12.6 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Collecting av==12.0.0\n",
            "  Downloading av-12.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-12.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-12.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ENTRENAMIENTO GRANDE R3D-18 (A/B/C) - Shoplifting (A100-OPT)\n",
        "# =========================\n",
        "# - Fase A: l2+l3+l4+fc (10 ep)\n",
        "# - Fase B: full unfreeze + LRs discriminativos (10 ep)\n",
        "# - Fase C: fine-tune fino (10-12 ep)\n",
        "# Incluye: acumulación de gradiente, sampler balanceado, stride temporal,\n",
        "# augment robusto, reanudación completa y barrido de umbral con F2\n",
        "# Optimizado para A100: TF32/bfloat16, DataLoader seguro en notebook,\n",
        "# torch.compile opcional, y lectura de video robusta.\n",
        "# =========================\n",
        "\n",
        "import os, math, json, random, csv, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\".*pow_by_natural.*\",\n",
        "    module=r\"torch\\.utils\\._sympy\\.interp\"\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, f1_score\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_video\n",
        "from torchvision.models.video import r3d_18\n",
        "\n",
        "# =========================\n",
        "# CONFIG RÁPIDA (EDITAR AQUÍ)\n",
        "# =========================\n",
        "PHASE       = \"B\"    # \"A\" | \"B\" | \"C\"\n",
        "SEED        = 1337\n",
        "\n",
        "# ⚠️ Rutas en Drive\n",
        "BASE_DIR    = \"/content/drive/MyDrive/tesisV2/cortos/video_clips_192\"\n",
        "TRAIN_CSV   = f\"{BASE_DIR}/train.csv\"\n",
        "VAL_CSV     = f\"{BASE_DIR}/val.csv\"\n",
        "\n",
        "# EXPERIMENTO\n",
        "EXP_NAME    = \"exp_l234_fullrun\"\n",
        "ROOT_OUT    = \"/content/drive/MyDrive/tesisV2/models\"\n",
        "OUT_DIR     = f\"{ROOT_OUT}/{EXP_NAME}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# CHECKPOINT INICIAL (pesos del mejor previo)\n",
        "CKPT_IN     = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "# EP, batch y acumulación por fase\n",
        "EPOCHS_A, EPOCHS_B, EPOCHS_C = 10, 10, 12\n",
        "BATCH_SIZE  = 12   # ↑ A100\n",
        "ACCUM_STEPS = 4    # batch efectivo ~48\n",
        "\n",
        "# CLIP/CROP/STRIDE (ajustados por fase más abajo)\n",
        "CLIP_LEN_A = 32; TEMP_STRIDE_A = 2; CROP_A = 128\n",
        "CLIP_LEN_B = 32; TEMP_STRIDE_B = 2; CROP_B = 128\n",
        "CLIP_LEN_C = 40; TEMP_STRIDE_C = 2; CROP_C = 160  # subir contexto en fine\n",
        "\n",
        "# Pérdida y pesos\n",
        "GAMMA_A = 1.2; WEIGHT_DEC_A = 7e-4\n",
        "GAMMA_B = 1.2; WEIGHT_DEC_B = 1e-3\n",
        "GAMMA_C = 1.2; WEIGHT_DEC_C = 5e-4\n",
        "CLASS_WEIGHTS = torch.tensor([1.0, 1.6], dtype=torch.float32)  # normal, hurto\n",
        "\n",
        "# LRs base y discriminativos\n",
        "LR_A = 1.0e-4\n",
        "LR_B_FC, LR_B_L4, LR_B_L3, LR_B_L2, LR_B_L1 = 1.5e-4, 1.0e-4, 7.5e-5, 5.0e-5, 3.0e-5\n",
        "LR_C_FC, LR_C_L4, LR_C_L3, LR_C_L2, LR_C_L1 = 8.0e-5, 6.0e-5, 4.5e-5, 3.0e-5, 2.0e-5\n",
        "\n",
        "# Sweep thresholds y F-beta\n",
        "TH_MIN, TH_MAX, TH_STEPS = 0.20, 0.60, 41\n",
        "FBETAS = [1.0, 1.5, 2.0]  # fija operativo por F2\n",
        "\n",
        "# LOGS/CHECKPOINTS\n",
        "CKPT_BESTF1 = os.path.join(OUT_DIR, \"bestF1.pt\")\n",
        "CKPT_BESTL  = os.path.join(OUT_DIR, \"bestLoss.pt\")\n",
        "CKPT_LAST   = os.path.join(OUT_DIR, \"last.pt\")\n",
        "CKPT_RUN    = os.path.join(OUT_DIR, \"runstate.pt\")\n",
        "LOG_JSON    = os.path.join(OUT_DIR, \"trainlog.json\")\n",
        "THR_CSV     = os.path.join(OUT_DIR, \"threshold_sweep.csv\")\n",
        "OPER_JSON   = os.path.join(OUT_DIR, \"operating_threshold.json\")\n",
        "\n",
        "# =========================\n",
        "# DataLoader (ajustes SEGUROS para notebooks/Colab)\n",
        "# =========================\n",
        "# ⚠️ Para evitar crashes del decodificador de video en workers multiproceso:\n",
        "#    - NUM_WORKERS = 0 (main process)\n",
        "#    - Sólo usamos prefetch_factor/persistent_workers si workers > 0\n",
        "NUM_WORKERS = 0\n",
        "PINMEM      = True\n",
        "PREFETCH    = 2\n",
        "PERSISTENT  = False\n",
        "\n",
        "# Sampler balanceado\n",
        "USE_SAMPLER = True\n",
        "\n",
        "# torch.compile ON/OFF\n",
        "USE_COMPILE = False  # <<<<<< APAGADO para evitar CUDAGraphs overwrite\n",
        "\n",
        "# =========================\n",
        "# UTILIDADES\n",
        "# =========================\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# — Seleccionar backend de video estable —\n",
        "try:\n",
        "    # Preferimos \"video_reader\" (C++), más estable que \"pyav\" en multiproceso\n",
        "    torchvision.set_video_backend(\"video_reader\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# autocast en bfloat16 (ideal en A100). Si no soporta, cae a fp16+scaler.\n",
        "USE_BF16 = torch.cuda.is_available()\n",
        "try:\n",
        "    from torch.amp import autocast, GradScaler\n",
        "    AUTOCast_KW = dict(device_type=\"cuda\", dtype=torch.bfloat16, enabled=USE_BF16)\n",
        "    scaler = GradScaler(\"cuda\", enabled=False)  # bf16 no necesita scaler\n",
        "except Exception:\n",
        "    from contextlib import contextmanager\n",
        "    @contextmanager\n",
        "    def autocast(*a, **k): yield\n",
        "    class GradScaler:\n",
        "        def __init__(self,*a,**k): pass\n",
        "        def scale(self,x): return x\n",
        "        def step(self,opt): opt.step()\n",
        "        def update(self): pass\n",
        "    AUTOCast_KW = dict()\n",
        "\n",
        "# =========================\n",
        "# DATASET\n",
        "# =========================\n",
        "RESIZE = 144\n",
        "\n",
        "# Control de warnings de lectura fallida (logua sólo 1 vez)\n",
        "_GLOBAL_READ_FAIL_LOGGED = False\n",
        "\n",
        "def _parse_label(s):\n",
        "    s = str(s).strip().lower()\n",
        "    if s.isdigit(): return int(s)\n",
        "    if s in (\"normal\", \"0\"): return 0\n",
        "    if s in (\"shoplifting\", \"hurto\", \"1\"): return 1\n",
        "    raise ValueError(f\"Etiqueta desconocida: {s}\")\n",
        "\n",
        "def load_list(csv_path):\n",
        "    items = []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        r = csv.reader(f)\n",
        "        for row in r:\n",
        "            if not row: continue\n",
        "            head = str(row[0]).strip().lower()\n",
        "            if head in (\"path\",\"ruta\",\"file\",\"filename\"):\n",
        "                continue\n",
        "            path = row[0].strip()\n",
        "            y    = _parse_label(row[1])\n",
        "            if not path.startswith(\"/\"):\n",
        "                cand = os.path.join(BASE_DIR, path)\n",
        "                if os.path.exists(cand): path = cand\n",
        "                else:\n",
        "                    subdir = \"normal\" if y == 0 else \"shoplifting\"\n",
        "                    path = os.path.join(BASE_DIR, subdir, path)\n",
        "            items.append((path, y))\n",
        "    return items\n",
        "\n",
        "def _safe_read_video(path, pts_unit=\"sec\"):\n",
        "    \"\"\"Lectura robusta: intenta backend actual; si falla, prueba pyav; si falla, devuelve None.\"\"\"\n",
        "    global _GLOBAL_READ_FAIL_LOGGED\n",
        "    try:\n",
        "        vid, _, _ = read_video(path, pts_unit=pts_unit, output_format=\"TCHW\")\n",
        "        return vid\n",
        "    except Exception:\n",
        "        # Intentar backend alternativo\n",
        "        try:\n",
        "            torchvision.set_video_backend(\"pyav\")\n",
        "            vid, _, _ = read_video(path, pts_unit=pts_unit, output_format=\"TCHW\")\n",
        "            # restaurar (si estaba disponible)\n",
        "            try: torchvision.set_video_backend(\"video_reader\")\n",
        "            except Exception: pass\n",
        "            return vid\n",
        "        except Exception:\n",
        "            # restaurar y devolver None\n",
        "            try: torchvision.set_video_backend(\"video_reader\")\n",
        "            except Exception: pass\n",
        "            if not _GLOBAL_READ_FAIL_LOGGED:\n",
        "                print(f\"⚠️ read_video falló: {path}. Se usará clip dummy para evitar caída (sólo se muestra una vez).\")\n",
        "                _GLOBAL_READ_FAIL_LOGGED = True\n",
        "            return None\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, items, train=True, clip_len=32, temp_stride=2, crop=128):\n",
        "        self.items = items\n",
        "        self.train = train\n",
        "        self.clip_len = clip_len\n",
        "        self.temp_stride = temp_stride\n",
        "        self.crop = crop\n",
        "        if train:\n",
        "            self.spatial = T.Compose([\n",
        "                T.Resize((RESIZE, RESIZE)),\n",
        "                T.RandomResizedCrop(self.crop, scale=(0.65, 1.0), ratio=(0.8, 1.25)),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.RandomApply([T.ColorJitter(0.2,0.2,0.2,0.06)], p=0.5),\n",
        "                T.RandomGrayscale(p=0.10),\n",
        "                T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.1)),\n",
        "            ])\n",
        "        else:\n",
        "            self.spatial = T.Compose([\n",
        "                T.Resize((RESIZE, RESIZE)),\n",
        "                T.CenterCrop(self.crop),\n",
        "            ])\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.items[idx]\n",
        "        video = _safe_read_video(path, pts_unit=\"sec\")\n",
        "        if video is None:\n",
        "            # Clip dummy (para no romper el entrenamiento si un archivo está corrupto)\n",
        "            T_total = self.clip_len * self.temp_stride\n",
        "            dummy = torch.zeros(T_total, 3, self.crop, self.crop, dtype=torch.uint8)\n",
        "            clip = dummy[::self.temp_stride][:self.clip_len]  # [T,C,H,W]\n",
        "        else:\n",
        "            T_total = video.shape[0]\n",
        "            need_total = self.clip_len * self.temp_stride\n",
        "            if T_total >= need_total:\n",
        "                start = random.randint(0, T_total - need_total) if self.train else max(0, (T_total - need_total)//2)\n",
        "                idxs = torch.arange(start, start+need_total, self.temp_stride)\n",
        "            else:\n",
        "                idxs = torch.arange(0, min(T_total, need_total), self.temp_stride)\n",
        "                if len(idxs) < self.clip_len:\n",
        "                    pad = self.clip_len - len(idxs)\n",
        "                    idxs = torch.cat([idxs, idxs[-1:].repeat(pad)])\n",
        "            idxs = idxs[idxs < (video.shape[0] if video is not None else 0)]\n",
        "            if video is None or len(idxs) < self.clip_len:\n",
        "                # fallback por seguridad si video corto o None\n",
        "                T_total = self.clip_len * self.temp_stride\n",
        "                dummy = torch.zeros(T_total, 3, self.crop, self.crop, dtype=torch.uint8)\n",
        "                clip = dummy[::self.temp_stride][:self.clip_len]\n",
        "            else:\n",
        "                clip = video[idxs]  # [T,C,H,W]\n",
        "\n",
        "        # Spatial aug por frame\n",
        "        clip = torch.stack([self.spatial(fr) for fr in clip])   # [T,C,H,W]\n",
        "        clip = clip.float() / 255.0\n",
        "        clip = (clip - self.mean) / self.std\n",
        "        clip = clip.permute(1,0,2,3).contiguous()               # [C,T,H,W]\n",
        "        return clip, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# =========================\n",
        "# MODELO / CONGELADO\n",
        "# =========================\n",
        "def build_model(num_classes=2):\n",
        "    try:\n",
        "        model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "    except TypeError:\n",
        "        model = r3d_18(pretrained=True)\n",
        "    in_feats = model.fc.in_features\n",
        "    model.fc = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(in_feats, num_classes))\n",
        "    return model\n",
        "\n",
        "def set_trainable_params(model, phase):\n",
        "    if phase == \"A\":\n",
        "        for n,p in model.named_parameters():\n",
        "            p.requires_grad = any(k in n for k in [\"layer2\",\"layer3\",\"layer4\",\"fc\"])\n",
        "    else:\n",
        "        for _,p in model.named_parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "def build_optimizer(model, phase):\n",
        "    if phase == \"A\":\n",
        "        return torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=LR_A, weight_decay=WEIGHT_DEC_A\n",
        "        )\n",
        "    elif phase == \"B\":\n",
        "        params = [\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"fc\" in n) and p.requires_grad], \"lr\": LR_B_FC},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer4\" in n) and p.requires_grad], \"lr\": LR_B_L4},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer3\" in n) and p.requires_grad], \"lr\": LR_B_L3},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer2\" in n) and p.requires_grad], \"lr\": LR_B_L2},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer1\" in n) and p.requires_grad], \"lr\": LR_B_L1},\n",
        "        ]\n",
        "        return torch.optim.AdamW(params, weight_decay=WEIGHT_DEC_B)\n",
        "    else:\n",
        "        params = [\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"fc\" in n) and p.requires_grad], \"lr\": LR_C_FC},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer4\" in n) and p.requires_grad], \"lr\": LR_C_L4},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer3\" in n) and p.requires_grad], \"lr\": LR_C_L3},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer2\" in n) and p.requires_grad], \"lr\": LR_C_L2},\n",
        "            {\"params\":[p for n,p in model.named_parameters() if (\"layer1\" in n) and p.requires_grad], \"lr\": LR_C_L1},\n",
        "        ]\n",
        "        return torch.optim.AdamW(params, weight_decay=WEIGHT_DEC_C)\n",
        "\n",
        "# =========================\n",
        "# CARGA COMPATIBLE DE CHECKPOINTS (_orig_mod., module.)\n",
        "# =========================\n",
        "def _load_state_compat(model, ckpt_path, strict=False):\n",
        "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    state = sd.get(\"model\", sd)\n",
        "    new_state = {}\n",
        "    for k, v in state.items():\n",
        "        nk = k\n",
        "        if nk.startswith(\"_orig_mod.\"):\n",
        "            nk = nk[len(\"_orig_mod.\"):]\n",
        "        if nk.startswith(\"module.\"):\n",
        "            nk = nk[len(\"module.\"):]\n",
        "        new_state[nk] = v\n",
        "    missing, unexpected = model.load_state_dict(new_state, strict=strict)\n",
        "    print(\"Cargado (compat):\", ckpt_path)\n",
        "    print(\"Missing (muestra):\", missing[:5], \"...\" if len(missing)>5 else \"\")\n",
        "    print(\"Unexpected (muestra):\", unexpected[:5], \"...\" if len(unexpected)>5 else \"\")\n",
        "    return sd\n",
        "\n",
        "# =========================\n",
        "# REANUDACIÓN\n",
        "# =========================\n",
        "def _rng_state():\n",
        "    return {\n",
        "        \"py_random\": random.getstate(),\n",
        "        \"np_random\": np.random.get_state(),\n",
        "        \"torch_cpu\": torch.random.get_rng_state(),\n",
        "        \"torch_cuda\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "    }\n",
        "\n",
        "def _set_rng_state(st):\n",
        "    if st is None: return\n",
        "    random.setstate(st.get(\"py_random\"))\n",
        "    np.random.set_state(st.get(\"np_random\"))\n",
        "    torch.random.set_rng_state(st.get(\"torch_cpu\"))\n",
        "    if torch.cuda.is_available() and st.get(\"torch_cuda\") is not None:\n",
        "        torch.cuda.set_rng_state_all(st.get(\"torch_cuda\"))\n",
        "\n",
        "def save_ckpt(path, epoch, best_f1, best_loss, row, model, optimizer, extra=None):\n",
        "    pack = {\n",
        "        \"epoch\": epoch,\n",
        "        \"best_f1\": best_f1,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"metrics\": row,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scaler\": None,  # bf16\n",
        "        \"rng\": _rng_state(),\n",
        "        \"class_weights\": CLASS_WEIGHTS,\n",
        "    }\n",
        "    if extra: pack.update(extra)\n",
        "    torch.save(pack, path)\n",
        "\n",
        "def try_resume(model, optimizer):\n",
        "    start_epoch = 0; best_f1 = -1.0; best_loss = float(\"inf\")\n",
        "    if os.path.exists(CKPT_LAST):\n",
        "        sd = torch.load(CKPT_LAST, map_location=\"cpu\")\n",
        "        _load_state_compat(model, CKPT_LAST, strict=False)\n",
        "        if \"optimizer\" in sd: optimizer.load_state_dict(sd[\"optimizer\"])\n",
        "        _set_rng_state(sd.get(\"rng\"))\n",
        "        start_epoch = sd.get(\"epoch\", 0)\n",
        "        best_f1     = sd.get(\"best_f1\", best_f1)\n",
        "        best_loss   = sd.get(\"best_loss\", best_loss)\n",
        "        print(f\"🔁 Reanudado desde {CKPT_LAST} (epoch={start_epoch})\")\n",
        "    else:\n",
        "        if os.path.exists(CKPT_IN):\n",
        "            _load_state_compat(model, CKPT_IN, strict=False)\n",
        "        else:\n",
        "            print(\"⚠️ No se encontró CKPT_IN; se parte de Kinetics.\")\n",
        "    return start_epoch, best_f1, best_loss\n",
        "\n",
        "# =========================\n",
        "# LOOP\n",
        "# =========================\n",
        "def run_epoch(loader, model, optimizer, criterion, train=True, current_epoch=0,\n",
        "              max_batches=None, global_step_start=0, accum_steps=1):\n",
        "    model.train() if train else model.eval()\n",
        "    tot_loss = 0.0\n",
        "    all_probs, all_ys = [], []\n",
        "    global_step = global_step_start\n",
        "    if train: optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches): break\n",
        "        x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Marcador de paso para casos con compilers (inofensivo con compile apagado)\n",
        "        try:\n",
        "            torch.compiler.cudagraph_mark_step_begin()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        with autocast(**AUTOCast_KW):\n",
        "            logits = model(x)\n",
        "            logits = logits.clone()  # evita aliasing ante acumulación\n",
        "            loss = criterion(logits, y)\n",
        "            if train: loss = loss / accum_steps\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if ((i + 1) % accum_steps == 0) or (i + 1 == len(loader)):\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 5.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                global_step += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs_hurto = torch.softmax(logits.float(), dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "        all_probs.append(probs_hurto)\n",
        "        all_ys.append(y.detach().cpu().numpy())\n",
        "        tot_loss += (loss.item() * (accum_steps if train else 1)) * y.size(0)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
        "    all_ys    = np.concatenate(all_ys)    if all_ys    else np.array([])\n",
        "    avg_loss  = tot_loss / max(1, len(loader.dataset))\n",
        "\n",
        "    if all_probs.size > 0:\n",
        "        y_pred = (all_probs >= 0.5).astype(int)\n",
        "        f1 = f1_score(all_ys, y_pred, pos_label=1)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "    return avg_loss, f1, all_probs, all_ys, global_step\n",
        "\n",
        "# =========================\n",
        "# MAIN\n",
        "# =========================\n",
        "def main():\n",
        "    if PHASE == \"A\":\n",
        "        clip_len, tstride, crop = CLIP_LEN_A, TEMP_STRIDE_A, CROP_A\n",
        "        epochs = EPOCHS_A; gamma = GAMMA_A\n",
        "    elif PHASE == \"B\":\n",
        "        clip_len, tstride, crop = CLIP_LEN_B, TEMP_STRIDE_B, CROP_B\n",
        "        epochs = EPOCHS_B; gamma = GAMMA_B\n",
        "    else:\n",
        "        clip_len, tstride, crop = CLIP_LEN_C, TEMP_STRIDE_C, CROP_C\n",
        "        epochs = EPOCHS_C; gamma = GAMMA_C\n",
        "\n",
        "    # Data\n",
        "    train_items = load_list(TRAIN_CSV)\n",
        "    val_items   = load_list(VAL_CSV)\n",
        "    print(f\"Train clips: {len(train_items)} | Val clips: {len(val_items)}\")\n",
        "    N_normal = sum(1 for _,y in train_items if y==0)\n",
        "    N_hurto  = sum(1 for _,y in train_items if y==1)\n",
        "    print(f\"Conteo train → normal={N_normal} | hurto={N_hurto}\")\n",
        "\n",
        "    train_ds = VideoDataset(train_items, train=True,  clip_len=clip_len, temp_stride=tstride, crop=crop)\n",
        "    val_ds   = VideoDataset(val_items,   train=False, clip_len=clip_len, temp_stride=tstride, crop=crop)\n",
        "\n",
        "    # kwargs seguros según número de workers\n",
        "    def _dl_kwargs(base_kwargs):\n",
        "        kwargs = dict(base_kwargs)\n",
        "        if NUM_WORKERS > 0:\n",
        "            kwargs[\"prefetch_factor\"]   = PREFETCH\n",
        "            kwargs[\"persistent_workers\"] = PERSISTENT\n",
        "        return kwargs\n",
        "\n",
        "    if USE_SAMPLER:\n",
        "        from torch.utils.data import WeightedRandomSampler\n",
        "        weight_per_class = {0: 1.0/max(1,N_normal), 1: 1.0/max(1,N_hurto)}\n",
        "        sample_weights = np.array([weight_per_class[y] for _, y in train_items], dtype=np.float32)\n",
        "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            **_dl_kwargs(dict(batch_size=BATCH_SIZE, sampler=sampler,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PINMEM,\n",
        "                              drop_last=True))\n",
        "        )\n",
        "    else:\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            **_dl_kwargs(dict(batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PINMEM,\n",
        "                              drop_last=True))\n",
        "        )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        **_dl_kwargs(dict(batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PINMEM))\n",
        "    )\n",
        "\n",
        "    # Modelo / Opt / Loss\n",
        "    model = build_model(num_classes=2).to(DEVICE)\n",
        "    set_trainable_params(model, PHASE)\n",
        "    optimizer = build_optimizer(model, PHASE)\n",
        "    criterion = FocalLoss(gamma=gamma, weight=CLASS_WEIGHTS.to(DEVICE))\n",
        "\n",
        "    # CARGA CHECKPOINTS (compat)\n",
        "    start_epoch, best_f1, best_loss = try_resume(model, optimizer)\n",
        "\n",
        "    # (compile apagado por config)\n",
        "    if USE_COMPILE:\n",
        "        try:\n",
        "            model = torch.compile(model, mode=\"max-autotune\", dynamic=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Historial\n",
        "    history = []\n",
        "    global_step = 0\n",
        "\n",
        "    # Entrenamiento\n",
        "    for epoch in range(start_epoch + 1, start_epoch + epochs + 1):\n",
        "        if PHASE == \"A\":\n",
        "            base_lr = LR_A\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_A, pg.get(\"lr\", base_lr), warmup=2)\n",
        "        elif PHASE == \"B\":\n",
        "            for pg in optimizer.param_groups:\n",
        "                base = pg[\"lr\"]\n",
        "                pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_B, base, warmup=2)\n",
        "        else:\n",
        "            for pg in optimizer.param_groups:\n",
        "                base = pg[\"lr\"]\n",
        "                pg[\"lr\"] = cosine_warmup_lr(epoch - 1, EPOCHS_C, base, warmup=2)\n",
        "\n",
        "        tr_loss, tr_f1, _, _, global_step = run_epoch(\n",
        "            train_loader, model, optimizer, criterion, train=True,\n",
        "            current_epoch=epoch, max_batches=None, global_step_start=global_step, accum_steps=ACCUM_STEPS\n",
        "        )\n",
        "        va_loss, va_f1, probs, ys, _ = run_epoch(\n",
        "            val_loader, model, optimizer, criterion, train=False,\n",
        "            current_epoch=epoch, max_batches=None, global_step_start=global_step, accum_steps=1\n",
        "        )\n",
        "\n",
        "        row = {\"epoch\": epoch, \"train_loss\": tr_loss, \"train_f1\": tr_f1,\n",
        "               \"val_loss\": va_loss, \"val_f1\": va_f1, \"phase\": PHASE}\n",
        "        history.append(row)\n",
        "        print(f\"[{epoch:02d}/{start_epoch+epochs:02d}] phase={PHASE} | train {tr_loss:.4f}/{tr_f1:.3f} | val {va_loss:.4f}/{va_f1:.3f}\")\n",
        "\n",
        "        save_ckpt(CKPT_LAST, epoch, best_f1, best_loss, row, model, optimizer)\n",
        "        try: shutil.copy2(CKPT_LAST, CKPT_RUN)\n",
        "        except Exception: pass\n",
        "\n",
        "        if va_f1 > best_f1:\n",
        "            best_f1 = va_f1\n",
        "            torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTF1)\n",
        "            print(\"✔️ Mejor F1 ->\", CKPT_BESTF1)\n",
        "        if va_loss < best_loss:\n",
        "            best_loss = va_loss\n",
        "            torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"metrics\": row}, CKPT_BESTL)\n",
        "            print(\"✔️ Mejor Loss ->\", CKPT_BESTL)\n",
        "\n",
        "    with open(LOG_JSON, \"w\") as f: json.dump(history, f, indent=2)\n",
        "    print(\"Log guardado en:\", LOG_JSON)\n",
        "\n",
        "    if 'probs' in locals() and probs.size > 0:\n",
        "        yp05 = (probs >= 0.5).astype(int)\n",
        "        print(\"\\n=== Classification Report (umbral 0.5) ===\")\n",
        "        print(classification_report(ys, yp05, target_names=[\"normal\",\"hurto\"]))\n",
        "        print(\"Matriz de confusión (val):\\n\", confusion_matrix(ys, yp05))\n",
        "\n",
        "        ths = np.round(np.linspace(TH_MIN, TH_MAX, TH_STEPS), 3)\n",
        "        with open(THR_CSV, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f); w.writerow([\"threshold\",\"beta\",\"precision_hurto\",\"recall_hurto\",\"f1_hurto\",\"fbeta_hurto\"])\n",
        "            for t in ths:\n",
        "                ypt = (probs >= t).astype(int)\n",
        "                p, r, f1, _ = precision_recall_fscore_support(ys, ypt, average=None, zero_division=0)\n",
        "                p1, r1, f1_1 = p[1], r[1], f1[1]\n",
        "                for beta in FBETAS:\n",
        "                    b2 = beta*beta\n",
        "                    fbeta = (1+b2) * (p1*r1) / (b2*p1 + r1 + 1e-12)\n",
        "                    w.writerow([t, beta, p1, r1, f1_1, fbeta])\n",
        "        print(\"Tabla de thresholds guardada en:\", THR_CSV)\n",
        "\n",
        "        rows = []\n",
        "        with open(THR_CSV, newline=\"\") as f:\n",
        "            r = csv.DictReader(f)\n",
        "            for row in r:\n",
        "                if float(row[\"beta\"]) == 2.0:\n",
        "                    rows.append((float(row[\"fbeta_hurto\"]), float(row[\"threshold\"]),\n",
        "                                 float(row[\"precision_hurto\"]), float(row[\"recall_hurto\"]),\n",
        "                                 float(row[\"f1_hurto\"])))\n",
        "        if rows:\n",
        "            rows.sort(reverse=True)\n",
        "            f2b, t_best, p1b, r1b, f1b = rows[0]\n",
        "            print(f\"\\n⭐ Mejor threshold por F2 (hurto): t={t_best:.3f} | P={p1b:.3f} | R={r1b:.3f} | F1={f1b:.3f} | F2={f2b:.3f}\")\n",
        "            with open(OPER_JSON, \"w\") as f:\n",
        "                json.dump({\"threshold\": float(t_best), \"metric\":\"F2\"}, f, indent=2)\n",
        "            print(\"Operating threshold ->\", OPER_JSON)\n",
        "    else:\n",
        "        print(\"\\n(No hubo validación con muestras; sin sweep de thresholds)\")\n",
        "\n",
        "# =========================\n",
        "# LOSS (Focal) y LR schedule\n",
        "# =========================\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.2, weight=None, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction=\"none\")\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = self.ce(logits, target)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1-pt)**self.gamma) * ce\n",
        "        if self.reduction == \"mean\": return loss.mean()\n",
        "        if self.reduction == \"sum\":  return loss.sum()\n",
        "        return loss\n",
        "\n",
        "def cosine_warmup_lr(t, T, base_lr, warmup=2):\n",
        "    if t < warmup: return base_lr * (t+1)/max(1,warmup)\n",
        "    tw = max(1, T - warmup); tc = max(0, t - warmup)\n",
        "    return 0.5 * base_lr * (1 + math.cos(math.pi * tc / tw))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwidowK7DiNI",
        "outputId": "28f0e58f-64c3-43b3-b243-6d4aedd3fb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train clips: 9220 | Val clips: 896\n",
            "Conteo train → normal=601 | hurto=8619\n",
            "Cargado (compat): /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "Missing (muestra): [] \n",
            "Unexpected (muestra): [] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/10] phase=B | train 0.0144/0.995 | val 0.0271/0.997\n",
            "✔️ Mejor F1 -> /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/bestF1.pt\n",
            "✔️ Mejor Loss -> /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/bestLoss.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/10] phase=B | train 0.0017/0.999 | val 0.0480/0.998\n",
            "✔️ Mejor F1 -> /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/bestF1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03/10] phase=B | train 0.0042/0.999 | val 0.0528/0.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/10] phase=B | train 0.0005/1.000 | val 0.0395/0.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/10] phase=B | train 0.0036/0.999 | val 0.0323/0.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/10] phase=B | train 0.0003/1.000 | val 0.0551/0.990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07/10] phase=B | train 0.0024/0.999 | val 0.0333/0.998\n",
            "✔️ Mejor F1 -> /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/bestF1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/10] phase=B | train 0.0002/1.000 | val 0.0395/0.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/10] phase=B | train 0.0001/1.000 | val 0.0468/0.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10/10] phase=B | train 0.0001/1.000 | val 0.0290/0.997\n",
            "Log guardado en: /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/trainlog.json\n",
            "\n",
            "=== Classification Report (umbral 0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       1.00      0.87      0.93        39\n",
            "       hurto       0.99      1.00      1.00       857\n",
            "\n",
            "    accuracy                           0.99       896\n",
            "   macro avg       1.00      0.94      0.96       896\n",
            "weighted avg       0.99      0.99      0.99       896\n",
            "\n",
            "Matriz de confusión (val):\n",
            " [[ 34   5]\n",
            " [  0 857]]\n",
            "Tabla de thresholds guardada en: /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/threshold_sweep.csv\n",
            "\n",
            "⭐ Mejor threshold por F2 (hurto): t=0.550 | P=0.998 | R=1.000 | F1=0.999 | F2=1.000\n",
            "Operating threshold -> /content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/operating_threshold.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# EVALUACIÓN Colab – R3D-18 + Barrido + Runtime-like POST\n",
        "# ============================================\n",
        "import os, re, csv, json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.video import r3d_18\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    precision_recall_curve, precision_recall_fscore_support\n",
        ")\n",
        "import cv2\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BASE_DIR   = \"/content/tesisV2/cortos/video_clips_192\"\n",
        "VAL_CSV    = f\"{BASE_DIR}/val.csv\"\n",
        "TEST_CSV   = f\"{BASE_DIR}/test.csv\"\n",
        "CKPT_PATH  = \"/content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\"\n",
        "\n",
        "NUM_CLASSES   = 2\n",
        "CLIP_LEN      = 16\n",
        "RESIZE        = 128\n",
        "CROP          = 112\n",
        "BATCH_SIZE    = 8\n",
        "\n",
        "# Barrido de thresholds\n",
        "T_MIN, T_MAX, T_STEPS = 0.05, 0.95, 37  # paso ~0.025\n",
        "FIXED_T = 0.55                           # opcional, se reporta además de \"best F2\"\n",
        "\n",
        "# POST tipo runtime\n",
        "HOP_SECONDS    = 0.5     # si tus clips tienen otro hop, cambiá esto\n",
        "SMOOTH_K       = 5       # ~2.5 s si hop=0.5\n",
        "HYST_WIDTH     = 0.08    # THR_LO = THR_HI - HYST_WIDTH\n",
        "MIN_EVENT_SEC  = 0.60\n",
        "COOLDOWN_SEC   = 2.0\n",
        "ABS_FLOOR_PEAK = 0.48    # si querés aplicar pisos (opcional)\n",
        "ABS_FLOOR_MEAN = 0.36\n",
        "BASE_MIN_FRAC  = 0.18\n",
        "\n",
        "OUT_DIR = Path(\"/content/eval_out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# DATASET\n",
        "# -----------------------------\n",
        "val_spatial = T.Compose([T.Resize((RESIZE, RESIZE)), T.CenterCrop(CROP)])\n",
        "\n",
        "class SimpleVideoEval(Dataset):\n",
        "    def __init__(self, csv_path, base_dir):\n",
        "        self.items=[]; self.base_dir=base_dir\n",
        "        with open(csv_path, newline='') as f:\n",
        "            r = csv.reader(f)\n",
        "            for row in r:\n",
        "                if not row: continue\n",
        "                head=row[0].strip().lower()\n",
        "                if head in (\"path\",\"ruta\",\"file\",\"filename\"):  # header\n",
        "                    continue\n",
        "                path=row[0].strip()\n",
        "                y = 0 if str(row[1]).lower() in (\"0\",\"normal\") else 1\n",
        "                if not path.startswith(\"/\"):\n",
        "                    cand=os.path.join(base_dir, path)\n",
        "                    if os.path.exists(cand):\n",
        "                        path=cand\n",
        "                    else:\n",
        "                        sub=\"normal\" if y==0 else \"shoplifting\"\n",
        "                        path=os.path.join(base_dir, sub, path)\n",
        "                self.items.append((path, y))\n",
        "        # Kinetics norm\n",
        "        self.mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1,3,1,1)\n",
        "        self.std  = torch.tensor([0.22803, 0.22145, 0.216989]).view(1,3,1,1)\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def _read_video_cv2(self, path):\n",
        "        cap=cv2.VideoCapture(str(path))\n",
        "        frames=[]\n",
        "        while True:\n",
        "            ok, fr=cap.read()\n",
        "            if not ok: break\n",
        "            fr=cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(fr)\n",
        "        cap.release()\n",
        "        if len(frames)==0:\n",
        "            raise RuntimeError(f\"No se pudo leer: {path}\")\n",
        "        T_total=len(frames)\n",
        "        if T_total>=CLIP_LEN:\n",
        "            start=max(0,(T_total-CLIP_LEN)//2)  # centro\n",
        "            frames=frames[start:start+CLIP_LEN]\n",
        "        else:\n",
        "            last=frames[-1]\n",
        "            frames=frames + [last]*(CLIP_LEN-T_total)\n",
        "        arr=np.stack(frames, axis=0)                 # [T,H,W,3] uint8\n",
        "        ten=torch.from_numpy(arr).permute(0,3,1,2)   # [T,3,H,W]\n",
        "        return ten\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path,y=self.items[idx]\n",
        "        clip=self._read_video_cv2(path)                  # [T,3,H,W]\n",
        "        clip=torch.stack([val_spatial(fr) for fr in clip])\n",
        "        clip=clip.float()/255.0\n",
        "        clip=(clip - self.mean)/self.std\n",
        "        clip=clip.permute(1,0,2,3).contiguous()          # [3,T,H,W]\n",
        "        return clip, torch.tensor(y, dtype=torch.long), path\n",
        "\n",
        "# -----------------------------\n",
        "# MODELO\n",
        "# -----------------------------\n",
        "def build_model():\n",
        "    try:\n",
        "        model=r3d_18(weights=\"KINETICS400_V1\")\n",
        "    except TypeError:\n",
        "        model=r3d_18(pretrained=True)\n",
        "    in_feats=model.fc.in_features\n",
        "    model.fc=nn.Sequential(nn.Dropout(0.5), nn.Linear(in_feats, NUM_CLASSES))\n",
        "    sd=torch.load(CKPT_PATH, map_location=\"cpu\")\n",
        "    state_dict = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Checkpoint cargado:\", CKPT_PATH)\n",
        "    if missing or unexpected:\n",
        "        print(\"Missing:\", missing, \"Unexpected:\", unexpected)\n",
        "    return model.to(DEVICE).eval()\n",
        "\n",
        "# -----------------------------\n",
        "# INFERENCIA\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def infer_loader(model, loader):\n",
        "    probs_all, ys_all, paths_all = [], [], []\n",
        "    soft=nn.Softmax(dim=1)\n",
        "    for x,y,paths in loader:\n",
        "        x=x.to(DEVICE, non_blocking=True)\n",
        "        logits=model(x)\n",
        "        probs=soft(logits)[:,1].detach().cpu().numpy()\n",
        "        probs_all.append(probs)\n",
        "        ys_all.append(y.numpy())\n",
        "        paths_all.extend(list(paths))\n",
        "    return np.concatenate(probs_all), np.concatenate(ys_all), paths_all\n",
        "\n",
        "# -----------------------------\n",
        "# HELPERS POST\n",
        "# -----------------------------\n",
        "def moving_avg(a,k):\n",
        "    if k<=1: return a\n",
        "    pad=(k-1)//2\n",
        "    a_pad=np.pad(a,(pad,pad),mode='edge')\n",
        "    ker=np.ones(k)/k\n",
        "    return np.convolve(a_pad,ker,mode='valid')\n",
        "\n",
        "def hysteresis_events(sm_probs, thr_on, thr_off, hop_s, min_event_s, cooldown_s,\n",
        "                      abs_floor_peak=None, abs_floor_mean=None, base_min_frac=None):\n",
        "    active=False; start=None; events=[]\n",
        "    above_mid_frac = []\n",
        "    mid_thr = max(thr_off, 0.85*thr_on)\n",
        "\n",
        "    # opcional: contabilidad de “fracción por encima del mid”\n",
        "    for i,p in enumerate(sm_probs):\n",
        "        above_mid_frac.append(1.0 if p>=mid_thr else 0.0)\n",
        "\n",
        "    for i,p in enumerate(sm_probs):\n",
        "        if not active and p>=thr_on:\n",
        "            active=True; start=i\n",
        "        elif active and p<=thr_off:\n",
        "            end=i\n",
        "            seg = sm_probs[start:end+1] if end>=start else sm_probs[start:start+1]\n",
        "            dur_ok = ((end-start+1)*hop_s) >= min_event_s\n",
        "            # filtros opcionales estilo runtime:\n",
        "            peak_ok = True if abs_floor_peak is None else (seg.max() >= abs_floor_peak)\n",
        "            mean_ok = True if abs_floor_mean is None else (seg.mean() >= abs_floor_mean)\n",
        "            frac_ok = True\n",
        "            if base_min_frac is not None:\n",
        "                sl = above_mid_frac[start:end+1]\n",
        "                frac_ok = (np.mean(sl) >= base_min_frac) if len(sl)>0 else False\n",
        "            if dur_ok and peak_ok and mean_ok and frac_ok:\n",
        "                events.append([start, end])\n",
        "            active=False\n",
        "\n",
        "    if active:\n",
        "        end=len(sm_probs)-1\n",
        "        seg = sm_probs[start:end+1]\n",
        "        dur_ok = ((end-start+1)*hop_s) >= min_event_s\n",
        "        peak_ok = True if abs_floor_peak is None else (seg.max() >= abs_floor_peak)\n",
        "        mean_ok = True if abs_floor_mean is None else (seg.mean() >= abs_floor_mean)\n",
        "        frac_ok = True\n",
        "        if base_min_frac is not None:\n",
        "            sl = above_mid_frac[start:end+1]\n",
        "            frac_ok = (np.mean(sl) >= base_min_frac) if len(sl)>0 else False\n",
        "        if dur_ok and peak_ok and mean_ok and frac_ok:\n",
        "            events.append([start,end])\n",
        "\n",
        "    # merge por cooldown\n",
        "    merged=[]\n",
        "    for s,e in events:\n",
        "        if not merged: merged.append([s,e])\n",
        "        else:\n",
        "            ps,pe=merged[-1]\n",
        "            if (s-pe)*hop_s < cooldown_s:\n",
        "                merged[-1][1]=e\n",
        "            else:\n",
        "                merged.append([s,e])\n",
        "    return merged\n",
        "\n",
        "def events_to_clip_labels(n, events):\n",
        "    y=np.zeros(n, dtype=np.int64)\n",
        "    for s,e in events: y[s:e+1]=1\n",
        "    return y\n",
        "\n",
        "_idx_pat = re.compile(r\"(?:_|-)(\\d{1,6})(?=\\D*$)\")\n",
        "def infer_vid_and_idx(path):\n",
        "    p=str(path); stem=Path(p).stem\n",
        "    m=_idx_pat.search(stem)\n",
        "    if m:\n",
        "        clip_idx=int(m.group(1))\n",
        "        video_id=stem[:m.start()] or Path(p).parent.name\n",
        "    else:\n",
        "        video_id=Path(p).parent.name\n",
        "        clip_idx=stem\n",
        "    return video_id, clip_idx\n",
        "\n",
        "def natkey(x):\n",
        "    try: return int(x)\n",
        "    except: return x\n",
        "\n",
        "# -----------------------------\n",
        "# EVAL: Barrido + Reports\n",
        "# -----------------------------\n",
        "def sweep_thresholds(y_true, probs, t_list):\n",
        "    rows=[]\n",
        "    for t in t_list:\n",
        "        y_pred = (probs >= t).astype(int)\n",
        "        P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "        beta2 = 2.0\n",
        "        F2 = (1+beta2**2)*P*R/((beta2**2)*P + R + 1e-12) if (P+R)>0 else 0.0\n",
        "        rows.append([t,P,R,F1,F2])\n",
        "    df = pd.DataFrame(rows, columns=[\"t\",\"precision\",\"recall\",\"f1\",\"f2\"])\n",
        "    return df\n",
        "\n",
        "def evaluate_split(split_name, csv_path, model):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"[{split_name}] No existe:\", csv_path); return\n",
        "\n",
        "    ds=SimpleVideoEval(csv_path, BASE_DIR)\n",
        "    dl=DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "    probs, ys, paths = infer_loader(model, dl)\n",
        "\n",
        "    # ---------- Barrido base (clip-level) ----------\n",
        "    t_list = np.linspace(T_MIN, T_MAX, T_STEPS)\n",
        "    sweep = sweep_thresholds(ys, probs, t_list)\n",
        "    sweep_path = OUT_DIR / f\"{split_name.lower()}_threshold_sweep.csv\"\n",
        "    sweep.to_csv(sweep_path, index=False)\n",
        "\n",
        "    best = sweep.sort_values(\"f2\", ascending=False).iloc[0].to_dict()\n",
        "    t_best = float(best[\"t\"])\n",
        "    print(f\"\\n[{split_name}] ⭐ Mejor threshold por F2: t={t_best:.3f} | P={best['precision']:.3f} | R={best['recall']:.3f} | F1={best['f1']:.3f} | F2={best['f2']:.3f}\")\n",
        "\n",
        "    # ---------- Baseline (umbral directo) ----------\n",
        "    def report_for_threshold(name_t, t):\n",
        "        y_pred = (probs >= t).astype(int)\n",
        "        cm = confusion_matrix(ys, y_pred, labels=[0,1])\n",
        "        rep = classification_report(ys, y_pred, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "        print(f\"\\n[{split_name}] --- BASELINE t={name_t} ---\")\n",
        "        print(cm); print(rep)\n",
        "        return cm, rep\n",
        "\n",
        "    cm_best, rep_best = report_for_threshold(f\"{t_best:.3f}\", t_best)\n",
        "    if FIXED_T is not None:\n",
        "        report_for_threshold(f\"{FIXED_T:.3f}\", float(FIXED_T))\n",
        "\n",
        "    # ---------- POST tipo runtime (video-level, luego clip labels) ----------\n",
        "    # usamos THR_HI/LO a partir de t_best\n",
        "    thr_hi = t_best\n",
        "    thr_lo = max(0.0, thr_hi - HYST_WIDTH)\n",
        "\n",
        "    buckets=defaultdict(list)\n",
        "    for p,pr,yv in zip(paths, probs, ys):\n",
        "        vid,idx=infer_vid_and_idx(p)\n",
        "        buckets[vid].append((idx, float(pr), int(yv)))\n",
        "\n",
        "    y_true_all=[]; y_pred_all=[]\n",
        "    for vid, triples in buckets.items():\n",
        "        triples.sort(key=lambda t: natkey(t[0]))\n",
        "        pv=np.array([t[1] for t in triples], float)\n",
        "        yv=np.array([t[2] for t in triples], int)\n",
        "        n=len(pv)\n",
        "        sm=moving_avg(pv, SMOOTH_K)\n",
        "        evs=hysteresis_events(\n",
        "            sm, thr_hi, thr_lo,\n",
        "            HOP_SECONDS, MIN_EVENT_SEC, COOLDOWN_SEC,\n",
        "            abs_floor_peak=ABS_FLOOR_PEAK, abs_floor_mean=ABS_FLOOR_MEAN, base_min_frac=BASE_MIN_FRAC\n",
        "        )\n",
        "        yhat=events_to_clip_labels(n, evs)\n",
        "        y_true_all.append(yv); y_pred_all.append(yhat)\n",
        "\n",
        "    y_true_all=np.concatenate(y_true_all) if y_true_all else ys\n",
        "    y_pred_all=np.concatenate(y_pred_all) if y_pred_all else (probs>=thr_hi).astype(int)\n",
        "\n",
        "    cm_post = confusion_matrix(y_true_all, y_pred_all, labels=[0,1])\n",
        "    rep_post = classification_report(y_true_all, y_pred_all, target_names=[\"normal\",\"hurto\"], digits=3)\n",
        "\n",
        "    print(f\"\\n[{split_name}] --- POST (smoothing+histéresis+minDur) con t_hi={thr_hi:.3f}, t_lo={thr_lo:.3f} ---\")\n",
        "    print(cm_post); print(rep_post)\n",
        "\n",
        "    # ---------- Guardar JSON de operating point ----------\n",
        "    op = dict(split=split_name, threshold=float(t_best),\n",
        "              precision=float(best[\"precision\"]),\n",
        "              recall=float(best[\"recall\"]),\n",
        "              f1=float(best[\"f1\"]),\n",
        "              f2=float(best[\"f2\"]),\n",
        "              thr_lo=float(thr_lo))\n",
        "    with open(OUT_DIR / f\"{split_name.lower()}_operating_threshold.json\",\"w\") as f:\n",
        "        json.dump(op, f, indent=2)\n",
        "\n",
        "    # ---------- PR Curve ----------\n",
        "    prec, rec, thr = precision_recall_curve(ys, probs)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(rec, prec)\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Precision–Recall ({split_name})\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # ---------- Devolver por si querés usar luego ----------\n",
        "    return dict(\n",
        "        probs=probs, ys=ys, paths=paths,\n",
        "        sweep=sweep, best=op,\n",
        "        cm_base=cm_best, rep_base=rep_best,\n",
        "        cm_post=cm_post, rep_post=rep_post\n",
        "    )\n",
        "\n",
        "# -----------------------------\n",
        "# RUN\n",
        "# -----------------------------\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "model = build_model()\n",
        "\n",
        "results = {}\n",
        "for split_name, csv_path in [(\"VAL\", VAL_CSV), (\"TEST\", TEST_CSV)]:\n",
        "    res = evaluate_split(split_name, csv_path, model)\n",
        "    results[split_name] = res\n",
        "\n",
        "print(\"\\nArchivos guardados en:\", str(OUT_DIR))\n",
        "for p in OUT_DIR.iterdir():\n",
        "    print(\" -\", p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t91-jAd6x_Nq",
        "outputId": "277b57a2-45e5-4745-8cb2-3ad460b53830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA disponible: True\n",
            "Checkpoint cargado: /content/drive/MyDrive/tesisV2/models/r3d18_shoplifting_ft_l34_bestF1.pt\n",
            "\n",
            "[VAL] ⭐ Mejor threshold por F2: t=0.050 | P=0.845 | R=0.813 | F1=0.829 | F2=0.819\n",
            "\n",
            "[VAL] --- BASELINE t=0.050 ---\n",
            "[[ 18 100]\n",
            " [125 544]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.126     0.153     0.138       118\n",
            "       hurto      0.845     0.813     0.829       669\n",
            "\n",
            "    accuracy                          0.714       787\n",
            "   macro avg      0.485     0.483     0.483       787\n",
            "weighted avg      0.737     0.714     0.725       787\n",
            "\n",
            "\n",
            "[VAL] --- BASELINE t=0.550 ---\n",
            "[[116   2]\n",
            " [477 192]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.196     0.983     0.326       118\n",
            "       hurto      0.990     0.287     0.445       669\n",
            "\n",
            "    accuracy                          0.391       787\n",
            "   macro avg      0.593     0.635     0.386       787\n",
            "weighted avg      0.871     0.391     0.427       787\n",
            "\n",
            "\n",
            "[VAL] --- POST (smoothing+histéresis+minDur) con t_hi=0.050, t_lo=0.000 ---\n",
            "[[118   0]\n",
            " [669   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.150     1.000     0.261       118\n",
            "       hurto      0.000     0.000     0.000       669\n",
            "\n",
            "    accuracy                          0.150       787\n",
            "   macro avg      0.075     0.500     0.130       787\n",
            "weighted avg      0.022     0.150     0.039       787\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TEST] ⭐ Mejor threshold por F2: t=0.050 | P=0.817 | R=0.974 | F1=0.889 | F2=0.938\n",
            "\n",
            "[TEST] --- BASELINE t=0.050 ---\n",
            "[[ 65 126]\n",
            " [ 15 564]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.812     0.340     0.480       191\n",
            "       hurto      0.817     0.974     0.889       579\n",
            "\n",
            "    accuracy                          0.817       770\n",
            "   macro avg      0.815     0.657     0.684       770\n",
            "weighted avg      0.816     0.817     0.787       770\n",
            "\n",
            "\n",
            "[TEST] --- BASELINE t=0.550 ---\n",
            "[[186   5]\n",
            " [476 103]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      0.281     0.974     0.436       191\n",
            "       hurto      0.954     0.178     0.300       579\n",
            "\n",
            "    accuracy                          0.375       770\n",
            "   macro avg      0.617     0.576     0.368       770\n",
            "weighted avg      0.787     0.375     0.334       770\n",
            "\n",
            "\n",
            "[TEST] --- POST (smoothing+histéresis+minDur) con t_hi=0.050, t_lo=0.000 ---\n",
            "[[191   0]\n",
            " [  0 579]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal      1.000     1.000     1.000       191\n",
            "       hurto      1.000     1.000     1.000       579\n",
            "\n",
            "    accuracy                          1.000       770\n",
            "   macro avg      1.000     1.000     1.000       770\n",
            "weighted avg      1.000     1.000     1.000       770\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivos guardados en: /content/eval_out\n",
            " - /content/eval_out/val_operating_threshold.json\n",
            " - /content/eval_out/val_threshold_sweep.csv\n",
            " - /content/eval_out/test_threshold_sweep.csv\n",
            " - /content/eval_out/test_operating_threshold.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cjRXWf5dEEwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events_raw = merge_alert_segments(win_times, states, probs_s)\n",
        "events_raw = merge_close_events(events_raw, gap_s=0.5)\n",
        "events, inspected = filter_events(events_raw, win_times, P_RAW, P_SMO,\n",
        "                                  MIN_DUR, MIN_PEAK, MIN_MEAN, MID_THR, MIN_FRAC)\n",
        "\n",
        "# Export de eventos y picos (clips .mp4 con re-encode compatible)\n",
        "clips = cut_events_to_clips(VIDEO_IN, events, out_dir, pad_s=PAD_S, max_dur_s=MAX_DUR, reencode=True)\n",
        "peaks_extra = pick_all_peaks_above(P_SMO, win_times, min_prob=min_prob_for_peaks, min_gap_s=PEAK_MIN_GAP_S)\n",
        "clips_peaks = cut_peak_centered_clips(VIDEO_IN, peaks_extra, out_dir_peaks, clip_dur_s=PEAK_CLIP_DUR_S, reencode=True)\n",
        "\n",
        "# Registros para auditoría\n",
        "json.dump([...], open(OUT_JSON,\"w\"), indent=2)   # por ventana\n",
        "json.dump(events, open(OUT_EVENTS,\"w\"), indent=2) # por evento\n"
      ],
      "metadata": {
        "id": "ASx_zSOE0cZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cambiá esto según tu ruta en Drive\n",
        "with open(\"/content/drive/MyDrive/tesisV2/models/exp_l234_fullrun/trainlog.json\", \"r\") as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "epochs = [row[\"epoch\"] for row in history]\n",
        "train_loss = [row[\"train_loss\"] for row in history]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(epochs, train_loss, label=\"Pérdida entrenamiento\", linewidth=2)\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Curva de pérdida de entrenamiento\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "HiFGohQQ6tKe",
        "outputId": "7c3e0298-c49c-43ed-855e-c0a4c50ec4d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d374O-qcXHaQ",
        "1e6aXTdlEyW8",
        "l0nZ5qLQoxVN",
        "NJnijYtdd7DK",
        "zc4I-AOkpTtG",
        "BSWhg5N3qyBl",
        "7jfOQgmS6szQ",
        "Gb9LYZ4072Hz",
        "kaEj4NPF9cNH",
        "7HmyRxJK96O5",
        "fS7bAIj1xeoB",
        "rnBWdtshxBbY"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMUMrfmil4xNeVzeQG3Khzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}